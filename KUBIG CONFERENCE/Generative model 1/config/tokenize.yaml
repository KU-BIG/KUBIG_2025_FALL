seed_everything: 31415

model:
  enc_desc:
    - - spacetime_downsample
      - in_channels : 3
        kernel_size : 3
        out_channels : 512
        time_factor : 1
        space_factor : 4
    - - space-time_attn
      - n_rep: 8
        n_head: 8
        d_head: 64
  dec_desc:
    - - space-time_attn
      - n_rep: 8
        n_head: 8
        d_head: 64
    - - depth2spacetime_upsample
      - in_channels : 512
        kernel_size : 3
        out_channels : 3
        time_factor : 1
        space_factor : 4
  disc_kwargs:
    inp_size: [64, 64] # Size of input frames
    model_dim: 64 # Dimension of the model
    dim_mults: [1, 2, 4] # Channel multipliers
    down_step: [null, 2, 2] # Down-sampling steps
    inp_channels: 3
    kernel_size: 3
    num_groups: 8
    act_fn: leaky # Use LeakyReLU as activation function
    use_blur: True # Use BlurPooling for down-sampling
    use_attn: True # Discriminator can have spatial attention
    num_heads: 4 # Number of (spatial) attention heads
    dim_head: 32 # Dimension of each spatial attention heads
  #
  d_codebook: 10
  n_codebook: 1
  #
  lfq_bias: True
  lfq_frac_sample: 1
  lfq_commit_weight: 0.25
  lfq_entropy_weight: 0.01
  lfq_diversity_weight: 1.
  lfq_beta: 10.
  #
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 2e-3  # Increased from 1e-3 to help generator learn faster
      weight_decay: 0.01

  perceptual_model: vgg16
  perc_feat_layers: [features.6, features.13, features.18, features.25]
  gan_discriminate: frames
  gan_frames_per_batch: 4
  gan_loss_weight: 1.
  perc_loss_weight: 1.
  quant_loss_weight: 0.1  # Reduced from 1.0 to prevent quant loss from dominating total loss

data:
  # TODO : 본인 로컬 경로로 바꾸기
  root: /mnt/d/workspace/KUBIG/25-2_conference/open-genie_local/data
  env_name: Ninja
  padding: none
  randomize: true
  transform: null
  num_frames: 64
  batch_size: 1
  output_format: c t h w

trainer:
  max_epochs: 40
  accelerator: gpu
  devices: 1
  strategy: ddp_find_unused_parameters_true
  precision: 16-mixed
  log_every_n_steps: 16
  limit_val_batches: 1.0  # 전체 val 배치를 사용해 검증 수행
  val_check_interval: 1.0  # 에폭마다 검증 실행
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min
        save_top_k: 1
        save_last: true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        # TODO : 본인 로컬 경로로 바꾸기
        save_dir: /mnt/d/workspace/KUBIG/25-2_conference/open-genie_local/log
        name: genie-tokenizer
        version: null