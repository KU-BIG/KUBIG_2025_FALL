{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMMnjLmh+qGe3XGzxT100G6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tKEokEEN-23","executionInfo":{"status":"ok","timestamp":1767059038716,"user_tz":-540,"elapsed":9088,"user":{"displayName":"동욱김","userId":"12228314751297835719"}},"outputId":"48942c1f-8e37-4738-dc7c-f269872193b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Downloading ETTh1 -> /content/data/ETTh1.csv\n","/content/data/ETTh1 100%[===================>]   2.47M  --.-KB/s    in 0.04s   \n","ETTh1: OK (/content/data/ETTh1.csv)\n","Downloading ETTh2 -> /content/data/ETTh2.csv\n","/content/data/ETTh2 100%[===================>]   2.31M  --.-KB/s    in 0.05s   \n","ETTh2: OK (/content/data/ETTh2.csv)\n","Downloading ETTm1 -> /content/data/ETTm1.csv\n","/content/data/ETTm1 100%[===================>]   9.88M  --.-KB/s    in 0.07s   \n","ETTm1: OK (/content/data/ETTm1.csv)\n","Downloading ETTm2 -> /content/data/ETTm2.csv\n","/content/data/ETTm2 100%[===================>]   9.23M  --.-KB/s    in 0.06s   \n","ETTm2: OK (/content/data/ETTm2.csv)\n","Downloading Electricity -> /content/data/electricity.csv\n","/content/data/elect 100%[===================>]  91.15M   177MB/s    in 0.5s    \n","Electricity: OK (/content/data/electricity.csv)\n","Downloading Weather -> /content/data/weather.csv\n","/content/data/weath 100%[===================>]   6.90M  --.-KB/s    in 0.07s   \n","Weather: OK (/content/data/weather.csv)\n","Downloading Traffic -> /content/data/traffic.csv\n","/content/data/traff 100%[===================>] 130.16M   186MB/s    in 0.7s    \n","Traffic: OK (/content/data/traffic.csv)\n"]}],"source":["import os\n","import time\n","from copy import deepcopy\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","# =========================\n","# 0. 공통 설정\n","# =========================\n","\n","def setup_seed(seed: int = 2025):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","setup_seed(2025)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# =========================\n","# 1. 실험 설정 (요청사항)\n","# =========================\n","\n","DATASET_NAMES = [\n","    'ETTh1',\n","    'ETTh2',\n","    'ETTm1',\n","    'ETTm2',\n","    'Electricity',\n","    'Weather',\n","    'Traffic',\n","]\n","\n","INPUT_LENS = [96, 336]\n","PRED_LENS = [96, 192, 336, 720]\n","\n","# 공정 비교용 기본 하이퍼파라미터(필요하면 아래만 조절)\n","BATCH_SIZE = 32\n","EPOCHS = 30\n","PATIENCE = 5\n","MIN_EPOCHS = 5\n","LR = 1e-3\n","WEIGHT_DECAY = 1e-4\n","GRAD_CLIP = 1.0\n","USE_AMP = True\n","\n","# =========================\n","# Colab 경로 설정\n","# =========================\n","# Colab에서는 기본 작업 디렉토리가 /content 입니다.\n","# 데이터와 결과물이 섞이지 않게 /content/data, /content/artifacts 로 통일합니다.\n","\n","import sys\n","\n","IN_COLAB = 'google.colab' in sys.modules\n","BASE_DIR = '/content' if IN_COLAB else '.'\n","DATA_DIR = os.path.join(BASE_DIR, 'data')\n","ARTIFACT_DIR = os.path.join(BASE_DIR, 'artifacts')\n","\n","# 결과/플롯 저장 폴더\n","PLOT_DIR = os.path.join(ARTIFACT_DIR, 'plots')\n","CSV_DIR = os.path.join(ARTIFACT_DIR, 'results')\n","os.makedirs(DATA_DIR, exist_ok=True)\n","os.makedirs(PLOT_DIR, exist_ok=True)\n","os.makedirs(CSV_DIR, exist_ok=True)\n","\n","# =========================\n","# 2. 데이터셋 다운로드 (wget)\n","# =========================\n","# - ETT는 확실히 zhouhaoyi/ETDataset에 존재\n","# - Electricity/Weather/Traffic도 보통 같은 레포의 표준 파일명을 씁니다.\n","#   (만약 다운로드가 실패하면, 아래 URL만 사용 환경에 맞게 바꿔주면 됩니다.)\n","\n","DATASET_SPECS = {\n","    'ETTh1': {\n","        'filename': 'ETTh1.csv',\n","        'url': 'https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv',\n","        'type': 'ett_hour',\n","    },\n","    'ETTh2': {\n","        'filename': 'ETTh2.csv',\n","        'url': 'https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh2.csv',\n","        'type': 'ett_hour',\n","    },\n","    'ETTm1': {\n","        'filename': 'ETTm1.csv',\n","        'url': 'https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm1.csv',\n","        'type': 'ett_minute',\n","    },\n","    'ETTm2': {\n","        'filename': 'ETTm2.csv',\n","        'url': 'https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv',\n","        'type': 'ett_minute',\n","    },\n","    'Electricity': {\n","        'filename': 'electricity.csv',\n","        # NOTE: zhouhaoyi/ETDataset에는 electricity/weather/traffic 파일이 없음.\n","        # HuggingFace mirror를 사용(Colab에서 wget으로 바로 받기 가능)\n","        'url': 'https://huggingface.co/datasets/dunzane/time-series-dataset/resolve/main/electricity/electricity.csv',\n","        'type': 'custom',\n","    },\n","    'Weather': {\n","        'filename': 'weather.csv',\n","        'url': 'https://huggingface.co/datasets/dunzane/time-series-dataset/resolve/main/weather/weather.csv',\n","        'type': 'custom',\n","    },\n","    'Traffic': {\n","        'filename': 'traffic.csv',\n","        'url': 'https://huggingface.co/datasets/dunzane/time-series-dataset/resolve/main/traffic/traffic.csv',\n","        'type': 'custom',\n","    },\n","}\n","\n","\n","def maybe_download(url: str, out_path: str, force: bool = False):\n","    \"\"\"url을 out_path로 다운로드.\n","\n","    - force=True이면 기존 파일이 있어도 다시 받습니다.\n","    - 다운로드 중 끊기면 0 bytes 파일이 남을 수 있어서, ensure_datasets에서 size 체크로 재시도합니다.\n","    \"\"\"\n","\n","    if (not force) and os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n","        return\n","\n","    os.makedirs(os.path.dirname(out_path) or '.', exist_ok=True)\n","\n","    # quiet + retry 옵션(네트워크 불안정 대비)\n","    !wget -q --show-progress --progress=bar:force:noscroll -O {out_path} {url}\n","\n","\n","def ensure_datasets(root: str = None, force: bool = False):\n","    \"\"\"필요한 데이터셋을 root 아래에 준비합니다.\n","\n","    - 파일이 없거나, **0 bytes**이면 자동으로 재다운로드합니다.\n","    - force=True이면 전부 다시 받습니다.\n","    \"\"\"\n","\n","    root = root or DATA_DIR  # Colab 기본: /content/data\n","    os.makedirs(root, exist_ok=True)\n","\n","    for name in DATASET_NAMES:\n","        spec = DATASET_SPECS[name]\n","        out_path = os.path.join(root, spec['filename'])\n","\n","        need = force or (not os.path.exists(out_path)) or (os.path.getsize(out_path) == 0)\n","\n","        if need:\n","            print(f\"Downloading {name} -> {out_path}\")\n","            maybe_download(spec['url'], out_path, force=True)\n","\n","        # 다운로드 후 최종 점검\n","        if not os.path.exists(out_path) or os.path.getsize(out_path) == 0:\n","            raise RuntimeError(f\"Dataset file is empty or missing: {out_path}\")\n","\n","        print(f\"{name}: OK ({out_path})\")\n","\n","\n","# 필요하면 아래를 실행하세요(Colab 기본 경로: /content/data).\n","ensure_datasets()  # 또는 ensure_datasets('/content/data')"]},{"cell_type":"code","source":["# =========================\n","# 3. 데이터셋 클래스\n","# =========================\n","\n","class Dataset_ETT_Hour(Dataset):\n","    \"\"\"ETTh1/ETTh2 표준 split (12개월/4개월/4개월).\"\"\"\n","\n","    def __init__(\n","        self,\n","        root_path,\n","        flag='train',\n","        size=None,\n","        features='M',\n","        data_path='ETTh1.csv',\n","        target='OT',\n","        scale=True,\n","    ):\n","        # size = [seq_len, label_len(미사용), pred_len]\n","        self.seq_len = size[0]\n","        self.pred_len = size[2]\n","\n","        assert flag in ['train', 'val', 'test']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","\n","        self.features = features\n","        self.target = target\n","        self.scale = scale\n","        self.root_path = root_path\n","        self.data_path = data_path\n","        self.__read_data__()\n","\n","    def __read_data__(self):\n","        self.scaler = StandardScaler()\n","        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n","\n","        # 12개월 / 4개월 / 4개월\n","        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n","        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n","\n","        border1 = border1s[self.set_type]\n","        border2 = border2s[self.set_type]\n","\n","        if self.features in ['M', 'MS']:\n","            cols_data = df_raw.columns[1:]\n","            df_data = df_raw[cols_data]\n","        else:\n","            df_data = df_raw[[self.target]]\n","\n","        if self.scale:\n","            train_data = df_data[border1s[0] : border2s[0]]\n","            self.scaler.fit(train_data.values)\n","            data = self.scaler.transform(df_data.values)\n","        else:\n","            data = df_data.values\n","\n","        self.data_x = data[border1:border2]\n","        self.data_y = data[border1:border2]\n","\n","    def __getitem__(self, index):\n","        s_begin = index\n","        s_end = s_begin + self.seq_len\n","        r_begin = s_end\n","        r_end = r_begin + self.pred_len\n","\n","        seq_x = self.data_x[s_begin:s_end]\n","        seq_y = self.data_y[r_begin:r_end]\n","        return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)\n","\n","    def __len__(self):\n","        return len(self.data_x) - self.seq_len - self.pred_len + 1\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)\n","\n","\n","class Dataset_ETT_Minute(Dataset):\n","    \"\"\"ETTm1/ETTm2 표준 split.\n","\n","    - 관례적으로 (12개월/4개월/4개월) 분할을 15분 간격 샘플 수로 환산합니다.\n","    - 1일 = 24*4 = 96\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        root_path,\n","        flag='train',\n","        size=None,\n","        features='M',\n","        data_path='ETTm1.csv',\n","        target='OT',\n","        scale=True,\n","    ):\n","        self.seq_len = size[0]\n","        self.pred_len = size[2]\n","\n","        assert flag in ['train', 'val', 'test']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","\n","        self.features = features\n","        self.target = target\n","        self.scale = scale\n","        self.root_path = root_path\n","        self.data_path = data_path\n","        self.__read_data__()\n","\n","    def __read_data__(self):\n","        self.scaler = StandardScaler()\n","        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n","\n","        # 15분 간격: 1일=96\n","        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n","        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n","\n","        border1 = border1s[self.set_type]\n","        border2 = border2s[self.set_type]\n","\n","        if self.features in ['M', 'MS']:\n","            cols_data = df_raw.columns[1:]\n","            df_data = df_raw[cols_data]\n","        else:\n","            df_data = df_raw[[self.target]]\n","\n","        if self.scale:\n","            train_data = df_data[border1s[0] : border2s[0]]\n","            self.scaler.fit(train_data.values)\n","            data = self.scaler.transform(df_data.values)\n","        else:\n","            data = df_data.values\n","\n","        self.data_x = data[border1:border2]\n","        self.data_y = data[border1:border2]\n","\n","    def __getitem__(self, index):\n","        s_begin = index\n","        s_end = s_begin + self.seq_len\n","        r_begin = s_end\n","        r_end = r_begin + self.pred_len\n","\n","        seq_x = self.data_x[s_begin:s_end]\n","        seq_y = self.data_y[r_begin:r_end]\n","        return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)\n","\n","    def __len__(self):\n","        return len(self.data_x) - self.seq_len - self.pred_len + 1\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)\n","\n","\n","class Dataset_CustomCSV(Dataset):\n","    \"\"\"Electricity/Weather/Traffic 같은 일반 CSV용.\n","\n","    - 기본 가정: (date 컬럼이 있으면 첫 컬럼) + 나머지 수치 컬럼들\n","    - split: train/val/test = 0.7/0.1/0.2 (표준 비율)\n","    - scale: train 구간으로 StandardScaler fit\n","    - (옵션) max_features로 변수 수가 너무 큰 데이터셋을 안전하게 축소 가능\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        root_path,\n","        flag='train',\n","        size=None,\n","        data_path='electricity.csv',\n","        scale=True,\n","        split_ratio=(0.7, 0.1, 0.2),\n","        max_features=None,\n","    ):\n","        self.seq_len = size[0]\n","        self.pred_len = size[2]\n","\n","        assert flag in ['train', 'val', 'test']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","\n","        self.root_path = root_path\n","        self.data_path = data_path\n","        self.scale = scale\n","        self.split_ratio = split_ratio\n","        self.max_features = max_features\n","\n","        self.__read_data__()\n","\n","    def __read_data__(self):\n","        self.scaler = StandardScaler()\n","        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n","\n","        # 수치 컬럼만 안전하게 선택 (date/문자열 컬럼 제거)\n","        df_num = df_raw.select_dtypes(include=[np.number])\n","        if df_num.shape[1] == 0:\n","            # fallback: 첫 컬럼이 date라고 가정\n","            df_num = df_raw[df_raw.columns[1:]]\n","\n","        if self.max_features is not None and df_num.shape[1] > int(self.max_features):\n","            df_num = df_num.iloc[:, : int(self.max_features)]\n","\n","        data_all = df_num.values\n","        n = len(data_all)\n","\n","        tr = int(n * self.split_ratio[0])\n","        va = int(n * self.split_ratio[1])\n","        te = n - tr - va\n","\n","        border1s = [0, tr - self.seq_len, tr + va - self.seq_len]\n","        border2s = [tr, tr + va, tr + va + te]\n","\n","        border1 = border1s[self.set_type]\n","        border2 = border2s[self.set_type]\n","\n","        if self.scale:\n","            train_data = data_all[border1s[0] : border2s[0]]\n","            self.scaler.fit(train_data)\n","            data = self.scaler.transform(data_all)\n","        else:\n","            data = data_all\n","\n","        self.data_x = data[border1:border2]\n","        self.data_y = data[border1:border2]\n","\n","    def __getitem__(self, index):\n","        s_begin = index\n","        s_end = s_begin + self.seq_len\n","        r_begin = s_end\n","        r_end = r_begin + self.pred_len\n","\n","        seq_x = self.data_x[s_begin:s_end]\n","        seq_y = self.data_y[r_begin:r_end]\n","        return torch.FloatTensor(seq_x), torch.FloatTensor(seq_y)\n","\n","    def __len__(self):\n","        return len(self.data_x) - self.seq_len - self.pred_len + 1\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)\n","\n","\n","def build_dataset(\n","    name: str,\n","    root: str,\n","    flag: str,\n","    seq_len: int,\n","    pred_len: int,\n","    max_features_custom=None,\n","):\n","    spec = DATASET_SPECS[name]\n","    ds_type = spec['type']\n","    if ds_type == 'ett_hour':\n","        return Dataset_ETT_Hour(root_path=root, flag=flag, size=[seq_len, 0, pred_len], data_path=spec['filename'])\n","    if ds_type == 'ett_minute':\n","        return Dataset_ETT_Minute(root_path=root, flag=flag, size=[seq_len, 0, pred_len], data_path=spec['filename'])\n","    if ds_type == 'custom':\n","        return Dataset_CustomCSV(\n","            root_path=root,\n","            flag=flag,\n","            size=[seq_len, 0, pred_len],\n","            data_path=spec['filename'],\n","            max_features=max_features_custom,\n","        )\n","    raise ValueError(f\"Unknown dataset type: {ds_type}\")"],"metadata":{"id":"bUHhSZZJOMzG","executionInfo":{"status":"ok","timestamp":1767059042549,"user_tz":-540,"elapsed":6,"user":{"displayName":"동욱김","userId":"12228314751297835719"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 4. 모델 정의 (요청하신 7개) - 기존 노트북 코드 그대로 이식\n","# =========================\n","\n","# -------------------------------------------------\n","# (A) DecompOpTCN+TCN+Basic (1).ipynb 에 있던 4개\n","#   - TCN\n","#   - DecompOpTCN\n","#   - Decomp-OpTCN-v3(patch)\n","#   - DLinear\n","# -------------------------------------------------\n","\n","\n","class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","\n","class RevIN(nn.Module):\n","    def __init__(self, num_features: int, eps=1e-5, affine=True):\n","        super(RevIN, self).__init__()\n","        self.num_features = num_features\n","        self.eps = eps\n","        self.affine = affine\n","        if self.affine:\n","            self._init_params()\n","\n","    def _init_params(self):\n","        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n","        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n","\n","    def forward(self, x, mode: str):\n","        if mode == 'norm':\n","            self._get_statistics(x)\n","            x = self._normalize(x)\n","        elif mode == 'denorm':\n","            x = self._denormalize(x)\n","        return x\n","\n","    def _get_statistics(self, x):\n","        dim2reduce = tuple(range(1, x.ndim - 1))\n","        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n","        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n","\n","    def _normalize(self, x):\n","        x = x - self.mean\n","        x = x / self.stdev\n","        if self.affine:\n","            x = x * self.affine_weight + self.affine_bias\n","        return x\n","\n","    def _denormalize(self, x):\n","        if self.affine:\n","            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)\n","        x = x * self.stdev\n","        x = x + self.mean\n","        return x\n","\n","\n","class MovingAverage(nn.Module):\n","    def __init__(self, kernel_size, stride):\n","        super(MovingAverage, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n","\n","    def forward(self, x):\n","        # x: [Batch, Length, Channel] -> [Batch, Channel, Length]\n","        x = x.permute(0, 2, 1)\n","        # Padding\n","        front = x[:, :, 0:1].repeat(1, 1, (self.kernel_size - 1) // 2)\n","        end = x[:, :, -1:].repeat(1, 1, (self.kernel_size - 1) // 2)\n","        x = torch.cat([front, x, end], dim=-1)\n","        x = self.avg(x)\n","        x = x.permute(0, 2, 1)\n","        return x\n","\n","\n","class SeriesDecomp(nn.Module):\n","    def __init__(self, kernel_size):\n","        super(SeriesDecomp, self).__init__()\n","        self.moving_avg = MovingAverage(kernel_size, stride=1)\n","\n","    def forward(self, x):\n","        trend = self.moving_avg(x)\n","        seasonal = x - trend\n","        return seasonal, trend\n","\n","\n","class OperatorMixer(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, period=24):\n","        super(OperatorMixer, self).__init__()\n","        self.dilation = dilation\n","        self.kernel_size = kernel_size\n","\n","        # Causal Padding 계산: (kernel_size - 1) * dilation\n","        self.padding = (kernel_size - 1) * dilation\n","        self.chomp = Chomp1d(self.padding)\n","\n","        # 연산자 커널 정의 (고정 가중치)\n","        # 1차 차분\n","        self.register_buffer('diff1_kernel', torch.tensor([-1.0, 1.0]).view(1, 1, 2))\n","        # 2차 차분\n","        self.register_buffer('diff2_kernel', torch.tensor([1.0, -2.0, 1.0]).view(1, 1, 3))\n","        # 이동 평균\n","        self.register_buffer('ma_kernel', torch.ones(1, 1, kernel_size) / kernel_size)\n","        # 계절성 차분\n","        seasonal_k = torch.zeros(1, 1, period + 1)\n","        seasonal_k[0, 0, 0] = -1.0\n","        seasonal_k[0, 0, period] = 1.0\n","        self.register_buffer('seasonal_kernel', seasonal_k)\n","\n","        # 4개 연산자 결과(diff1, diff2, ma, seasonal)를 concat -> in_channels*4\n","        self.mixer = nn.Conv1d(in_channels * 4, out_channels, 1)\n","\n","    def forward(self, x):\n","        # x shape: [B, C, L]\n","        B, C, L = x.shape\n","\n","        def apply_op(kernel, pad):\n","            out = F.conv1d(\n","                x,\n","                kernel.repeat(C, 1, 1),\n","                padding=pad * self.dilation,\n","                dilation=self.dilation,\n","                groups=C,\n","            )\n","            if pad > 0:\n","                out = out[:, :, : -(pad * self.dilation)]\n","            return out\n","\n","        op1 = apply_op(self.diff1_kernel, 1)\n","        op2 = apply_op(self.diff2_kernel, 2)\n","        op3 = apply_op(self.ma_kernel, self.kernel_size - 1)\n","\n","        period = self.seasonal_kernel.shape[-1] - 1\n","        op4 = apply_op(self.seasonal_kernel, period)\n","\n","        out = torch.cat([op1, op2, op3, op4], dim=1)\n","        return self.mixer(out)\n","\n","\n","# Model 1: TCN (Baseline) + RevIN\n","class TemporalBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n","        self.chomp1 = Chomp1d(padding)\n","        self.bn1 = nn.BatchNorm1d(n_outputs)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n","        self.chomp2 = Chomp1d(padding)\n","        self.bn2 = nn.BatchNorm1d(n_outputs)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.net = nn.Sequential(\n","            self.conv1,\n","            self.chomp1,\n","            self.bn1,\n","            self.relu1,\n","            self.dropout1,\n","            self.conv2,\n","            self.chomp2,\n","            self.bn2,\n","            self.relu2,\n","            self.dropout2,\n","        )\n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TCN_Baseline(nn.Module):\n","    def __init__(self, num_inputs, seq_len, pred_len, num_channels=[32] * 4, kernel_size=5, dropout=0.3):\n","        super(TCN_Baseline, self).__init__()\n","        self.revin = RevIN(num_inputs)\n","        layers = []\n","        for i in range(len(num_channels)):\n","            dilation_size = 2**i\n","            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n","            out_channels = num_channels[i]\n","            layers += [\n","                TemporalBlock(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size,\n","                    stride=1,\n","                    dilation=dilation_size,\n","                    padding=(kernel_size - 1) * dilation_size,\n","                    dropout=dropout,\n","                )\n","            ]\n","        self.network = nn.Sequential(*layers)\n","        self.linear_time = nn.Linear(seq_len, pred_len)\n","        self.projection = nn.Linear(num_channels[-1], num_inputs)\n","\n","    def forward(self, x):\n","        x = self.revin(x, 'norm')\n","        x = x.permute(0, 2, 1)  # [B, C, L]\n","        out = self.network(x)\n","        out = self.linear_time(out)\n","        out = out.permute(0, 2, 1)\n","        out = self.projection(out)\n","        out = self.revin(out, 'denorm')\n","        return out\n","\n","\n","# Model 2: DLinear (Baseline) + RevIN\n","class DLinear(nn.Module):\n","    def __init__(self, seq_len, pred_len, num_inputs):\n","        super(DLinear, self).__init__()\n","        self.revin = RevIN(num_inputs)\n","        self.decompsition = SeriesDecomp(25)\n","        self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n","        self.Linear_Trend = nn.Linear(seq_len, pred_len)\n","\n","    def forward(self, x):\n","        x = self.revin(x, 'norm')\n","        seasonal_init, trend_init = self.decompsition(x)\n","        seasonal_init = seasonal_init.permute(0, 2, 1)\n","        trend_init = trend_init.permute(0, 2, 1)\n","        seasonal_output = self.Linear_Seasonal(seasonal_init)\n","        trend_output = self.Linear_Trend(trend_init)\n","        x = seasonal_output + trend_output\n","        x = x.permute(0, 2, 1)\n","        x = self.revin(x, 'denorm')\n","        return x\n","\n","\n","# Model 3: Decomp-OpTCN (Proposed)\n","class OpTCNBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout=0.2):\n","        super(OpTCNBlock, self).__init__()\n","        self.op_mixer1 = OperatorMixer(n_inputs, n_outputs, kernel_size, dilation)\n","        self.bn1 = nn.BatchNorm1d(n_outputs)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        self.op_mixer2 = OperatorMixer(n_outputs, n_outputs, kernel_size, dilation)\n","        self.bn2 = nn.BatchNorm1d(n_outputs)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.op_mixer1(x)\n","        out = self.bn1(out)\n","        out = self.relu1(out)\n","        out = self.dropout1(out)\n","        out = self.op_mixer2(out)\n","        out = self.bn2(out)\n","        out = self.relu2(out)\n","        out = self.dropout2(out)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class DecompOpTCN(nn.Module):\n","    def __init__(self, num_inputs, seq_len, pred_len, num_channels=[32] * 4, kernel_size=5, dropout=0.3):\n","        super(DecompOpTCN, self).__init__()\n","        self.revin = RevIN(num_inputs)\n","        self.decomp = SeriesDecomp(kernel_size=25)\n","\n","        layers = []\n","        for i in range(len(num_channels)):\n","            dilation_size = 2**i\n","            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n","            out_channels = num_channels[i]\n","            layers += [OpTCNBlock(in_channels, out_channels, kernel_size, dilation=dilation_size, dropout=dropout)]\n","        self.seasonal_backbone = nn.Sequential(*layers)\n","\n","        self.seasonal_head = nn.Linear(seq_len, pred_len)\n","        self.seasonal_proj = nn.Linear(num_channels[-1], num_inputs)\n","        self.trend_head = nn.Linear(seq_len, pred_len)\n","\n","    def forward(self, x):\n","        x = self.revin(x, 'norm')\n","        seasonal_init, trend_init = self.decomp(x)\n","\n","        trend_out = self.trend_head(trend_init.permute(0, 2, 1)).permute(0, 2, 1)\n","\n","        seasonal_x = seasonal_init.permute(0, 2, 1)\n","        seasonal_feat = self.seasonal_backbone(seasonal_x)\n","        seasonal_out = self.seasonal_head(seasonal_feat).permute(0, 2, 1)\n","        seasonal_out = self.seasonal_proj(seasonal_out)\n","\n","        x_out = seasonal_out + trend_out\n","        x_out = self.revin(x_out, 'denorm')\n","        return x_out\n","\n","\n","# ---- Decomp-OpTCN-v3(Patch) 의존 모듈들 (원본 그대로) ----\n","\n","class SpectralOperator(nn.Module):\n","    \"\"\"FFT 기반 low-pass로 global 성분을 보조 피처로 제공.\"\"\"\n","\n","    def __init__(self, k=20):\n","        super().__init__()\n","        self.k = k\n","\n","    def forward(self, x):\n","        # x: [B, C, L]\n","        B, C, L = x.shape\n","        x_dtype = x.dtype\n","\n","        x32 = x.float()\n","        x_ft = torch.fft.rfft(x32, dim=-1)\n","        if x_ft.shape[-1] > self.k:\n","            x_ft[:, :, self.k:] = 0\n","        out = torch.fft.irfft(x_ft, n=L, dim=-1)\n","        return out.to(dtype=x_dtype)\n","\n","\n","class SEScale(nn.Module):\n","    def __init__(self, channels, reduction=4):\n","        super().__init__()\n","        hidden = max(1, channels // reduction)\n","        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channels, hidden, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden, channels, bias=False),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        b, c, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1)\n","        return x * y\n","\n","\n","class AdaptiveOperatorMixerV2(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        dilation=1,\n","        periods=(24, 168),\n","        use_spectral=True,\n","        spectral_k=20,\n","        se_reduction=4,\n","    ):\n","        super().__init__()\n","        self.dilation = dilation\n","        self.kernel_size = kernel_size\n","        self.periods = tuple(periods)\n","        self.use_spectral = use_spectral\n","\n","        self.register_buffer('diff1_kernel', torch.tensor([-1.0, 1.0]).view(1, 1, 2))\n","        self.register_buffer('diff2_kernel', torch.tensor([1.0, -2.0, 1.0]).view(1, 1, 3))\n","        self.register_buffer('ma_kernel', torch.ones(1, 1, kernel_size) / kernel_size)\n","\n","        for i, p in enumerate(self.periods):\n","            k = torch.zeros(1, 1, p + 1)\n","            k[0, 0, 0], k[0, 0, p] = -1.0, 1.0\n","            self.register_buffer(f's_kernel_{i}', k)\n","\n","        if self.use_spectral:\n","            self.spectral_op = SpectralOperator(k=spectral_k)\n","\n","        self.total_ops = 1 + 2 + 1 + len(self.periods) + (1 if self.use_spectral else 0)\n","        self.op_gates = nn.Parameter(torch.ones(self.total_ops))\n","\n","        self.se_block = SEScale(in_channels * self.total_ops, reduction=se_reduction)\n","        self.mixer = nn.Conv1d(in_channels * self.total_ops, out_channels, 1)\n","\n","    def forward(self, x):\n","        # x: [B, C, L]\n","        B, C, L = x.shape\n","\n","        def apply_op(kernel, pad):\n","            out = F.conv1d(\n","                x,\n","                kernel.repeat(C, 1, 1),\n","                padding=pad * self.dilation,\n","                dilation=self.dilation,\n","                groups=C,\n","            )\n","            if pad > 0:\n","                out = out[:, :, : -(pad * self.dilation)]\n","            return out\n","\n","        ops = []\n","        ops.append(x)  # identity\n","        ops.append(apply_op(self.diff1_kernel, 1))\n","        ops.append(apply_op(self.diff2_kernel, 2))\n","        ops.append(apply_op(self.ma_kernel, self.kernel_size - 1))\n","\n","        for i in range(len(self.periods)):\n","            p_kernel = getattr(self, f's_kernel_{i}')\n","            ops.append(apply_op(p_kernel, p_kernel.shape[-1] - 1))\n","\n","        if self.use_spectral:\n","            ops.append(self.spectral_op(x))\n","\n","        out = torch.cat(ops, dim=1)  # [B, C*total_ops, L]\n","\n","        gate = torch.repeat_interleave(self.op_gates, repeats=C).to(out.dtype)\n","        out = out * gate.view(1, -1, 1)\n","\n","        out = self.se_block(out)\n","        return self.mixer(out)\n","\n","\n","class OpTCNBlockV2(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout=0.2, gn_groups=8):\n","        super().__init__()\n","        self.op_mixer1 = AdaptiveOperatorMixerV2(n_inputs, n_outputs, kernel_size, dilation=dilation)\n","        self.norm1 = nn.GroupNorm(num_groups=min(gn_groups, n_outputs), num_channels=n_outputs)\n","        self.act1 = nn.GELU()\n","        self.drop1 = nn.Dropout(dropout)\n","\n","        self.ffn1 = nn.Sequential(\n","            nn.Conv1d(n_outputs, 2 * n_outputs, 1),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Conv1d(2 * n_outputs, n_outputs, 1),\n","        )\n","\n","        self.op_mixer2 = AdaptiveOperatorMixerV2(n_outputs, n_outputs, kernel_size, dilation=dilation)\n","        self.norm2 = nn.GroupNorm(num_groups=min(gn_groups, n_outputs), num_channels=n_outputs)\n","        self.act2 = nn.GELU()\n","        self.drop2 = nn.Dropout(dropout)\n","\n","        self.ffn2 = nn.Sequential(\n","            nn.Conv1d(n_outputs, 2 * n_outputs, 1),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Conv1d(2 * n_outputs, n_outputs, 1),\n","        )\n","\n","        self.res_conv = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else nn.Identity()\n","\n","    def forward(self, x):\n","        res = self.res_conv(x)\n","\n","        out = self.op_mixer1(x)\n","        out = self.norm1(out)\n","        out = self.act1(out)\n","        out = self.drop1(out)\n","        out = out + self.ffn1(out)\n","\n","        out = self.op_mixer2(out)\n","        out = self.norm2(out)\n","        out = self.act2(out)\n","        out = self.drop2(out)\n","        out = out + self.ffn2(out)\n","\n","        return out + res\n","\n","\n","class PatchingLayer(nn.Module):\n","    \"\"\"x: [B, C, L] -> patches: [B, C, N, P]\"\"\"\n","\n","    def __init__(self, patch_len=16, stride=8):\n","        super().__init__()\n","        self.patch_len = patch_len\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        return x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n","\n","    def num_patches(self, seq_len):\n","        return (seq_len - self.patch_len) // self.stride + 1\n","\n","\n","class ChannelCorrMixer(nn.Module):\n","    \"\"\"입력/출력: [B, C, T]\"\"\"\n","\n","    def __init__(self, channels, d=32, dropout=0.1):\n","        super().__init__()\n","        self.q = nn.Linear(channels, d)\n","        self.k = nn.Linear(channels, d)\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = d ** -0.5\n","\n","    def forward(self, x):\n","        p = x.mean(dim=-1)  # [B, C]\n","        q = self.q(p)  # [B, d]\n","        k = self.k(p)  # [B, d]\n","\n","        e_q = p.unsqueeze(-1) * q.unsqueeze(1)  # [B, C, d]\n","        e_k = p.unsqueeze(-1) * k.unsqueeze(1)  # [B, C, d]\n","\n","        scores = torch.matmul(e_q, e_k.transpose(1, 2)) * self.scale  # [B, C, C]\n","        w = torch.softmax(scores, dim=-1)\n","        w = self.dropout(w)\n","\n","        out = torch.einsum('bij,bjt->bit', w, x)\n","        return out + x\n","\n","\n","class DecompOpTCN_V3_Patch(nn.Module):\n","    def __init__(\n","        self,\n","        num_inputs,\n","        seq_len,\n","        pred_len,\n","        num_channels=[32] * 4,\n","        kernel_size=5,\n","        dropout=0.3,\n","        patch_len=8,\n","        patch_stride=4,\n","        patch_embed_dim=8,\n","    ):\n","        super().__init__()\n","        self.revin = RevIN(num_inputs)\n","        self.decomp = SeriesDecomp(kernel_size=25)\n","\n","        self.patching = PatchingLayer(patch_len=patch_len, stride=patch_stride)\n","        self.n_patches = self.patching.num_patches(seq_len)\n","\n","        self.patch_embed = nn.Linear(patch_len, patch_embed_dim)\n","        self.patch_reduce = nn.Linear(patch_embed_dim, 1)\n","\n","        self.channel_corr = ChannelCorrMixer(num_inputs, d=32, dropout=0.1)\n","\n","        layers = []\n","        for i in range(len(num_channels)):\n","            dilation_size = 2**i\n","            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n","            out_channels = num_channels[i]\n","            layers += [OpTCNBlockV2(in_channels, out_channels, kernel_size, dilation=dilation_size, dropout=dropout)]\n","        self.seasonal_backbone = nn.Sequential(*layers)\n","\n","        self.seasonal_head = nn.Linear(self.n_patches, pred_len)\n","        self.seasonal_proj = nn.Linear(num_channels[-1], num_inputs)\n","\n","        hidden = max(16, seq_len // 4)\n","        self.trend_head = nn.Sequential(\n","            nn.Linear(seq_len, hidden),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden, pred_len),\n","        )\n","\n","    def forward(self, x):\n","        x = self.revin(x, 'norm')\n","        seasonal_init, trend_init = self.decomp(x)\n","\n","        trend_out = self.trend_head(trend_init.permute(0, 2, 1)).permute(0, 2, 1)\n","\n","        seasonal_x = seasonal_init.permute(0, 2, 1)  # [B, C, L]\n","        patches = self.patching(seasonal_x)  # [B, C, N, P]\n","        emb = self.patch_embed(patches)  # [B, C, N, d]\n","        tok = self.patch_reduce(emb).squeeze(-1)  # [B, C, N]\n","\n","        tok = self.channel_corr(tok)  # [B, C, N]\n","\n","        seasonal_feat = self.seasonal_backbone(tok)  # [B, C_hidden, N]\n","        seasonal_out = self.seasonal_head(seasonal_feat).permute(0, 2, 1)  # [B, H, C_hidden]\n","        seasonal_out = self.seasonal_proj(seasonal_out)  # [B, H, C]\n","\n","        out = seasonal_out + trend_out\n","        out = self.revin(out, 'denorm')\n","        return out\n","\n","\n","# -------------------------------------------------\n","# (B) ModernTCN2.ipynb 에 있던 3개\n","#   - ModernTCN\n","#   - ModernTCN+Decomp\n","#   - ModernTCN(seasonal)+Linear(trend)\n","# -------------------------------------------------\n","\n","\n","def _make_odd(k: int) -> int:\n","    return k if (k % 2 == 1) else (k + 1)\n","\n","\n","class moving_avg(nn.Module):\n","    def __init__(self, kernel_size, stride=1):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n","\n","    def forward(self, x):\n","        # x: [B, L, C]\n","        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        x = torch.cat([front, x, end], dim=1)\n","        x = self.avg(x.permute(0, 2, 1)).permute(0, 2, 1)\n","        return x\n","\n","\n","class series_decomp(nn.Module):\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.moving_avg = moving_avg(kernel_size, stride=1)\n","\n","    def forward(self, x):\n","        moving_mean = self.moving_avg(x)\n","        res = x - moving_mean\n","        return res, moving_mean\n","\n","\n","class Flatten_Head(nn.Module):\n","    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0.0):\n","        super().__init__()\n","        self.individual = individual\n","        self.n_vars = n_vars\n","\n","        if self.individual:\n","            self.linears = nn.ModuleList()\n","            self.dropouts = nn.ModuleList()\n","            self.flattens = nn.ModuleList()\n","            for _ in range(self.n_vars):\n","                self.flattens.append(nn.Flatten(start_dim=-2))\n","                self.linears.append(nn.Linear(nf, target_window))\n","                self.dropouts.append(nn.Dropout(head_dropout))\n","        else:\n","            self.flatten = nn.Flatten(start_dim=-2)\n","            self.linear = nn.Linear(nf, target_window)\n","            self.dropout = nn.Dropout(head_dropout)\n","\n","    def forward(self, x):\n","        # x: [B, nvars, d_model, patch_num]\n","        if self.individual:\n","            x_out = []\n","            for i in range(self.n_vars):\n","                z = self.flattens[i](x[:, i, :, :])\n","                z = self.linears[i](z)\n","                z = self.dropouts[i](z)\n","                x_out.append(z)\n","            x = torch.stack(x_out, dim=1)\n","        else:\n","            x = self.flatten(x)\n","            x = self.linear(x)\n","            x = self.dropout(x)\n","        return x\n","\n","\n","class RevIN_Official(nn.Module):\n","    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n","        super().__init__()\n","        self.num_features = num_features\n","        self.eps = eps\n","        self.affine = affine\n","        self.subtract_last = subtract_last\n","        if self.affine:\n","            self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n","            self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n","\n","    def forward(self, x, mode: str):\n","        if mode == 'norm':\n","            self._get_statistics(x)\n","            x = self._normalize(x)\n","        elif mode == 'denorm':\n","            x = self._denormalize(x)\n","        else:\n","            raise NotImplementedError\n","        return x\n","\n","    def _get_statistics(self, x):\n","        dim2reduce = tuple(range(1, x.ndim - 1))\n","        if self.subtract_last:\n","            self.last = x[:, -1, :].unsqueeze(1)\n","        else:\n","            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n","        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n","\n","    def _normalize(self, x):\n","        if self.subtract_last:\n","            x = x - self.last\n","        else:\n","            x = x - self.mean\n","        x = x / self.stdev\n","        if self.affine:\n","            x = x * self.affine_weight\n","            x = x + self.affine_bias\n","        return x\n","\n","    def _denormalize(self, x):\n","        if self.affine:\n","            x = x - self.affine_bias\n","            x = x / (self.affine_weight + self.eps * self.eps)\n","        x = x * self.stdev\n","        if self.subtract_last:\n","            x = x + self.last\n","        else:\n","            x = x + self.mean\n","        return x\n","\n","\n","def _get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n","    return nn.Conv1d(\n","        in_channels=in_channels,\n","        out_channels=out_channels,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        padding=padding,\n","        dilation=dilation,\n","        groups=groups,\n","        bias=bias,\n","    )\n","\n","\n","def _conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1, bias=False):\n","    if padding is None:\n","        padding = kernel_size // 2\n","    return nn.Sequential(\n","        _get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=bias),\n","        nn.BatchNorm1d(out_channels),\n","    )\n","\n","\n","def _fuse_bn(conv: nn.Conv1d, bn: nn.BatchNorm1d):\n","    kernel = conv.weight\n","    running_mean = bn.running_mean\n","    running_var = bn.running_var\n","    gamma = bn.weight\n","    beta = bn.bias\n","    eps = bn.eps\n","    std = (running_var + eps).sqrt()\n","    t = (gamma / std).reshape(-1, 1, 1)\n","    return kernel * t, beta - running_mean * gamma / std\n","\n","\n","def _pad_two_edge_1d(w, pad_left: int, pad_right: int):\n","    if pad_left == 0 and pad_right == 0:\n","        return w\n","    left = torch.zeros(w.shape[0], w.shape[1], pad_left, device=w.device, dtype=w.dtype)\n","    right = torch.zeros(w.shape[0], w.shape[1], pad_right, device=w.device, dtype=w.dtype)\n","    return torch.cat([left, w, right], dim=-1)\n","\n","\n","class ReparamLargeKernelConv(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        stride,\n","        groups,\n","        small_kernel,\n","        small_kernel_merged=False,\n","    ):\n","        super().__init__()\n","        kernel_size = _make_odd(int(kernel_size))\n","        small_kernel = _make_odd(int(small_kernel)) if small_kernel is not None else None\n","\n","        self.kernel_size = kernel_size\n","        self.small_kernel = small_kernel\n","\n","        padding = kernel_size // 2\n","        if small_kernel_merged:\n","            self.lkb_reparam = nn.Conv1d(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                dilation=1,\n","                groups=groups,\n","                bias=True,\n","            )\n","        else:\n","            self.lkb_origin = _conv_bn(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                kernel_size=kernel_size,\n","                stride=stride,\n","                padding=padding,\n","                groups=groups,\n","                dilation=1,\n","                bias=False,\n","            )\n","            if small_kernel is not None:\n","                assert small_kernel <= kernel_size\n","                self.small_conv = _conv_bn(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=small_kernel,\n","                    stride=stride,\n","                    padding=small_kernel // 2,\n","                    groups=groups,\n","                    dilation=1,\n","                    bias=False,\n","                )\n","\n","    def forward(self, x):\n","        if hasattr(self, 'lkb_reparam'):\n","            return self.lkb_reparam(x)\n","        out = self.lkb_origin(x)\n","        if hasattr(self, 'small_conv'):\n","            out = out + self.small_conv(x)\n","        return out\n","\n","    def get_equivalent_kernel_bias(self):\n","        eq_k, eq_b = _fuse_bn(self.lkb_origin[0], self.lkb_origin[1])\n","        if hasattr(self, 'small_conv'):\n","            small_k, small_b = _fuse_bn(self.small_conv[0], self.small_conv[1])\n","            eq_b = eq_b + small_b\n","            pad = (self.kernel_size - self.small_kernel) // 2\n","            eq_k = eq_k + _pad_two_edge_1d(small_k, pad, pad)\n","        return eq_k, eq_b\n","\n","    @torch.no_grad()\n","    def merge_kernel(self):\n","        if hasattr(self, 'lkb_reparam'):\n","            return\n","        eq_k, eq_b = self.get_equivalent_kernel_bias()\n","        conv: nn.Conv1d = self.lkb_origin[0]\n","        self.lkb_reparam = nn.Conv1d(\n","            in_channels=conv.in_channels,\n","            out_channels=conv.out_channels,\n","            kernel_size=conv.kernel_size,\n","            stride=conv.stride,\n","            padding=conv.padding,\n","            dilation=conv.dilation,\n","            groups=conv.groups,\n","            bias=True,\n","        )\n","        self.lkb_reparam.weight.data = eq_k\n","        self.lkb_reparam.bias.data = eq_b\n","        delattr(self, 'lkb_origin')\n","        if hasattr(self, 'small_conv'):\n","            delattr(self, 'small_conv')\n","\n","\n","class ModernTCN_Block_Official(nn.Module):\n","    def __init__(self, large_size, small_size, dmodel, dff, nvars, small_kernel_merged=False, drop=0.1):\n","        super().__init__()\n","        self.dw = ReparamLargeKernelConv(\n","            in_channels=nvars * dmodel,\n","            out_channels=nvars * dmodel,\n","            kernel_size=large_size,\n","            stride=1,\n","            groups=nvars * dmodel,\n","            small_kernel=small_size,\n","            small_kernel_merged=small_kernel_merged,\n","        )\n","        self.norm = nn.BatchNorm1d(dmodel)\n","\n","        # ConvFFN1 (variable-independent)\n","        self.ffn1pw1 = nn.Conv1d(nvars * dmodel, nvars * dff, kernel_size=1, stride=1, padding=0, groups=nvars)\n","        self.ffn1act = nn.GELU()\n","        self.ffn1pw2 = nn.Conv1d(nvars * dff, nvars * dmodel, kernel_size=1, stride=1, padding=0, groups=nvars)\n","        self.ffn1drop1 = nn.Dropout(drop)\n","        self.ffn1drop2 = nn.Dropout(drop)\n","\n","        # ConvFFN2 (cross-variable)\n","        self.ffn2pw1 = nn.Conv1d(nvars * dmodel, nvars * dff, kernel_size=1, stride=1, padding=0, groups=dmodel)\n","        self.ffn2act = nn.GELU()\n","        self.ffn2pw2 = nn.Conv1d(nvars * dff, nvars * dmodel, kernel_size=1, stride=1, padding=0, groups=dmodel)\n","        self.ffn2drop1 = nn.Dropout(drop)\n","        self.ffn2drop2 = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        # x: [B, M, D, N]\n","        shortcut = x\n","        B, M, D, N = x.shape\n","\n","        # 1) DWConv\n","        x = x.reshape(B, M * D, N)\n","        x = self.dw(x)\n","        x = x.reshape(B, M, D, N)\n","\n","        # 2) BN over D per-variable\n","        x = x.reshape(B * M, D, N)\n","        x = self.norm(x)\n","        x = x.reshape(B, M, D, N)\n","\n","        # 3) ConvFFN1\n","        x = x.reshape(B, M * D, N)\n","        x = self.ffn1drop1(self.ffn1pw1(x))\n","        x = self.ffn1act(x)\n","        x = self.ffn1drop2(self.ffn1pw2(x))\n","        x = x.reshape(B, M, D, N)\n","\n","        # 4) ConvFFN2\n","        x = x.permute(0, 2, 1, 3).contiguous()  # [B, D, M, N]\n","        x = x.reshape(B, D * M, N)\n","        x = self.ffn2drop1(self.ffn2pw1(x))\n","        x = self.ffn2act(x)\n","        x = self.ffn2drop2(self.ffn2pw2(x))\n","        x = x.reshape(B, D, M, N)\n","        x = x.permute(0, 2, 1, 3).contiguous()  # [B, M, D, N]\n","\n","        return shortcut + x\n","\n","\n","class ModernTCN_Stage_Official(nn.Module):\n","    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, nvars, small_kernel_merged=False, drop=0.1):\n","        super().__init__()\n","        d_ffn = dmodel * ffn_ratio\n","        self.blocks = nn.ModuleList(\n","            [\n","                ModernTCN_Block_Official(\n","                    large_size=large_size,\n","                    small_size=small_size,\n","                    dmodel=dmodel,\n","                    dff=d_ffn,\n","                    nvars=nvars,\n","                    small_kernel_merged=small_kernel_merged,\n","                    drop=drop,\n","                )\n","                for _ in range(num_blocks)\n","            ]\n","        )\n","\n","    def forward(self, x):\n","        for blk in self.blocks:\n","            x = blk(x)\n","        return x\n","\n","\n","class ModernTCN_Official(nn.Module):\n","    \"\"\"입력: x [B, M, L] / 출력: y [B, M, target_window]\"\"\"\n","\n","    def __init__(\n","        self,\n","        patch_size,\n","        patch_stride,\n","        downsample_ratio,\n","        ffn_ratio,\n","        num_blocks,\n","        large_size,\n","        small_size,\n","        dims,\n","        nvars,\n","        small_kernel_merged=False,\n","        backbone_dropout=0.1,\n","        head_dropout=0.1,\n","        use_multi_scale=False,\n","        revin=True,\n","        affine=True,\n","        subtract_last=False,\n","        seq_len=96,\n","        individual=False,\n","        target_window=96,\n","    ):\n","        super().__init__()\n","\n","        self.revin = revin\n","        if self.revin:\n","            self.revin_layer = RevIN_Official(nvars, affine=affine, subtract_last=subtract_last)\n","\n","        self.downsample_layers = nn.ModuleList()\n","        self.patch_size = patch_size\n","        self.patch_stride = patch_stride\n","        self.downsample_ratio = downsample_ratio\n","\n","        stem = nn.Sequential(\n","            nn.Conv1d(1, dims[0], kernel_size=patch_size, stride=patch_stride),\n","            nn.BatchNorm1d(dims[0]),\n","        )\n","        self.downsample_layers.append(stem)\n","\n","        for i in range(3):\n","            downsample_layer = nn.Sequential(\n","                nn.BatchNorm1d(dims[i]),\n","                nn.Conv1d(dims[i], dims[i + 1], kernel_size=downsample_ratio, stride=downsample_ratio),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.num_stage = len(num_blocks)\n","        self.stages = nn.ModuleList(\n","            [\n","                ModernTCN_Stage_Official(\n","                    ffn_ratio=ffn_ratio,\n","                    num_blocks=num_blocks[stage_idx],\n","                    large_size=large_size[stage_idx],\n","                    small_size=small_size[stage_idx],\n","                    dmodel=dims[stage_idx],\n","                    nvars=nvars,\n","                    small_kernel_merged=small_kernel_merged,\n","                    drop=backbone_dropout,\n","                )\n","                for stage_idx in range(self.num_stage)\n","            ]\n","        )\n","\n","        patch_num = seq_len // patch_stride\n","        self.n_vars = nvars\n","        self.individual = individual\n","        d_model = dims[-1]\n","\n","        self.use_multi_scale = use_multi_scale\n","        if self.use_multi_scale:\n","            self.head_nf = d_model * patch_num\n","        else:\n","            div = pow(downsample_ratio, (self.num_stage - 1))\n","            if patch_num % div == 0:\n","                self.head_nf = d_model * patch_num // div\n","            else:\n","                self.head_nf = d_model * (patch_num // div + 1)\n","\n","        self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n","\n","    def forward_feature(self, x):\n","        # x: [B,M,L]\n","        B, M, L = x.shape\n","        x = x.unsqueeze(-2)  # [B,M,1,L]\n","\n","        for i in range(self.num_stage):\n","            B, M, D, N = x.shape\n","            x = x.reshape(B * M, D, N)\n","\n","            if i == 0:\n","                if self.patch_size != self.patch_stride:\n","                    pad_len = self.patch_size - self.patch_stride\n","                    pad = x[:, :, -1:].repeat(1, 1, pad_len)\n","                    x = torch.cat([x, pad], dim=-1)\n","            else:\n","                if N % self.downsample_ratio != 0:\n","                    pad_len = self.downsample_ratio - (N % self.downsample_ratio)\n","                    x = torch.cat([x, x[:, :, -pad_len:]], dim=-1)\n","\n","            x = self.downsample_layers[i](x)\n","            _, D_, N_ = x.shape\n","            x = x.reshape(B, M, D_, N_)\n","            x = self.stages[i](x)\n","\n","        return x\n","\n","    def forward(self, x):\n","        if self.revin:\n","            x = x.permute(0, 2, 1)\n","            x = self.revin_layer(x, 'norm')\n","            x = x.permute(0, 2, 1)\n","\n","        feat = self.forward_feature(x)\n","        y = self.head(feat)\n","\n","        if self.revin:\n","            y = y.permute(0, 2, 1)\n","            y = self.revin_layer(y, 'denorm')\n","            y = y.permute(0, 2, 1)\n","\n","        return y\n","\n","    def structural_reparam(self):\n","        for m in self.modules():\n","            if hasattr(m, 'merge_kernel'):\n","                m.merge_kernel()\n","\n","\n","class ModernTCN_Official_Wrapper(nn.Module):\n","    \"\"\"입력 [B,L,C] -> 출력 [B,H,C] wrapper.\"\"\"\n","\n","    def __init__(\n","        self,\n","        seq_len: int,\n","        pred_len: int,\n","        enc_in: int,\n","        decomposition: bool = False,\n","        decomp_kernel: int = 25,\n","        patch_size: int = 16,\n","        patch_stride: int = 16,\n","        downsample_ratio: int = 2,\n","        ffn_ratio: int = 2,\n","        num_blocks=(1, 1, 1, 1),\n","        large_size=(51, 51, 51, 51),\n","        small_size=(7, 7, 7, 7),\n","        dims=(16, 16, 16, 16),\n","        small_kernel_merged: bool = False,\n","        dropout: float = 0.1,\n","        head_dropout: float = 0.0,\n","        use_multi_scale: bool = False,\n","        revin: bool = True,\n","        affine: bool = True,\n","        subtract_last: bool = False,\n","        individual: bool = False,\n","    ):\n","        super().__init__()\n","\n","        self.decomposition = decomposition\n","        if self.decomposition:\n","            self.decomp_module = series_decomp(decomp_kernel)\n","            self.model_res = ModernTCN_Official(\n","                patch_size=patch_size,\n","                patch_stride=patch_stride,\n","                downsample_ratio=downsample_ratio,\n","                ffn_ratio=ffn_ratio,\n","                num_blocks=list(num_blocks),\n","                large_size=list(large_size),\n","                small_size=list(small_size),\n","                dims=list(dims),\n","                nvars=enc_in,\n","                small_kernel_merged=small_kernel_merged,\n","                backbone_dropout=dropout,\n","                head_dropout=head_dropout,\n","                use_multi_scale=use_multi_scale,\n","                revin=revin,\n","                affine=affine,\n","                subtract_last=subtract_last,\n","                seq_len=seq_len,\n","                individual=individual,\n","                target_window=pred_len,\n","            )\n","            self.model_trend = ModernTCN_Official(\n","                patch_size=patch_size,\n","                patch_stride=patch_stride,\n","                downsample_ratio=downsample_ratio,\n","                ffn_ratio=ffn_ratio,\n","                num_blocks=list(num_blocks),\n","                large_size=list(large_size),\n","                small_size=list(small_size),\n","                dims=list(dims),\n","                nvars=enc_in,\n","                small_kernel_merged=small_kernel_merged,\n","                backbone_dropout=dropout,\n","                head_dropout=head_dropout,\n","                use_multi_scale=use_multi_scale,\n","                revin=revin,\n","                affine=affine,\n","                subtract_last=subtract_last,\n","                seq_len=seq_len,\n","                individual=individual,\n","                target_window=pred_len,\n","            )\n","        else:\n","            self.model = ModernTCN_Official(\n","                patch_size=patch_size,\n","                patch_stride=patch_stride,\n","                downsample_ratio=downsample_ratio,\n","                ffn_ratio=ffn_ratio,\n","                num_blocks=list(num_blocks),\n","                large_size=list(large_size),\n","                small_size=list(small_size),\n","                dims=list(dims),\n","                nvars=enc_in,\n","                small_kernel_merged=small_kernel_merged,\n","                backbone_dropout=dropout,\n","                head_dropout=head_dropout,\n","                use_multi_scale=use_multi_scale,\n","                revin=revin,\n","                affine=affine,\n","                subtract_last=subtract_last,\n","                seq_len=seq_len,\n","                individual=individual,\n","                target_window=pred_len,\n","            )\n","\n","    def forward(self, x):\n","        # x: [B,L,C]\n","        if self.decomposition:\n","            res_init, trend_init = self.decomp_module(x)\n","            res_init = res_init.permute(0, 2, 1)\n","            trend_init = trend_init.permute(0, 2, 1)\n","            res = self.model_res(res_init)\n","            trend = self.model_trend(trend_init)\n","            y = res + trend\n","            return y.permute(0, 2, 1)\n","        x_c = x.permute(0, 2, 1)\n","        y = self.model(x_c)\n","        return y.permute(0, 2, 1)\n","\n","    @torch.no_grad()\n","    def structural_reparam(self):\n","        if self.decomposition:\n","            self.model_res.structural_reparam()\n","            self.model_trend.structural_reparam()\n","        else:\n","            self.model.structural_reparam()\n","\n","\n","class ModernTCN_Seasonal_LinearTrend(nn.Module):\n","    def __init__(\n","        self,\n","        seq_len: int,\n","        pred_len: int,\n","        enc_in: int,\n","        decomp_kernel: int = 25,\n","        patch_size: int = 16,\n","        patch_stride: int = 16,\n","        downsample_ratio: int = 2,\n","        ffn_ratio: int = 2,\n","        num_blocks=(1, 1, 1, 1),\n","        large_size=(51, 51, 51, 51),\n","        small_size=(7, 7, 7, 7),\n","        dims=(16, 16, 16, 16),\n","        small_kernel_merged: bool = False,\n","        dropout: float = 0.1,\n","        head_dropout: float = 0.0,\n","        use_multi_scale: bool = False,\n","        revin: bool = True,\n","        affine: bool = True,\n","        subtract_last: bool = False,\n","        individual: bool = False,\n","    ):\n","        super().__init__()\n","\n","        self.revin = revin\n","        if self.revin:\n","            self.revin_layer = RevIN_Official(enc_in, affine=affine, subtract_last=subtract_last)\n","\n","        self.decomp = series_decomp(decomp_kernel)\n","\n","        self.trend_head = nn.Linear(seq_len, pred_len)\n","\n","        self.seasonal_model = ModernTCN_Official(\n","            patch_size=patch_size,\n","            patch_stride=patch_stride,\n","            downsample_ratio=downsample_ratio,\n","            ffn_ratio=ffn_ratio,\n","            num_blocks=list(num_blocks),\n","            large_size=list(large_size),\n","            small_size=list(small_size),\n","            dims=list(dims),\n","            nvars=enc_in,\n","            small_kernel_merged=small_kernel_merged,\n","            backbone_dropout=dropout,\n","            head_dropout=head_dropout,\n","            use_multi_scale=use_multi_scale,\n","            revin=False,\n","            affine=affine,\n","            subtract_last=subtract_last,\n","            seq_len=seq_len,\n","            individual=individual,\n","            target_window=pred_len,\n","        )\n","\n","    def forward(self, x):\n","        # x: [B,L,C]\n","        if self.revin:\n","            x = self.revin_layer(x, 'norm')\n","\n","        res, trend = self.decomp(x)\n","\n","        trend_out = self.trend_head(trend.permute(0, 2, 1)).permute(0, 2, 1)\n","\n","        seasonal_out = self.seasonal_model(res.permute(0, 2, 1))\n","        seasonal_out = seasonal_out.permute(0, 2, 1)\n","\n","        out = seasonal_out + trend_out\n","\n","        if self.revin:\n","            out = self.revin_layer(out, 'denorm')\n","\n","        return out"],"metadata":{"id":"_CCWDJCLOKwa","executionInfo":{"status":"ok","timestamp":1767059043888,"user_tz":-540,"elapsed":106,"user":{"displayName":"동욱김","userId":"12228314751297835719"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 5. 학습/평가 유틸 (MSE/MAE)  +  6. 실험 러너\n","# =========================\n","# Colab에서 위→아래로 실행해도 바로 돌아가도록,\n","# 원래 뒤쪽(셀 5)에 있던 학습/평가 유틸을 이 셀 상단으로 올려둡니다.\n","\n","\n","def _inverse_transform_3d(arr_3d, scaler):\n","    \"\"\"arr_3d: [N, T, C]\"\"\"\n","    if scaler is None:\n","        return arr_3d\n","    N, T, C = arr_3d.shape\n","    return scaler.inverse_transform(arr_3d.reshape(-1, C)).reshape(N, T, C)\n","\n","\n","def _calc_metrics(preds, trues):\n","    mse = float(np.mean((preds - trues) ** 2))\n","    mae = float(np.mean(np.abs(preds - trues)))\n","    return mse, mae\n","\n","\n","def train_model(\n","    model,\n","    train_loader,\n","    val_loader,\n","    epochs=30,\n","    lr=1e-3,\n","    weight_decay=1e-4,\n","    patience=5,\n","    min_epochs=5,\n","    grad_clip=1.0,\n","    use_amp=True,\n","):\n","    criterion = nn.MSELoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-5)\n","\n","    model.to(device)\n","\n","    if hasattr(torch, 'amp') and hasattr(torch.amp, 'GradScaler'):\n","        scaler_amp = torch.amp.GradScaler(enabled=bool(use_amp and device.type == 'cuda'))\n","    else:\n","        scaler_amp = torch.cuda.amp.GradScaler(enabled=bool(use_amp and device.type == 'cuda'))\n","\n","    best_state = None\n","    best_val = float('inf')\n","    wait = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_losses = []\n","\n","        for batch_x, batch_y in train_loader:\n","            batch_x = batch_x.to(device).float()\n","            batch_y = batch_y.to(device).float()\n","\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            if hasattr(torch, 'amp') and hasattr(torch.amp, 'autocast'):\n","                autocast_ctx = torch.amp.autocast(device_type=device.type, enabled=bool(use_amp and device.type == 'cuda'))\n","            else:\n","                autocast_ctx = torch.cuda.amp.autocast(enabled=bool(use_amp and device.type == 'cuda'))\n","\n","            with autocast_ctx:\n","                outputs = model(batch_x)\n","                loss = criterion(outputs, batch_y)\n","\n","            scaler_amp.scale(loss).backward()\n","\n","            if grad_clip is not None:\n","                scaler_amp.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            scaler_amp.step(optimizer)\n","            scaler_amp.update()\n","\n","            train_losses.append(loss.item())\n","\n","        model.eval()\n","        val_losses = []\n","        with torch.no_grad():\n","            for batch_x, batch_y in val_loader:\n","                batch_x = batch_x.to(device).float()\n","                batch_y = batch_y.to(device).float()\n","                outputs = model(batch_x)\n","                val_loss = criterion(outputs, batch_y)\n","                val_losses.append(val_loss.item())\n","\n","        avg_val = float(np.mean(val_losses))\n","        scheduler.step(avg_val)\n","\n","        if avg_val < best_val:\n","            best_val = avg_val\n","            best_state = deepcopy(model.state_dict())\n","            wait = 0\n","        else:\n","            wait += 1\n","\n","        if (epoch + 1) >= min_epochs and wait >= patience:\n","            break\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    return model\n","\n","\n","def evaluate_model(model, data_loader, scaler=None, capture_sample=True):\n","    model.eval()\n","    preds = []\n","    trues = []\n","\n","    sample = None\n","\n","    with torch.no_grad():\n","        for i, (batch_x, batch_y) in enumerate(data_loader):\n","            batch_x = batch_x.to(device).float()\n","            batch_y = batch_y.to(device).float()\n","            outputs = model(batch_x)\n","\n","            pred = outputs.detach().cpu().numpy()\n","            true = batch_y.detach().cpu().numpy()\n","\n","            if capture_sample and i == 0:\n","                inp = batch_x.detach().cpu().numpy()\n","                inp_u = _inverse_transform_3d(inp, scaler)\n","                pred_u = _inverse_transform_3d(pred, scaler)\n","                true_u = _inverse_transform_3d(true, scaler)\n","\n","                sample = {\n","                    'input': inp_u[0, :, -1],\n","                    'true': true_u[0, :, -1],\n","                    'pred': pred_u[0, :, -1],\n","                }\n","\n","            preds.append(pred)\n","            trues.append(true)\n","\n","    preds = np.concatenate(preds, axis=0)\n","    trues = np.concatenate(trues, axis=0)\n","\n","    mse_s, mae_s = _calc_metrics(preds, trues)\n","\n","    preds_u = _inverse_transform_3d(preds, scaler)\n","    trues_u = _inverse_transform_3d(trues, scaler)\n","    mse_u, mae_u = _calc_metrics(preds_u, trues_u)\n","\n","    return {\n","        'mse_scaled': mse_s,\n","        'mae_scaled': mae_s,\n","        'mse_unscaled': mse_u,\n","        'mae_unscaled': mae_u,\n","        'sample': sample,\n","    }\n","\n","\n","# =========================\n","# 6. 실험 러너 (데이터셋 x 모델 x input_len x pred_len)\n","# =========================\n","\n","# 큰 데이터셋(Electricity/Traffic 등)에서 변수 수가 너무 크면 모델이 매우 무거워질 수 있어\n","# 기본값으로는 custom 데이터셋 변수 수를 16으로 제한해두었습니다.\n","# - 원본 풀 변수를 쓰고 싶으면: CUSTOM_MAX_FEATURES = None\n","CUSTOM_MAX_FEATURES = 16\n","\n","\n","def make_model(model_name: str, num_inputs: int, seq_len: int, pred_len: int):\n","    if model_name == 'TCN':\n","        return TCN_Baseline(num_inputs, seq_len, pred_len, num_channels=[32] * 4, kernel_size=5)\n","\n","    if model_name == 'DLinear':\n","        return DLinear(seq_len, pred_len, num_inputs)\n","\n","    if model_name == 'DecompOpTCN':\n","        return DecompOpTCN(num_inputs, seq_len, pred_len, num_channels=[32] * 4, kernel_size=5)\n","\n","    if model_name == 'Decomp-OpTCN-v3(Patch)':\n","        return DecompOpTCN_V3_Patch(\n","            num_inputs,\n","            seq_len,\n","            pred_len,\n","            num_channels=[32] * 4,\n","            kernel_size=5,\n","            patch_len=16,\n","            patch_stride=8,\n","            patch_embed_dim=16,\n","        )\n","\n","    common_cfg = dict(\n","        seq_len=seq_len,\n","        enc_in=num_inputs,\n","        patch_size=16,\n","        patch_stride=16,\n","        downsample_ratio=2,\n","        ffn_ratio=2,\n","        num_blocks=(1, 1, 1, 1),\n","        large_size=(51, 51, 51, 51),\n","        small_size=(7, 7, 7, 7),\n","        dims=(16, 16, 16, 16),\n","        small_kernel_merged=False,\n","        dropout=0.1,\n","        head_dropout=0.0,\n","        use_multi_scale=False,\n","        revin=True,\n","        affine=True,\n","        subtract_last=False,\n","        individual=False,\n","    )\n","\n","    if model_name == 'ModernTCN':\n","        return ModernTCN_Official_Wrapper(pred_len=pred_len, decomposition=False, **common_cfg)\n","\n","    if model_name == 'ModernTCN+Decomp':\n","        return ModernTCN_Official_Wrapper(pred_len=pred_len, decomposition=True, decomp_kernel=25, **common_cfg)\n","\n","    if model_name == 'ModernTCN(seasonal)+Linear(trend)':\n","        return ModernTCN_Seasonal_LinearTrend(pred_len=pred_len, decomp_kernel=25, **common_cfg)\n","\n","    raise ValueError(f\"Unknown model_name: {model_name}\")\n","\n","\n","MODEL_NAMES = [\n","    'TCN',\n","    'DecompOpTCN',\n","    'Decomp-OpTCN-v3(Patch)',\n","    'DLinear',\n","    'ModernTCN',\n","    'ModernTCN+Decomp',\n","    'ModernTCN(seasonal)+Linear(trend)',\n","]\n","\n","\n","def run_grid(\n","    data_root: str = None,\n","    dataset_names=None,\n","    model_names=None,\n","    input_lens=None,\n","    pred_lens=None,\n","    batch_size: int = BATCH_SIZE,\n","    epochs: int = EPOCHS,\n","    patience: int = PATIENCE,\n","    min_epochs: int = MIN_EPOCHS,\n","    lr: float = LR,\n","    weight_decay: float = WEIGHT_DECAY,\n","    grad_clip: float = GRAD_CLIP,\n","    use_amp: bool = USE_AMP,\n","    resume: bool = True,\n","):\n","    \"\"\"전체 그리드 실험을 실행하고, 결과 DF와 sample(dict)를 반환합니다.\n","\n","    - resume=True이면, 이전 실행의 `results_running.csv`를 읽어 이미 완료된 조합은 스킵하고 이어서 실행합니다.\n","    - 단, **학습 epoch 중간부터 이어붙이기**는 체크포인트가 없어서 지원하지 않습니다(그리드 단위 resume).\n","    \"\"\"\n","\n","    data_root = data_root or DATA_DIR  # Colab 기본: /content/data\n","\n","    dataset_names = dataset_names or DATASET_NAMES\n","    model_names = model_names or MODEL_NAMES\n","    input_lens = input_lens or INPUT_LENS\n","    pred_lens = pred_lens or PRED_LENS\n","\n","    # 결과 저장 폴더\n","    SAMPLE_DIR = os.path.join(ARTIFACT_DIR, 'samples')\n","    os.makedirs(SAMPLE_DIR, exist_ok=True)\n","\n","    running_csv = os.path.join(CSV_DIR, 'results_running.csv')\n","\n","    results = []\n","    done_keys = set()\n","\n","    if resume and os.path.exists(running_csv):\n","        try:\n","            df_prev = pd.read_csv(running_csv)\n","            # schema 체크\n","            need_cols = {'Dataset', 'Model', 'InputLen', 'PredLen'}\n","            if need_cols.issubset(set(df_prev.columns)):\n","                results = df_prev.to_dict('records')\n","                for _, r in df_prev[['Dataset', 'Model', 'InputLen', 'PredLen']].iterrows():\n","                    done_keys.add((str(r['Dataset']), str(r['Model']), int(r['InputLen']), int(r['PredLen'])))\n","                print(f\"[resume] loaded {len(results)} rows from {running_csv}\")\n","            else:\n","                print(f\"[resume] found {running_csv} but columns mismatch -> start fresh\")\n","        except Exception as e:\n","            print(f\"[resume] failed to read {running_csv} ({e}) -> start fresh\")\n","\n","    samples = {}  # 이번 실행에서 새로 계산한 샘플들만 메모리에 쌓음\n","\n","    print(f\"Running grid: datasets={len(dataset_names)}, models={len(model_names)}, in={input_lens}, out={pred_lens}\")\n","    print(f\"DATA_DIR={data_root}\")\n","    print(f\"CUSTOM_MAX_FEATURES={CUSTOM_MAX_FEATURES} (custom datasets only)\")\n","\n","    for dname in dataset_names:\n","        for seq_len in input_lens:\n","            for pred_len in pred_lens:\n","                train_ds = build_dataset(dname, root=data_root, flag='train', seq_len=seq_len, pred_len=pred_len, max_features_custom=CUSTOM_MAX_FEATURES)\n","                val_ds = build_dataset(dname, root=data_root, flag='val', seq_len=seq_len, pred_len=pred_len, max_features_custom=CUSTOM_MAX_FEATURES)\n","                test_ds = build_dataset(dname, root=data_root, flag='test', seq_len=seq_len, pred_len=pred_len, max_features_custom=CUSTOM_MAX_FEATURES)\n","\n","                num_inputs = int(train_ds[0][0].shape[-1])\n","                scaler = train_ds.scaler\n","\n","                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n","                val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n","                test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n","\n","                for mname in model_names:\n","                    # resume: 이미 끝난 조합은 스킵\n","                    k = (dname, mname, int(seq_len), int(pred_len))\n","                    if k in done_keys:\n","                        continue\n","\n","                    setup_seed(2025)\n","                    model = make_model(mname, num_inputs, seq_len, pred_len)\n","\n","                    print(f\"[{dname}] in={seq_len} out={pred_len} vars={num_inputs} | {mname} ...\", end=' ')\n","                    t0 = time.time()\n","\n","                    model = train_model(\n","                        model,\n","                        train_loader,\n","                        val_loader,\n","                        epochs=epochs,\n","                        lr=lr,\n","                        weight_decay=weight_decay,\n","                        patience=patience,\n","                        min_epochs=min_epochs,\n","                        grad_clip=grad_clip,\n","                        use_amp=use_amp,\n","                    )\n","\n","                    dt = time.time() - t0\n","                    metrics = evaluate_model(model, test_loader, scaler=scaler, capture_sample=True)\n","\n","                    row = {\n","                        'Dataset': dname,\n","                        'Model': mname,\n","                        'InputLen': int(seq_len),\n","                        'PredLen': int(pred_len),\n","                        'NumVars': int(num_inputs),\n","                        'MSE_scaled': metrics['mse_scaled'],\n","                        'MAE_scaled': metrics['mae_scaled'],\n","                        'MSE_unscaled': metrics['mse_unscaled'],\n","                        'MAE_unscaled': metrics['mae_unscaled'],\n","                        'TrainSeconds': float(dt),\n","                    }\n","                    results.append(row)\n","                    done_keys.add((dname, mname, int(seq_len), int(pred_len)))\n","\n","                    print(f\"done {dt:.1f}s | MSE={row['MSE_scaled']:.4f}, MAE={row['MAE_scaled']:.4f} (scaled)\")\n","\n","                    key = (dname, mname, seq_len, pred_len)\n","                    samples[key] = metrics['sample']\n","\n","                    # 1) 메트릭 중간 저장\n","                    pd.DataFrame(results).to_csv(os.path.join(CSV_DIR, 'results_running.csv'), index=False)\n","\n","                    # 2) 샘플 inference 저장 (런타임 종료/재시작 대비)\n","                    # - 각 run마다 1개 샘플(마지막 변수)만 저장\n","                    s = metrics['sample']\n","                    if s is not None:\n","                        # 파일명 안전화\n","                        safe_m = mname.replace('/', '_')\n","                        out_path = os.path.join(\n","                            SAMPLE_DIR,\n","                            f\"{dname}__{safe_m}__in{seq_len}__out{pred_len}.npz\",\n","                        )\n","                        np.savez_compressed(out_path, input=s['input'], true=s['true'], pred=s['pred'])\n","\n","    df = pd.DataFrame(results)\n","    out_csv = os.path.join(CSV_DIR, 'results_final.csv')\n","    df.to_csv(out_csv, index=False)\n","    print(f\"Saved: {out_csv}\")\n","    return df, samples\n","\n","\n","# Colab 실행 예시 (전체 그리드 1번에 실행):\n","# - datasets: DATASET_NAMES (7개)\n","# - models:   MODEL_NAMES (7개)\n","# - input:    INPUT_LENS (2개)\n","# - output:   PRED_LENS (4개)\n","# 총 7*7*2*4 = 392번 학습/평가를 수행하므로 시간이 많이 걸립니다.\n","ensure_datasets()  # /content/data 로 다운로드\n","df, samples = run_grid(input_lens=[336])  # 모든 데이터셋/모델/input/pred 조합 실행\n","\n","# (디버그용) 일부만 빠르게 돌리고 싶으면 예:\n","# df, samples = run_grid(dataset_names=['ETTh1'], input_lens=[96], pred_lens=[96], model_names=['DLinear','TCN'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CN_0jM3COpXg","executionInfo":{"status":"ok","timestamp":1767092935005,"user_tz":-540,"elapsed":33889911,"user":{"displayName":"동욱김","userId":"12228314751297835719"}},"outputId":"a04b230e-be20-4c24-d9ee-98762b4666b0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ETTh1: OK (/content/data/ETTh1.csv)\n","ETTh2: OK (/content/data/ETTh2.csv)\n","ETTm1: OK (/content/data/ETTm1.csv)\n","ETTm2: OK (/content/data/ETTm2.csv)\n","Electricity: OK (/content/data/electricity.csv)\n","Weather: OK (/content/data/weather.csv)\n","Traffic: OK (/content/data/traffic.csv)\n","Running grid: datasets=7, models=7, in=[336], out=[96, 192, 336, 720]\n","DATA_DIR=/content/data\n","CUSTOM_MAX_FEATURES=16 (custom datasets only)\n","[ETTh1] in=336 out=96 vars=7 | TCN ... done 27.2s | MSE=0.4433, MAE=0.4600 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | DecompOpTCN ... done 34.0s | MSE=0.4011, MAE=0.4231 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | Decomp-OpTCN-v3(Patch) ... done 77.2s | MSE=0.4854, MAE=0.4837 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | DLinear ... done 22.4s | MSE=0.3755, MAE=0.3956 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | ModernTCN ... done 42.5s | MSE=0.4227, MAE=0.4377 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | ModernTCN+Decomp ... done 66.6s | MSE=0.4163, MAE=0.4365 (scaled)\n","[ETTh1] in=336 out=96 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 43.4s | MSE=0.4035, MAE=0.4186 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | TCN ... done 50.4s | MSE=0.5946, MAE=0.5338 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | DecompOpTCN ... done 29.8s | MSE=0.4493, MAE=0.4527 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | Decomp-OpTCN-v3(Patch) ... done 72.6s | MSE=0.4989, MAE=0.4862 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | DLinear ... done 8.4s | MSE=0.4218, MAE=0.4284 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | ModernTCN ... done 35.0s | MSE=0.4737, MAE=0.4692 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | ModernTCN+Decomp ... done 87.4s | MSE=0.4875, MAE=0.4692 (scaled)\n","[ETTh1] in=336 out=192 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 36.7s | MSE=0.4431, MAE=0.4480 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | TCN ... done 62.3s | MSE=0.5972, MAE=0.5357 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | DecompOpTCN ... done 29.3s | MSE=0.5240, MAE=0.5045 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | Decomp-OpTCN-v3(Patch) ... done 71.9s | MSE=0.5199, MAE=0.5038 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | DLinear ... done 12.8s | MSE=0.4409, MAE=0.4375 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | ModernTCN ... done 34.7s | MSE=0.5001, MAE=0.4858 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | ModernTCN+Decomp ... done 303.7s | MSE=0.5892, MAE=0.5190 (scaled)\n","[ETTh1] in=336 out=336 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 36.3s | MSE=0.4630, MAE=0.4568 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | TCN ... done 18.1s | MSE=0.6065, MAE=0.5609 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | DecompOpTCN ... done 28.2s | MSE=0.5650, MAE=0.5555 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | Decomp-OpTCN-v3(Patch) ... done 68.3s | MSE=0.5530, MAE=0.5241 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | DLinear ... done 22.1s | MSE=0.4555, MAE=0.4645 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | ModernTCN ... done 77.4s | MSE=0.7533, MAE=0.5920 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | ModernTCN+Decomp ... done 206.1s | MSE=0.6684, MAE=0.5628 (scaled)\n","[ETTh1] in=336 out=720 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 69.8s | MSE=0.6548, MAE=0.5553 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | TCN ... done 19.0s | MSE=0.3894, MAE=0.4199 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | DecompOpTCN ... done 50.4s | MSE=0.3167, MAE=0.3684 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | Decomp-OpTCN-v3(Patch) ... done 85.9s | MSE=0.2994, MAE=0.3586 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | DLinear ... done 27.0s | MSE=0.2751, MAE=0.3382 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | ModernTCN ... done 35.8s | MSE=0.3262, MAE=0.3747 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | ModernTCN+Decomp ... done 66.4s | MSE=0.3341, MAE=0.3754 (scaled)\n","[ETTh2] in=336 out=96 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 37.2s | MSE=0.2897, MAE=0.3507 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | TCN ... done 18.8s | MSE=0.4727, MAE=0.4746 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | DecompOpTCN ... done 54.8s | MSE=0.3834, MAE=0.4185 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | Decomp-OpTCN-v3(Patch) ... done 72.5s | MSE=0.3562, MAE=0.3892 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | DLinear ... done 10.0s | MSE=0.3505, MAE=0.3894 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | ModernTCN ... done 34.9s | MSE=0.3639, MAE=0.4013 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | ModernTCN+Decomp ... done 66.3s | MSE=0.3737, MAE=0.4038 (scaled)\n","[ETTh2] in=336 out=192 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 37.1s | MSE=0.3704, MAE=0.3987 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | TCN ... done 18.9s | MSE=0.4140, MAE=0.4475 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | DecompOpTCN ... done 34.7s | MSE=0.4157, MAE=0.4441 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | Decomp-OpTCN-v3(Patch) ... done 71.9s | MSE=0.3681, MAE=0.4102 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | DLinear ... done 11.3s | MSE=0.3860, MAE=0.4212 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | ModernTCN ... done 34.8s | MSE=0.3848, MAE=0.4183 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | ModernTCN+Decomp ... done 65.8s | MSE=0.3747, MAE=0.4144 (scaled)\n","[ETTh2] in=336 out=336 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 36.7s | MSE=0.3681, MAE=0.4096 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | TCN ... done 27.4s | MSE=0.5054, MAE=0.5043 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | DecompOpTCN ... done 47.4s | MSE=0.4822, MAE=0.4770 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | Decomp-OpTCN-v3(Patch) ... done 68.2s | MSE=0.3966, MAE=0.4312 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | DLinear ... done 22.4s | MSE=0.4233, MAE=0.4492 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | ModernTCN ... done 38.9s | MSE=0.4526, MAE=0.4617 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | ModernTCN+Decomp ... done 61.8s | MSE=0.4367, MAE=0.4601 (scaled)\n","[ETTh2] in=336 out=720 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 34.9s | MSE=0.4069, MAE=0.4438 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | TCN ... done 145.9s | MSE=0.4406, MAE=0.4295 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | DecompOpTCN ... done 125.9s | MSE=0.3159, MAE=0.3667 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | Decomp-OpTCN-v3(Patch) ... done 357.2s | MSE=0.3048, MAE=0.3563 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | DLinear ... done 105.8s | MSE=0.3043, MAE=0.3463 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | ModernTCN ... done 147.9s | MSE=0.3165, MAE=0.3641 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | ModernTCN+Decomp ... done 279.1s | MSE=0.3109, MAE=0.3608 (scaled)\n","[ETTm1] in=336 out=96 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 156.1s | MSE=0.3053, MAE=0.3551 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | TCN ... done 80.1s | MSE=0.5464, MAE=0.4896 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | DecompOpTCN ... done 190.0s | MSE=0.3937, MAE=0.4131 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | Decomp-OpTCN-v3(Patch) ... done 358.8s | MSE=0.3876, MAE=0.3990 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | DLinear ... done 77.7s | MSE=0.3417, MAE=0.3677 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | ModernTCN ... done 148.4s | MSE=0.3735, MAE=0.3979 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | ModernTCN+Decomp ... done 509.7s | MSE=0.4138, MAE=0.4124 (scaled)\n","[ETTm1] in=336 out=192 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 155.9s | MSE=0.3506, MAE=0.3814 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | TCN ... done 80.3s | MSE=0.5263, MAE=0.4832 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | DecompOpTCN ... done 231.4s | MSE=0.4134, MAE=0.4255 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | Decomp-OpTCN-v3(Patch) ... done 358.4s | MSE=0.3777, MAE=0.3972 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | DLinear ... done 54.4s | MSE=0.3838, MAE=0.3955 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | ModernTCN ... done 148.7s | MSE=0.4097, MAE=0.4204 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | ModernTCN+Decomp ... done 278.6s | MSE=0.4148, MAE=0.4245 (scaled)\n","[ETTm1] in=336 out=336 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 155.1s | MSE=0.3959, MAE=0.4063 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | TCN ... done 81.2s | MSE=0.5421, MAE=0.4923 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | DecompOpTCN ... done 254.2s | MSE=0.4521, MAE=0.4480 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | Decomp-OpTCN-v3(Patch) ... done 357.5s | MSE=0.4307, MAE=0.4231 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | DLinear ... done 175.5s | MSE=0.4273, MAE=0.4162 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | ModernTCN ... done 149.2s | MSE=0.4750, MAE=0.4590 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | ModernTCN+Decomp ... done 321.7s | MSE=0.4940, MAE=0.4579 (scaled)\n","[ETTm1] in=336 out=720 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 386.9s | MSE=0.5433, MAE=0.4709 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | TCN ... done 79.1s | MSE=0.2030, MAE=0.2868 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | DecompOpTCN ... done 125.0s | MSE=0.1808, MAE=0.2704 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | Decomp-OpTCN-v3(Patch) ... done 405.3s | MSE=0.1713, MAE=0.2604 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | DLinear ... done 64.6s | MSE=0.1646, MAE=0.2537 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | ModernTCN ... done 146.1s | MSE=0.1795, MAE=0.2655 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | ModernTCN+Decomp ... done 923.2s | MSE=0.1960, MAE=0.2728 (scaled)\n","[ETTm2] in=336 out=96 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 285.3s | MSE=0.1858, MAE=0.2681 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | TCN ... done 80.1s | MSE=0.2475, MAE=0.3192 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | DecompOpTCN ... done 232.0s | MSE=0.3129, MAE=0.3612 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | Decomp-OpTCN-v3(Patch) ... done 359.0s | MSE=0.2253, MAE=0.2961 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | DLinear ... done 71.9s | MSE=0.2224, MAE=0.2916 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | ModernTCN ... done 198.9s | MSE=0.2633, MAE=0.3205 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | ModernTCN+Decomp ... done 278.4s | MSE=0.2429, MAE=0.3076 (scaled)\n","[ETTm2] in=336 out=192 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 156.1s | MSE=0.2313, MAE=0.3016 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | TCN ... done 80.7s | MSE=0.3751, MAE=0.3935 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | DecompOpTCN ... done 127.2s | MSE=0.4194, MAE=0.4161 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | Decomp-OpTCN-v3(Patch) ... done 306.6s | MSE=0.2809, MAE=0.3326 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | DLinear ... done 90.4s | MSE=0.2746, MAE=0.3259 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | ModernTCN ... done 147.1s | MSE=0.3031, MAE=0.3490 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | ModernTCN+Decomp ... done 510.9s | MSE=0.3166, MAE=0.3573 (scaled)\n","[ETTm2] in=336 out=336 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 156.4s | MSE=0.2949, MAE=0.3403 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | TCN ... done 108.8s | MSE=0.4763, MAE=0.4503 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | DecompOpTCN ... done 127.4s | MSE=0.7789, MAE=0.5865 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | Decomp-OpTCN-v3(Patch) ... done 306.0s | MSE=0.3618, MAE=0.3800 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | DLinear ... done 94.4s | MSE=0.3679, MAE=0.3855 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | ModernTCN ... done 148.5s | MSE=0.3941, MAE=0.4031 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | ModernTCN+Decomp ... done 278.4s | MSE=0.4137, MAE=0.4092 (scaled)\n","[ETTm2] in=336 out=720 vars=7 | ModernTCN(seasonal)+Linear(trend) ... done 156.8s | MSE=0.4949, MAE=0.4558 (scaled)\n","[Electricity] in=336 out=96 vars=16 | TCN ... done 48.1s | MSE=0.2870, MAE=0.3258 (scaled)\n","[Electricity] in=336 out=96 vars=16 | DecompOpTCN ... done 117.2s | MSE=0.2092, MAE=0.2875 (scaled)\n","[Electricity] in=336 out=96 vars=16 | Decomp-OpTCN-v3(Patch) ... done 207.7s | MSE=0.2303, MAE=0.3094 (scaled)\n","[Electricity] in=336 out=96 vars=16 | DLinear ... done 93.3s | MSE=0.2100, MAE=0.2820 (scaled)\n","[Electricity] in=336 out=96 vars=16 | ModernTCN ... done 169.1s | MSE=0.2826, MAE=0.3210 (scaled)\n","[Electricity] in=336 out=96 vars=16 | ModernTCN+Decomp ... done 652.0s | MSE=0.2536, MAE=0.3025 (scaled)\n","[Electricity] in=336 out=96 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 149.8s | MSE=0.2094, MAE=0.2900 (scaled)\n","[Electricity] in=336 out=192 vars=16 | TCN ... done 110.4s | MSE=0.3108, MAE=0.3383 (scaled)\n","[Electricity] in=336 out=192 vars=16 | DecompOpTCN ... done 75.2s | MSE=0.2386, MAE=0.3046 (scaled)\n","[Electricity] in=336 out=192 vars=16 | Decomp-OpTCN-v3(Patch) ... done 207.6s | MSE=0.2850, MAE=0.3393 (scaled)\n","[Electricity] in=336 out=192 vars=16 | DLinear ... done 94.7s | MSE=0.2379, MAE=0.2976 (scaled)\n","[Electricity] in=336 out=192 vars=16 | ModernTCN ... done 108.3s | MSE=0.3561, MAE=0.3596 (scaled)\n","[Electricity] in=336 out=192 vars=16 | ModernTCN+Decomp ... done 411.9s | MSE=0.3077, MAE=0.3374 (scaled)\n","[Electricity] in=336 out=192 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 112.3s | MSE=0.2592, MAE=0.3244 (scaled)\n","[Electricity] in=336 out=336 vars=16 | TCN ... done 42.1s | MSE=0.3246, MAE=0.3567 (scaled)\n","[Electricity] in=336 out=336 vars=16 | DecompOpTCN ... done 65.1s | MSE=0.2932, MAE=0.3448 (scaled)\n","[Electricity] in=336 out=336 vars=16 | Decomp-OpTCN-v3(Patch) ... done 207.7s | MSE=0.3066, MAE=0.3514 (scaled)\n","[Electricity] in=336 out=336 vars=16 | DLinear ... done 98.1s | MSE=0.2659, MAE=0.3165 (scaled)\n","[Electricity] in=336 out=336 vars=16 | ModernTCN ... done 108.6s | MSE=0.3804, MAE=0.3757 (scaled)\n","[Electricity] in=336 out=336 vars=16 | ModernTCN+Decomp ... done 307.5s | MSE=0.3375, MAE=0.3557 (scaled)\n","[Electricity] in=336 out=336 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 112.8s | MSE=0.2854, MAE=0.3425 (scaled)\n","[Electricity] in=336 out=720 vars=16 | TCN ... done 78.5s | MSE=0.3545, MAE=0.3737 (scaled)\n","[Electricity] in=336 out=720 vars=16 | DecompOpTCN ... done 107.9s | MSE=0.3824, MAE=0.3813 (scaled)\n","[Electricity] in=336 out=720 vars=16 | Decomp-OpTCN-v3(Patch) ... done 179.6s | MSE=0.3990, MAE=0.4034 (scaled)\n","[Electricity] in=336 out=720 vars=16 | DLinear ... done 93.5s | MSE=0.3598, MAE=0.3691 (scaled)\n","[Electricity] in=336 out=720 vars=16 | ModernTCN ... done 143.7s | MSE=0.4654, MAE=0.4226 (scaled)\n","[Electricity] in=336 out=720 vars=16 | ModernTCN+Decomp ... done 369.2s | MSE=0.4158, MAE=0.3956 (scaled)\n","[Electricity] in=336 out=720 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 316.6s | MSE=0.4854, MAE=0.4176 (scaled)\n","[Weather] in=336 out=96 vars=16 | TCN ... done 84.1s | MSE=0.2329, MAE=0.2723 (scaled)\n","[Weather] in=336 out=96 vars=16 | DecompOpTCN ... done 130.3s | MSE=0.2025, MAE=0.2489 (scaled)\n","[Weather] in=336 out=96 vars=16 | Decomp-OpTCN-v3(Patch) ... done 422.0s | MSE=0.1821, MAE=0.2278 (scaled)\n","[Weather] in=336 out=96 vars=16 | DLinear ... done 57.7s | MSE=0.2135, MAE=0.2547 (scaled)\n","[Weather] in=336 out=96 vars=16 | ModernTCN ... done 220.8s | MSE=0.1990, MAE=0.2424 (scaled)\n","[Weather] in=336 out=96 vars=16 | ModernTCN+Decomp ... done 419.2s | MSE=0.2058, MAE=0.2482 (scaled)\n","[Weather] in=336 out=96 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 228.1s | MSE=0.1898, MAE=0.2374 (scaled)\n","[Weather] in=336 out=192 vars=16 | TCN ... done 85.2s | MSE=0.3125, MAE=0.3393 (scaled)\n","[Weather] in=336 out=192 vars=16 | DecompOpTCN ... done 154.0s | MSE=0.3016, MAE=0.3293 (scaled)\n","[Weather] in=336 out=192 vars=16 | Decomp-OpTCN-v3(Patch) ... done 318.5s | MSE=0.2433, MAE=0.2827 (scaled)\n","[Weather] in=336 out=192 vars=16 | DLinear ... done 64.9s | MSE=0.2683, MAE=0.2999 (scaled)\n","[Weather] in=336 out=192 vars=16 | ModernTCN ... done 219.7s | MSE=0.2723, MAE=0.3026 (scaled)\n","[Weather] in=336 out=192 vars=16 | ModernTCN+Decomp ... done 694.5s | MSE=0.2845, MAE=0.3073 (scaled)\n","[Weather] in=336 out=192 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 227.3s | MSE=0.2669, MAE=0.3031 (scaled)\n","[Weather] in=336 out=336 vars=16 | TCN ... done 86.1s | MSE=0.3813, MAE=0.3810 (scaled)\n","[Weather] in=336 out=336 vars=16 | DecompOpTCN ... done 262.9s | MSE=0.3741, MAE=0.3659 (scaled)\n","[Weather] in=336 out=336 vars=16 | Decomp-OpTCN-v3(Patch) ... done 421.0s | MSE=0.2995, MAE=0.3221 (scaled)\n","[Weather] in=336 out=336 vars=16 | DLinear ... done 87.1s | MSE=0.3263, MAE=0.3407 (scaled)\n","[Weather] in=336 out=336 vars=16 | ModernTCN ... done 219.6s | MSE=0.3401, MAE=0.3542 (scaled)\n","[Weather] in=336 out=336 vars=16 | ModernTCN+Decomp ... done 415.0s | MSE=0.3470, MAE=0.3524 (scaled)\n","[Weather] in=336 out=336 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 227.7s | MSE=0.3646, MAE=0.3608 (scaled)\n","[Weather] in=336 out=720 vars=16 | TCN ... done 88.3s | MSE=0.4260, MAE=0.4134 (scaled)\n","[Weather] in=336 out=720 vars=16 | DecompOpTCN ... done 424.6s | MSE=0.4074, MAE=0.4008 (scaled)\n","[Weather] in=336 out=720 vars=16 | Decomp-OpTCN-v3(Patch) ... done 368.9s | MSE=0.3909, MAE=0.3800 (scaled)\n","[Weather] in=336 out=720 vars=16 | DLinear ... done 177.8s | MSE=0.4095, MAE=0.3963 (scaled)\n","[Weather] in=336 out=720 vars=16 | ModernTCN ... done 258.0s | MSE=0.4967, MAE=0.4295 (scaled)\n","[Weather] in=336 out=720 vars=16 | ModernTCN+Decomp ... done 414.1s | MSE=0.4470, MAE=0.4109 (scaled)\n","[Weather] in=336 out=720 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 228.5s | MSE=0.4572, MAE=0.4136 (scaled)\n","[Traffic] in=336 out=96 vars=16 | TCN ... done 54.0s | MSE=0.3686, MAE=0.3059 (scaled)\n","[Traffic] in=336 out=96 vars=16 | DecompOpTCN ... done 140.0s | MSE=0.3684, MAE=0.2914 (scaled)\n","[Traffic] in=336 out=96 vars=16 | Decomp-OpTCN-v3(Patch) ... done 338.5s | MSE=0.3582, MAE=0.3200 (scaled)\n","[Traffic] in=336 out=96 vars=16 | DLinear ... done 61.2s | MSE=0.3648, MAE=0.2809 (scaled)\n","[Traffic] in=336 out=96 vars=16 | ModernTCN ... done 189.0s | MSE=0.3416, MAE=0.3015 (scaled)\n","[Traffic] in=336 out=96 vars=16 | ModernTCN+Decomp ... done 673.9s | MSE=0.3379, MAE=0.3006 (scaled)\n","[Traffic] in=336 out=96 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 281.8s | MSE=0.3329, MAE=0.2862 (scaled)\n","[Traffic] in=336 out=192 vars=16 | TCN ... done 53.7s | MSE=0.3720, MAE=0.2976 (scaled)\n","[Traffic] in=336 out=192 vars=16 | DecompOpTCN ... done 104.6s | MSE=0.3694, MAE=0.2999 (scaled)\n","[Traffic] in=336 out=192 vars=16 | Decomp-OpTCN-v3(Patch) ... done 507.3s | MSE=0.3728, MAE=0.3246 (scaled)\n","[Traffic] in=336 out=192 vars=16 | DLinear ... done 62.0s | MSE=0.3835, MAE=0.2883 (scaled)\n","[Traffic] in=336 out=192 vars=16 | ModernTCN ... done 310.3s | MSE=0.3575, MAE=0.3065 (scaled)\n","[Traffic] in=336 out=192 vars=16 | ModernTCN+Decomp ... done 313.9s | MSE=0.3654, MAE=0.3217 (scaled)\n","[Traffic] in=336 out=192 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 259.4s | MSE=0.3466, MAE=0.2954 (scaled)\n","[Traffic] in=336 out=336 vars=16 | TCN ... done 31.8s | MSE=0.3851, MAE=0.3205 (scaled)\n","[Traffic] in=336 out=336 vars=16 | DecompOpTCN ... done 91.2s | MSE=0.3813, MAE=0.3027 (scaled)\n","[Traffic] in=336 out=336 vars=16 | Decomp-OpTCN-v3(Patch) ... done 506.7s | MSE=0.3893, MAE=0.3349 (scaled)\n","[Traffic] in=336 out=336 vars=16 | DLinear ... done 64.5s | MSE=0.4018, MAE=0.2948 (scaled)\n","[Traffic] in=336 out=336 vars=16 | ModernTCN ... done 295.6s | MSE=0.3731, MAE=0.3110 (scaled)\n","[Traffic] in=336 out=336 vars=16 | ModernTCN+Decomp ... done 356.2s | MSE=0.3764, MAE=0.3227 (scaled)\n","[Traffic] in=336 out=336 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 339.3s | MSE=0.3615, MAE=0.3005 (scaled)\n","[Traffic] in=336 out=720 vars=16 | TCN ... done 49.2s | MSE=0.3925, MAE=0.3199 (scaled)\n","[Traffic] in=336 out=720 vars=16 | DecompOpTCN ... done 150.5s | MSE=0.3878, MAE=0.3136 (scaled)\n","[Traffic] in=336 out=720 vars=16 | Decomp-OpTCN-v3(Patch) ... done 244.2s | MSE=0.4256, MAE=0.3543 (scaled)\n","[Traffic] in=336 out=720 vars=16 | DLinear ... done 64.8s | MSE=0.4237, MAE=0.3087 (scaled)\n","[Traffic] in=336 out=720 vars=16 | ModernTCN ... done 274.3s | MSE=0.4013, MAE=0.3186 (scaled)\n","[Traffic] in=336 out=720 vars=16 | ModernTCN+Decomp ... done 645.5s | MSE=0.3903, MAE=0.3193 (scaled)\n","[Traffic] in=336 out=720 vars=16 | ModernTCN(seasonal)+Linear(trend) ... done 358.2s | MSE=0.3999, MAE=0.3208 (scaled)\n","Saved: /content/artifacts/results/results_final.csv\n","cp: cannot create directory '/content/drive/MyDrive/02_Work/01_KoreaUniv/02_KUBIG/BeyondCNN/': No such file or directory\n"]}]},{"cell_type":"code","source":["# =========================\n","# 7. 결과 요약/시각화 유틸 (inference 출력)\n","# =========================\n","\n","\n","def plot_sample(sample, seq_len: int, pred_len: int, title: str = ''):\n","    \"\"\"sample = {'input','true','pred'} (각 1D array)\"\"\"\n","    if sample is None:\n","        print('No sample')\n","        return\n","\n","    input_seq = sample['input']\n","    true_seq = sample['true']\n","    pred_seq = sample['pred']\n","\n","    x_past = np.arange(0, seq_len)\n","    x_future = np.arange(seq_len, seq_len + pred_len)\n","\n","    plt.figure(figsize=(14, 4))\n","    plt.plot(x_past, input_seq, label='Input (Past)', color='grey', alpha=0.6)\n","    plt.plot(x_future, true_seq, label='Ground Truth', color='black', linewidth=2)\n","    plt.plot(x_future, pred_seq, label='Prediction', color='tab:blue', linewidth=2)\n","    plt.axvline(x=seq_len, color='k', linestyle=':', alpha=0.3)\n","    plt.title(title)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(loc='upper left')\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def save_sample_plot(sample, seq_len: int, pred_len: int, out_path: str, title: str = ''):\n","    if sample is None:\n","        return\n","\n","    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n","\n","    input_seq = sample['input']\n","    true_seq = sample['true']\n","    pred_seq = sample['pred']\n","\n","    x_past = np.arange(0, seq_len)\n","    x_future = np.arange(seq_len, seq_len + pred_len)\n","\n","    plt.figure(figsize=(14, 4))\n","    plt.plot(x_past, input_seq, label='Input (Past)', color='grey', alpha=0.6)\n","    plt.plot(x_future, true_seq, label='Ground Truth', color='black', linewidth=2)\n","    plt.plot(x_future, pred_seq, label='Prediction', color='tab:blue', linewidth=2)\n","    plt.axvline(x=seq_len, color='k', linestyle=':', alpha=0.3)\n","    plt.title(title)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend(loc='upper left')\n","    plt.tight_layout()\n","    plt.savefig(out_path, dpi=160)\n","    plt.close()\n","\n","\n","def summarize(df: pd.DataFrame, metric: str = 'MSE_scaled'):\n","    \"\"\"데이터셋/모델별로 보기 쉬운 피벗.\"\"\"\n","    if df is None or len(df) == 0:\n","        print('Empty df')\n","        return None\n","\n","    piv = df.pivot_table(\n","        index=['Dataset', 'Model'],\n","        columns=['InputLen', 'PredLen'],\n","        values=[metric],\n","        aggfunc='mean',\n","    )\n","    return piv\n","\n","\n","def save_all_sample_plots(samples: dict, out_dir: str = PLOT_DIR):\n","    \"\"\"run_grid()가 반환한 samples를 전부 파일로 저장.\"\"\"\n","    for (dname, mname, seq_len, pred_len), sample in samples.items():\n","        fname = f\"{dname}__{mname}__in{seq_len}__out{pred_len}.png\".replace('/', '_')\n","        out_path = os.path.join(out_dir, dname, fname)\n","        title = f\"{dname} | {mname} | in={seq_len} out={pred_len}\"\n","        save_sample_plot(sample, seq_len, pred_len, out_path, title=title)\n","\n","\n","# =========================\n","# 디스크에서 결과/샘플 로드 (런타임 재시작 후에도 plot 가능)\n","# =========================\n","\n","def load_results_from_disk(csv_dir: str = CSV_DIR):\n","    \"\"\"results_final.csv가 있으면 그걸, 없으면 results_running.csv를 로드.\"\"\"\n","    cand_final = os.path.join(csv_dir, 'results_final.csv')\n","    cand_running = os.path.join(csv_dir, 'results_running.csv')\n","\n","    if os.path.exists(cand_final) and os.path.getsize(cand_final) > 0:\n","        return pd.read_csv(cand_final)\n","    if os.path.exists(cand_running) and os.path.getsize(cand_running) > 0:\n","        return pd.read_csv(cand_running)\n","    raise FileNotFoundError(f\"No results CSV found in {csv_dir} (expected results_final.csv or results_running.csv)\")\n","\n","\n","def load_samples_from_disk(sample_dir: str = None):\n","    \"\"\"/content/artifacts/samples/*.npz 를 읽어서 samples dict 형태로 복원.\n","\n","    파일명 포맷: {Dataset}__{Model}__in{seq}__out{pred}.npz\n","    \"\"\"\n","    sample_dir = sample_dir or os.path.join(ARTIFACT_DIR, 'samples')\n","    if not os.path.exists(sample_dir):\n","        raise FileNotFoundError(f\"Sample dir not found: {sample_dir}\")\n","\n","    out = {}\n","    for fn in os.listdir(sample_dir):\n","        if not fn.endswith('.npz'):\n","            continue\n","        base = fn[:-4]\n","        parts = base.split('__')\n","        if len(parts) < 4:\n","            continue\n","        dname = parts[0]\n","        mname = '__'.join(parts[1:-2])\n","        in_part = parts[-2]\n","        out_part = parts[-1]\n","        if not (in_part.startswith('in') and out_part.startswith('out')):\n","            continue\n","        try:\n","            seq_len = int(in_part[2:])\n","            pred_len = int(out_part[3:])\n","        except Exception:\n","            continue\n","\n","        path = os.path.join(sample_dir, fn)\n","        data = np.load(path)\n","        out[(dname, mname, seq_len, pred_len)] = {\n","            'input': data['input'],\n","            'true': data['true'],\n","            'pred': data['pred'],\n","        }\n","    return out\n","\n","\n","def save_all_sample_plots_from_disk(sample_dir: str = None, out_dir: str = PLOT_DIR):\n","    \"\"\"samples 폴더의 .npz만으로 전부 PNG 저장 (df/samples 변수 없이도 가능).\"\"\"\n","    sample_dir = sample_dir or os.path.join(ARTIFACT_DIR, 'samples')\n","    if not os.path.exists(sample_dir):\n","        raise FileNotFoundError(f\"Sample dir not found: {sample_dir}\")\n","\n","    n = 0\n","    for fn in sorted(os.listdir(sample_dir)):\n","        if not fn.endswith('.npz'):\n","            continue\n","\n","        base = fn[:-4]\n","        parts = base.split('__')\n","        if len(parts) < 4:\n","            continue\n","\n","        dname = parts[0]\n","        mname = '__'.join(parts[1:-2])\n","        in_part = parts[-2]\n","        out_part = parts[-1]\n","        if not (in_part.startswith('in') and out_part.startswith('out')):\n","            continue\n","\n","        try:\n","            seq_len = int(in_part[2:])\n","            pred_len = int(out_part[3:])\n","        except Exception:\n","            continue\n","\n","        path = os.path.join(sample_dir, fn)\n","        data = np.load(path)\n","        sample = {'input': data['input'], 'true': data['true'], 'pred': data['pred']}\n","\n","        out_path = os.path.join(out_dir, dname, f\"{dname}__{mname}__in{seq_len}__out{pred_len}.png\".replace('/', '_'))\n","        title = f\"{dname} | {mname} | in={seq_len} out={pred_len}\"\n","        save_sample_plot(sample, seq_len, pred_len, out_path, title=title)\n","        n += 1\n","\n","    print(f\"Saved {n} plots -> {out_dir}\")\n","\n","\n","# 사용 예시(Colab) - 런타임 재시작 후 Drive 백업본 복구해서 plotting만 할 때:\n","# 1) (필요 시) restore_artifacts_from_drive()\n","# 2) df는 CSV에서 로드해서 pivot 등 요약\n","df_ = load_results_from_disk()\n","piv = summarize(df_, metric='MSE_scaled'); print(piv)\n","# 3) samples는 .npz만으로 바로 png 저장\n","save_all_sample_plots_from_disk()  # -> /content/artifacts/plots"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hG4uB6yjPE4N","executionInfo":{"status":"ok","timestamp":1767098837645,"user_tz":-540,"elapsed":52283,"user":{"displayName":"동욱김","userId":"12228314751297835719"}},"outputId":"5d677855-f852-4dcf-b052-5299cc078167"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              MSE_scaled                      \\\n","InputLen                                             336                       \n","PredLen                                              96        192       336   \n","Dataset     Model                                                              \n","ETTh1       DLinear                             0.375545  0.421809  0.440936   \n","            Decomp-OpTCN-v3(Patch)              0.485429  0.498916  0.519915   \n","            DecompOpTCN                         0.401059  0.449332  0.523952   \n","            ModernTCN                           0.422733  0.473717  0.500149   \n","            ModernTCN(seasonal)+Linear(trend)   0.403498  0.443086  0.463041   \n","            ModernTCN+Decomp                    0.416270  0.487477  0.589186   \n","            TCN                                 0.443319  0.594575  0.597250   \n","ETTh2       DLinear                             0.275135  0.350481  0.386020   \n","            Decomp-OpTCN-v3(Patch)              0.299351  0.356216  0.368138   \n","            DecompOpTCN                         0.316656  0.383351  0.415723   \n","            ModernTCN                           0.326210  0.363908  0.384823   \n","            ModernTCN(seasonal)+Linear(trend)   0.289674  0.370418  0.368087   \n","            ModernTCN+Decomp                    0.334130  0.373731  0.374731   \n","            TCN                                 0.389428  0.472657  0.414022   \n","ETTm1       DLinear                             0.304316  0.341651  0.383795   \n","            Decomp-OpTCN-v3(Patch)              0.304761  0.387648  0.377744   \n","            DecompOpTCN                         0.315934  0.393694  0.413434   \n","            ModernTCN                           0.316493  0.373543  0.409705   \n","            ModernTCN(seasonal)+Linear(trend)   0.305287  0.350581  0.395948   \n","            ModernTCN+Decomp                    0.310931  0.413838  0.414789   \n","            TCN                                 0.440556  0.546383  0.526283   \n","ETTm2       DLinear                             0.164624  0.222421  0.274647   \n","            Decomp-OpTCN-v3(Patch)              0.171320  0.225340  0.280892   \n","            DecompOpTCN                         0.180785  0.312852  0.419390   \n","            ModernTCN                           0.179500  0.263346  0.303149   \n","            ModernTCN(seasonal)+Linear(trend)   0.185808  0.231271  0.294943   \n","            ModernTCN+Decomp                    0.195972  0.242926  0.316630   \n","            TCN                                 0.202958  0.247474  0.375114   \n","Electricity DLinear                             0.210003  0.237947  0.265852   \n","            Decomp-OpTCN-v3(Patch)              0.230344  0.284961  0.306609   \n","            DecompOpTCN                         0.209175  0.238593  0.293164   \n","            ModernTCN                           0.282556  0.356129  0.380444   \n","            ModernTCN(seasonal)+Linear(trend)   0.209365  0.259248  0.285439   \n","            ModernTCN+Decomp                    0.253579  0.307722  0.337493   \n","            TCN                                 0.287020  0.310812  0.324560   \n","Traffic     DLinear                             0.364755  0.383531  0.401790   \n","            Decomp-OpTCN-v3(Patch)              0.358196  0.372829  0.389318   \n","            DecompOpTCN                         0.368409  0.369445  0.381277   \n","            ModernTCN                           0.341648  0.357456  0.373067   \n","            ModernTCN(seasonal)+Linear(trend)   0.332937  0.346640  0.361479   \n","            ModernTCN+Decomp                    0.337926  0.365428  0.376439   \n","            TCN                                 0.368608  0.371987  0.385077   \n","Weather     DLinear                             0.213451  0.268271  0.326299   \n","            Decomp-OpTCN-v3(Patch)              0.182134  0.243304  0.299510   \n","            DecompOpTCN                         0.202490  0.301647  0.374125   \n","            ModernTCN                           0.198995  0.272264  0.340126   \n","            ModernTCN(seasonal)+Linear(trend)   0.189799  0.266857  0.364649   \n","            ModernTCN+Decomp                    0.205831  0.284533  0.347042   \n","            TCN                                 0.232864  0.312466  0.381283   \n","\n","                                                         \n","InputLen                                                 \n","PredLen                                             720  \n","Dataset     Model                                        \n","ETTh1       DLinear                            0.455505  \n","            Decomp-OpTCN-v3(Patch)             0.552964  \n","            DecompOpTCN                        0.564976  \n","            ModernTCN                          0.753284  \n","            ModernTCN(seasonal)+Linear(trend)  0.654786  \n","            ModernTCN+Decomp                   0.668427  \n","            TCN                                0.606493  \n","ETTh2       DLinear                            0.423326  \n","            Decomp-OpTCN-v3(Patch)             0.396584  \n","            DecompOpTCN                        0.482163  \n","            ModernTCN                          0.452636  \n","            ModernTCN(seasonal)+Linear(trend)  0.406931  \n","            ModernTCN+Decomp                   0.436653  \n","            TCN                                0.505352  \n","ETTm1       DLinear                            0.427267  \n","            Decomp-OpTCN-v3(Patch)             0.430715  \n","            DecompOpTCN                        0.452103  \n","            ModernTCN                          0.475031  \n","            ModernTCN(seasonal)+Linear(trend)  0.543275  \n","            ModernTCN+Decomp                   0.494032  \n","            TCN                                0.542067  \n","ETTm2       DLinear                            0.367911  \n","            Decomp-OpTCN-v3(Patch)             0.361750  \n","            DecompOpTCN                        0.778890  \n","            ModernTCN                          0.394124  \n","            ModernTCN(seasonal)+Linear(trend)  0.494857  \n","            ModernTCN+Decomp                   0.413708  \n","            TCN                                0.476328  \n","Electricity DLinear                            0.359754  \n","            Decomp-OpTCN-v3(Patch)             0.399048  \n","            DecompOpTCN                        0.382352  \n","            ModernTCN                          0.465350  \n","            ModernTCN(seasonal)+Linear(trend)  0.485435  \n","            ModernTCN+Decomp                   0.415811  \n","            TCN                                0.354490  \n","Traffic     DLinear                            0.423740  \n","            Decomp-OpTCN-v3(Patch)             0.425555  \n","            DecompOpTCN                        0.387808  \n","            ModernTCN                          0.401289  \n","            ModernTCN(seasonal)+Linear(trend)  0.399890  \n","            ModernTCN+Decomp                   0.390287  \n","            TCN                                0.392462  \n","Weather     DLinear                            0.409549  \n","            Decomp-OpTCN-v3(Patch)             0.390913  \n","            DecompOpTCN                        0.407367  \n","            ModernTCN                          0.496727  \n","            ModernTCN(seasonal)+Linear(trend)  0.457240  \n","            ModernTCN+Decomp                   0.446974  \n","            TCN                                0.425970  \n","Saved 196 plots -> /content/artifacts/plots\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bTBLIwhk18c9"},"execution_count":null,"outputs":[]}]}