{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fab3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "AGE-CGAN High — Young → Senior (Balanced, Mid+)\n",
    "- 256px / ResNet-6 + Self-Attention / Multi-Scale SN PatchGAN\n",
    "- Hinge GAN + (옵션) R1 GP, Feature Matching, Identity, Cycle, Perceptual(VGG19 ON)\n",
    "- DiffAug(색/이동) 옵션, Replay Buffer, EMA\n",
    "- Epoch=40, steps/epoch ≤ 400 (가볍진 않지만 과하지 않게 상향)\n",
    "\n",
    "사용법은 기존과 동일. 성능/VRAM 맞춰 하이퍼만 조절하세요.\n",
    "\"\"\"\n",
    "\n",
    "import os, random, time, copy\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn.utils import spectral_norm as SN\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===== 기본 설정 =====\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "print(\"DEVICE:\", DEVICE, \"| AMP:\", USE_AMP)\n",
    "\n",
    "DATA_DIR   = Path(\"data\")\n",
    "YOUNG_DIR  = DATA_DIR/\"Young\"\n",
    "SENIOR_DIR = DATA_DIR/\"Senior\"\n",
    "\n",
    "OUT_DIR    = Path(\"outputs_high\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR   = OUT_DIR/\"ckpt\"; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLE_DIR = OUT_DIR/\"samples\"; SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== 하이퍼(상향 세팅) =====\n",
    "IMG_SIZE   = 256\n",
    "BATCH_SIZE = 4              # 256px + 멀티스케일 → VRAM 고려 (여유면 6~8)\n",
    "EPOCHS     = 40\n",
    "MAX_STEPS_PER_EPOCH = 400   # None이면 전체\n",
    "LR       = 2e-4\n",
    "BETAS    = (0.5, 0.999)\n",
    "DECAY_FROM = 25             # 이후 선형 감쇠\n",
    "\n",
    "GEN_BLOCKS = 6              # 4→6 (조금 더 표현력)\n",
    "NGF = 64; NDF = 64          # 48→64\n",
    "LAMBDA_CYC = 10.0\n",
    "LAMBDA_IDT = 3.0\n",
    "USE_PERCEPTUAL = True\n",
    "LAMBDA_PER = 0.2\n",
    "LAMBDA_FM  = 10.0           # Feature Matching 강도\n",
    "\n",
    "USE_DIFFAUG = True          # 간단한 색/이동 DiffAug\n",
    "USE_R1 = True               # Hinge + R1(옵션)\n",
    "R1_LAMBDA = 10.0\n",
    "R1_INTERVAL = 16            # N 스텝마다 R1\n",
    "\n",
    "REPLAY_POOL_SIZE = 50\n",
    "EMA_DECAY = 0.998\n",
    "\n",
    "SAVE_EVERY_EPOCHS = 2\n",
    "LOG_EVERY_STEPS   = 50\n",
    "\n",
    "# ===== 유틸 =====\n",
    "def denorm(t):  # [-1,1]→[0,1]\n",
    "    return (t*0.5 + 0.5).clamp(0,1)\n",
    "\n",
    "def is_img(p: Path):\n",
    "    return p.suffix.lower() in (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
    "\n",
    "def safe_open(path: Path, img_size: int):\n",
    "    try:\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] bad image: {path} ({e})\")\n",
    "        return Image.new(\"RGB\", (img_size, img_size), (0,0,0))\n",
    "\n",
    "# ===== 전처리: Resize(286) → RandomCrop(256) + HFlip =====\n",
    "class RandomJitter256:\n",
    "    def __call__(self, img):\n",
    "        img = TF.resize(img, 286, interpolation=T.InterpolationMode.BICUBIC)\n",
    "        i, j, h, w = T.RandomCrop.get_params(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = TF.crop(img, i, j, h, w)\n",
    "        if random.random() < 0.5: img = TF.hflip(img)\n",
    "        return img\n",
    "\n",
    "# ===== DiffAug(간단) =====\n",
    "def diffaug(x):\n",
    "    # x in [-1,1]\n",
    "    if random.random() < 0.5:\n",
    "        # 색 변화 (brightness/contrast/saturation/hue)\n",
    "        x = (x+1)/2\n",
    "        b = (1.0 + (random.random()*0.4 - 0.2))\n",
    "        c = (1.0 + (random.random()*0.4 - 0.2))\n",
    "        s = (1.0 + (random.random()*0.4 - 0.2))\n",
    "        h = (random.random()*0.1 - 0.05)\n",
    "        x = TF.adjust_brightness(x, b)\n",
    "        x = TF.adjust_contrast(x, c)\n",
    "        x = TF.adjust_saturation(x, s)\n",
    "        x = TF.adjust_hue(x, h)\n",
    "        x = x*2-1\n",
    "    if random.random() < 0.5:\n",
    "        # 작은 이동\n",
    "        B, C, H, W = x.shape\n",
    "        tx = int(np.random.randint(-H//20, H//20))\n",
    "        ty = int(np.random.randint(-W//20, W//20))\n",
    "        grid_y, grid_x = torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
    "                                        torch.linspace(-1,1,W,device=x.device), indexing='ij')\n",
    "        grid = torch.stack((grid_x, grid_y), -1).unsqueeze(0).repeat(B,1,1,1)\n",
    "        grid[...,0] = grid[...,0] + (2*tx/W)\n",
    "        grid[...,1] = grid[...,1] + (2*ty/H)\n",
    "        x = F.grid_sample(x, grid, padding_mode='reflection', align_corners=True)\n",
    "    return x\n",
    "\n",
    "# ===== 데이터셋 =====\n",
    "class UnpairedDogs(Dataset):\n",
    "    def __init__(self, young_dir, senior_dir, augment=True):\n",
    "        self.young_paths  = sorted([p for p in Path(young_dir).glob(\"*\") if is_img(p)])\n",
    "        self.senior_paths = sorted([p for p in Path(senior_dir).glob(\"*\") if is_img(p)])\n",
    "        assert self.young_paths and self.senior_paths, \"Young/Senior 폴더 확인!\"\n",
    "        self.Ny, self.Ns = len(self.young_paths), len(self.senior_paths)\n",
    "        ops = [RandomJitter256()] if augment else [T.Resize((IMG_SIZE, IMG_SIZE))]\n",
    "        ops += [T.ToTensor(), T.Normalize((0.5,)*3, (0.5,)*3)]\n",
    "        self.tf = T.Compose(ops)\n",
    "    def __len__(self): return max(self.Ny, self.Ns)\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.tf(safe_open(self.young_paths[idx % self.Ny], IMG_SIZE))\n",
    "        s = self.tf(safe_open(self.senior_paths[random.randint(0, self.Ns-1)], IMG_SIZE))\n",
    "        return {\"young\": y, \"senior\": s}\n",
    "\n",
    "# ===== Self-Attention =====\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.f = SN(nn.Conv2d(in_ch, in_ch//8, 1))\n",
    "        self.g = SN(nn.Conv2d(in_ch, in_ch//8, 1))\n",
    "        self.h = SN(nn.Conv2d(in_ch, in_ch, 1))\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        f = self.f(x).view(B, -1, H*W)         # B, C/8, N\n",
    "        g = self.g(x).view(B, -1, H*W)         # B, C/8, N\n",
    "        beta = torch.softmax(torch.bmm(f.transpose(1,2), g), dim=-1)  # B, N, N\n",
    "        h = self.h(x).view(B, C, H*W)          # B, C, N\n",
    "        o = torch.bmm(h, beta).view(B, C, H, W)\n",
    "        return x + self.gamma * o\n",
    "\n",
    "# ===== 모델 =====\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(dim, dim, 3), nn.InstanceNorm2d(dim), nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(dim, dim, 3), nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, n_blocks=GEN_BLOCKS, ngf=NGF):\n",
    "        super().__init__()\n",
    "        m = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_c, ngf, 7), nn.InstanceNorm2d(ngf), nn.ReLU(True),\n",
    "            nn.Conv2d(ngf, ngf*2, 3, 2, 1), nn.InstanceNorm2d(ngf*2), nn.ReLU(True),\n",
    "            nn.Conv2d(ngf*2, ngf*4, 3, 2, 1), nn.InstanceNorm2d(ngf*4), nn.ReLU(True),\n",
    "        ]\n",
    "        for i in range(n_blocks):\n",
    "            m += [ResnetBlock(ngf*4)]\n",
    "            if i == n_blocks//2:\n",
    "                m += [SelfAttention(ngf*4)]  # 중간 해상도에서 주의집중\n",
    "        m += [\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, output_padding=1), nn.InstanceNorm2d(ngf*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, ngf,   3, 2, 1, output_padding=1), nn.InstanceNorm2d(ngf),   nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(ngf, out_c, 7), nn.Tanh()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*m)\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "# 멀티스케일 PatchGAN(두 스케일) + Feature map 반환\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_c=3, ndf=NDF):\n",
    "        super().__init__()\n",
    "        def block(i,o,norm=True,s=2):\n",
    "            layers=[SN(nn.Conv2d(i,o,4,s,1))]\n",
    "            if norm: layers += [nn.InstanceNorm2d(o)]\n",
    "            layers += [nn.LeakyReLU(0.2, True)]\n",
    "            return nn.Sequential(*layers)\n",
    "        self.b1 = block(in_c, ndf, norm=False)     # 128\n",
    "        self.b2 = block(ndf, ndf*2)                # 64\n",
    "        self.b3 = block(ndf*2, ndf*4)              # 32\n",
    "        self.out = SN(nn.Conv2d(ndf*4, 1, 4, 1, 1))\n",
    "    def forward(self, x):\n",
    "        f1 = self.b1(x)\n",
    "        f2 = self.b2(f1)\n",
    "        f3 = self.b3(f2)\n",
    "        logits = self.out(f3)\n",
    "        return logits, [f1, f2, f3]\n",
    "\n",
    "class MultiScaleD(nn.Module):\n",
    "    def __init__(self, in_c=3, ndf=NDF, scales=2):\n",
    "        super().__init__()\n",
    "        self.scales = nn.ModuleList([PatchDiscriminator(in_c, ndf) for _ in range(scales)])\n",
    "        self.pool = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "    def forward(self, x):\n",
    "        preds, feats = [], []\n",
    "        xi = x\n",
    "        for i, d in enumerate(self.scales):\n",
    "            logit, fmap = d(xi)\n",
    "            preds.append(logit); feats.append(fmap)\n",
    "            xi = self.pool(xi)\n",
    "        return preds, feats  # list per scale\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None: nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.InstanceNorm2d):\n",
    "        if m.weight is not None: nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if m.bias is not None: nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# ===== 손실 =====\n",
    "def hinge_g_loss(fake_logits_list):\n",
    "    loss = 0.0\n",
    "    for lg in fake_logits_list:\n",
    "        loss = loss + (-lg).mean()\n",
    "    return loss\n",
    "\n",
    "def hinge_d_loss(real_logits_list, fake_logits_list):\n",
    "    loss = 0.0\n",
    "    for lr, lf in zip(real_logits_list, fake_logits_list):\n",
    "        loss = loss + F.relu(1.0 - lr).mean() + F.relu(1.0 + lf).mean()\n",
    "    return loss\n",
    "\n",
    "def r1_reg(D, real):\n",
    "    real.requires_grad_(True)\n",
    "    preds, _ = D(real)\n",
    "    # 합산하여 하나의 스칼라\n",
    "    s = 0.0\n",
    "    for p in preds: s = s + p.sum()\n",
    "    grad = torch.autograd.grad(s, real, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    reg = grad.view(grad.size(0), -1).pow(2).sum(1).mean()\n",
    "    real.requires_grad_(False)\n",
    "    return reg\n",
    "\n",
    "def cycle_loss(x, xr): return F.l1_loss(xr, x)\n",
    "def idt_loss(x, y_same): return F.l1_loss(y_same, x)\n",
    "\n",
    "# (옵션) Perceptual Loss\n",
    "if USE_PERCEPTUAL:\n",
    "    try:\n",
    "        from torchvision.models import vgg19, VGG19_Weights\n",
    "        VGG = vgg19(weights=VGG19_Weights.DEFAULT).features[:16].to(DEVICE).eval()\n",
    "        for p in VGG.parameters(): p.requires_grad=False\n",
    "        print(\"VGG19 loaded for perceptual loss.\")\n",
    "        def perceptual_loss(x,y):\n",
    "            x = denorm(x); y = denorm(y)\n",
    "            mean = torch.tensor([0.485,0.456,0.406], device=x.device).view(1,3,1,1)\n",
    "            std  = torch.tensor([0.229,0.224,0.225], device=x.device).view(1,3,1,1)\n",
    "            x = (x-mean)/std; y = (y-mean)/std\n",
    "            return F.l1_loss(VGG(x), VGG(y))\n",
    "    except Exception as e:\n",
    "        USE_PERCEPTUAL = False\n",
    "        print(\"[WARN] VGG unavailable, perceptual loss disabled.\", e)\n",
    "\n",
    "# ===== Feature Matching Loss =====\n",
    "def feature_matching_loss(real_feats_scales, fake_feats_scales):\n",
    "    # real/fake: list over scales → each: [f1,f2,f3]\n",
    "    loss = 0.0\n",
    "    for r_scale, f_scale in zip(real_feats_scales, fake_feats_scales):\n",
    "        for r, f in zip(r_scale, f_scale):\n",
    "            loss = loss + F.l1_loss(f, r.detach())\n",
    "    return loss\n",
    "\n",
    "# ===== Replay Buffer =====\n",
    "class ImagePool:\n",
    "    def __init__(self, size=REPLAY_POOL_SIZE): self.size=size; self.pool=[]\n",
    "    def query(self, imgs):\n",
    "        out=[]\n",
    "        for im in imgs:\n",
    "            im = im.detach().unsqueeze(0)\n",
    "            if len(self.pool) < self.size:\n",
    "                self.pool.append(im); out.append(im)\n",
    "            elif random.random() > 0.5:\n",
    "                idx = random.randint(0, len(self.pool)-1)\n",
    "                tmp = self.pool[idx].clone()\n",
    "                self.pool[idx] = im; out.append(tmp)\n",
    "            else:\n",
    "                out.append(im)\n",
    "        return torch.cat(out, 0)\n",
    "\n",
    "# ===== EMA =====\n",
    "class EMAWrap:\n",
    "    def __init__(self, model, decay=EMA_DECAY):\n",
    "        self.decay=decay; self.ema=copy.deepcopy(model).eval()\n",
    "        for p in self.ema.parameters(): p.requires_grad=False\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for pe, p in zip(self.ema.parameters(), model.parameters()):\n",
    "            pe.mul_(self.decay).add_(p.data, alpha=1.0-self.decay)\n",
    "\n",
    "# ===== 데이터로더 =====\n",
    "train_ds = UnpairedDogs(YOUNG_DIR, SENIOR_DIR, augment=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, drop_last=True, persistent_workers=False,\n",
    "                          pin_memory=torch.cuda.is_available())\n",
    "print(f\"~{len(train_loader)*BATCH_SIZE} imgs/epoch (batch={BATCH_SIZE}, size={IMG_SIZE})\")\n",
    "\n",
    "# 고정 샘플\n",
    "fix_batch = next(iter(DataLoader(UnpairedDogs(YOUNG_DIR, SENIOR_DIR, augment=False),\n",
    "                                 batch_size=4, shuffle=True, num_workers=0, drop_last=True)))\n",
    "fix_y = fix_batch[\"young\"].to(DEVICE, non_blocking=True)\n",
    "fix_s = fix_batch[\"senior\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "# ===== 모델/옵티마/스케줄러/스케일러 =====\n",
    "G_y2s = ResnetGenerator().to(DEVICE); G_s2y = ResnetGenerator().to(DEVICE)\n",
    "D_y   = MultiScaleD().to(DEVICE);     D_s   = MultiScaleD().to(DEVICE)\n",
    "for m in (G_y2s,G_s2y):\n",
    "    m.apply(weights_init_normal)\n",
    "\n",
    "opt_G = torch.optim.Adam(list(G_y2s.parameters())+list(G_s2y.parameters()), lr=LR, betas=BETAS)\n",
    "opt_D = torch.optim.Adam(list(D_y.parameters())+list(D_s.parameters()), lr=LR, betas=BETAS)\n",
    "\n",
    "def lr_lambda(ep):\n",
    "    return 1.0 if ep < DECAY_FROM else max(0.0, 1.0 - (ep+1-DECAY_FROM)/(EPOCHS-DECAY_FROM))\n",
    "sch_G = torch.optim.lr_scheduler.LambdaLR(opt_G, lr_lambda)\n",
    "sch_D = torch.optim.lr_scheduler.LambdaLR(opt_D, lr_lambda)\n",
    "scaler_G = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "scaler_D = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "ema_y2s = EMAWrap(G_y2s); ema_s2y = EMAWrap(G_s2y)\n",
    "pool_s, pool_y = ImagePool(), ImagePool()\n",
    "\n",
    "# ===== 체크포인트 =====\n",
    "def save_ckpt(epoch):\n",
    "    torch.save({\n",
    "        \"G_y2s\": G_y2s.state_dict(), \"G_s2y\": G_s2y.state_dict(),\n",
    "        \"D_y\": D_y.state_dict(), \"D_s\": D_s.state_dict(),\n",
    "        \"EMA_y2s\": ema_y2s.ema.state_dict(), \"EMA_s2y\": ema_s2y.ema.state_dict(),\n",
    "        \"opt_G\": opt_G.state_dict(), \"opt_D\": opt_D.state_dict(),\n",
    "        \"sch_G\": sch_G.state_dict(), \"sch_D\": sch_D.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }, CKPT_DIR/f\"e{epoch:04d}.pt\")\n",
    "\n",
    "# ===== 학습 =====\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    G_y2s.train(); G_s2y.train(); D_y.train(); D_s.train()\n",
    "    running={\"G\":0.0,\"D\":0.0,\"cyc\":0.0,\"idt\":0.0,\"per\":0.0,\"fm\":0.0,\"r1\":0.0,\"adv\":0.0}\n",
    "    n_steps=0; t0=time.time()\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    for batch in pbar:\n",
    "        real_y = batch[\"young\"].to(DEVICE, non_blocking=True)\n",
    "        real_s = batch[\"senior\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if USE_DIFFAUG:\n",
    "            real_y = diffaug(real_y); real_s = diffaug(real_s)\n",
    "\n",
    "        # -- G --\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            fake_s = G_y2s(real_y); rec_y = G_s2y(fake_s)\n",
    "            fake_y = G_s2y(real_s); rec_s = G_y2s(fake_y)\n",
    "            idt_s  = G_y2s(real_s); idt_y = G_s2y(real_y)\n",
    "\n",
    "            # Discriminator preds for adv + FM\n",
    "            pred_s_fake, feats_s_fake = D_s(fake_s)\n",
    "            pred_y_fake, feats_y_fake = D_y(fake_y)\n",
    "\n",
    "            # adv (hinge, generator part)\n",
    "            loss_adv = hinge_g_loss(pred_s_fake) + hinge_g_loss(pred_y_fake)\n",
    "\n",
    "            # cycle / identity\n",
    "            loss_cyc = LAMBDA_CYC*(cycle_loss(real_y, rec_y) + cycle_loss(real_s, rec_s))\n",
    "            loss_idt = LAMBDA_IDT*(idt_loss(real_s, idt_s) + idt_loss(real_y, idt_y))\n",
    "\n",
    "            # perceptual (옵션)\n",
    "            if USE_PERCEPTUAL:\n",
    "                loss_per = LAMBDA_PER*(perceptual_loss(fake_s, real_s) + perceptual_loss(fake_y, real_y))\n",
    "            else:\n",
    "                loss_per = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        # FM: real feats 필요 → D에 real도 통과\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            pred_s_real, feats_s_real = D_s(real_s)\n",
    "            pred_y_real, feats_y_real = D_y(real_y)\n",
    "            loss_fm = LAMBDA_FM*(feature_matching_loss(feats_s_real, feats_s_fake) +\n",
    "                                 feature_matching_loss(feats_y_real, feats_y_fake))\n",
    "\n",
    "            loss_G = loss_adv + loss_cyc + loss_idt + loss_per + loss_fm\n",
    "\n",
    "        scaler_G.scale(loss_G).backward()\n",
    "        scaler_G.step(opt_G); scaler_G.update()\n",
    "\n",
    "        ema_y2s.update(G_y2s); ema_s2y.update(G_s2y)\n",
    "\n",
    "        # -- D --\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            fake_s_pool = pool_s.query(fake_s)\n",
    "            fake_y_pool = pool_y.query(fake_y)\n",
    "\n",
    "            pred_s_real, _ = D_s(real_s.detach())\n",
    "            pred_s_fake, _ = D_s(fake_s_pool.detach())\n",
    "            pred_y_real, _ = D_y(real_y.detach())\n",
    "            pred_y_fake, _ = D_y(fake_y_pool.detach())\n",
    "\n",
    "            loss_D = hinge_d_loss(pred_s_real, pred_s_fake) + hinge_d_loss(pred_y_real, pred_y_fake)\n",
    "\n",
    "            loss_R1 = torch.tensor(0.0, device=DEVICE)\n",
    "            if USE_R1 and (global_step % R1_INTERVAL == 0):\n",
    "                loss_R1 = (R1_LAMBDA * 0.5) * (r1_reg(D_s, real_s.detach()) + r1_reg(D_y, real_y.detach()))\n",
    "\n",
    "            loss_D_total = loss_D + loss_R1\n",
    "\n",
    "        scaler_D.scale(loss_D_total).backward()\n",
    "        scaler_D.step(opt_D); scaler_D.update()\n",
    "\n",
    "        # 로그 적산\n",
    "        running[\"G\"]  += float(loss_G)\n",
    "        running[\"D\"]  += float(loss_D)\n",
    "        running[\"cyc\"]+= float(loss_cyc)\n",
    "        running[\"idt\"]+= float(loss_idt)\n",
    "        running[\"per\"]+= float(loss_per)\n",
    "        running[\"fm\"] += float(loss_fm)\n",
    "        running[\"adv\"]+= float(loss_adv)\n",
    "        running[\"r1\"] += float(loss_R1)\n",
    "        n_steps+=1; global_step+=1\n",
    "\n",
    "        if n_steps % LOG_EVERY_STEPS == 0:\n",
    "            pbar.set_postfix(G=running[\"G\"]/n_steps, D=running[\"D\"]/n_steps, adv=running[\"adv\"]/n_steps)\n",
    "\n",
    "        if (MAX_STEPS_PER_EPOCH is not None) and (n_steps >= MAX_STEPS_PER_EPOCH):\n",
    "            break\n",
    "\n",
    "    for k in running: running[k]/=max(1,n_steps)\n",
    "    print(f\"[E{epoch+1:03d}] G={running['G']:.3f} D={running['D']:.3f} adv={running['adv']:.3f} \"\n",
    "          f\"cyc={running['cyc']:.3f} idt={running['idt']:.3f} per={running['per']:.3f} \"\n",
    "          f\"fm={running['fm']:.3f} r1={running['r1']:.3f} | {n_steps} steps | {time.time()-t0:.1f}s\")\n",
    "\n",
    "    sch_G.step(); sch_D.step()\n",
    "\n",
    "    if (epoch+1)%SAVE_EVERY_EPOCHS==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=USE_AMP):\n",
    "            Gs = ema_y2s.ema.eval(); Gy = ema_s2y.ema.eval()\n",
    "            f_s = Gs(fix_y); r_y = Gy(f_s)\n",
    "            f_y = Gy(fix_s); r_s = Gs(f_y)\n",
    "            grid = vutils.make_grid(torch.cat([denorm(fix_y),denorm(f_s),denorm(r_y),\n",
    "                                               denorm(fix_s),denorm(f_y),denorm(r_s)],0), nrow=4)\n",
    "            vutils.save_image(grid, SAMPLE_DIR/f\"e{epoch+1:04d}.jpg\")\n",
    "        save_ckpt(epoch+1)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ===== 추론 =====\n",
    "@torch.no_grad()\n",
    "def load_for_infer(ckpt_path=None):\n",
    "    g = ResnetGenerator().to(DEVICE).eval()\n",
    "    if ckpt_path is None:\n",
    "        ckpts = sorted(CKPT_DIR.glob(\"e*.pt\")); assert ckpts, \"체크포인트가 없습니다.\"\n",
    "        ckpt_path = ckpts[-1]\n",
    "    data = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    state = data.get(\"EMA_y2s\", None) or data[\"G_y2s\"]\n",
    "    g.load_state_dict(state); print(\"Loaded:\", Path(ckpt_path).name)\n",
    "    return g\n",
    "\n",
    "infer_tf = T.Compose([T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                      T.ToTensor(), T.Normalize((0.5,)*3,(0.5,)*3)])\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_single(input_path, out_path=None, ckpt_path=None):\n",
    "    G = load_for_infer(ckpt_path)\n",
    "    x = infer_tf(safe_open(Path(input_path), IMG_SIZE)).unsqueeze(0).to(DEVICE, non_blocking=True)\n",
    "    with torch.cuda.amp.autocast(enabled=USE_AMP): y = denorm(G(x))\n",
    "    out_path = out_path or OUT_DIR/f\"pred_{Path(input_path).stem}.jpg\"\n",
    "    vutils.save_image(y, out_path); print(\"Saved:\", out_path); return out_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_dir(in_dir, out_dir=None, ckpt_path=None):\n",
    "    G = load_for_infer(ckpt_path)\n",
    "    out_dir = Path(out_dir or OUT_DIR/\"inferred\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths = sorted([p for p in Path(in_dir).glob(\"*\") if is_img(p)])\n",
    "    for p in paths:\n",
    "        x = infer_tf(safe_open(p, IMG_SIZE)).unsqueeze(0).to(DEVICE, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=USE_AMP): y = denorm(G(x))\n",
    "        vutils.save_image(y, out_dir/f\"{p.stem}_senior.jpg\")\n",
    "    print(f\"Saved {len(paths)} files to {out_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
