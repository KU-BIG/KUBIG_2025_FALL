{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#import"
      ],
      "metadata": {
        "id": "YkP1b0lJ196G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6BSTGDQ1T80",
        "outputId": "c2c82a82-249b-411e-c4bc-734c75d7c6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, gc, sys, time, math, json, random, hashlib, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "# 시각화 설정\n",
        "plt.style.use('ggplot')\n",
        "sns.set(font_scale=1.0)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "k7oP_Hg-19rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple installation\n",
        "!pip install tabpfn\n",
        "\n",
        "# Local development installation\n",
        "!git clone https://github.com/PriorLabs/TabPFN.git\n",
        "!pip install -e \"TabPFN[dev]\"\n",
        "\n",
        "# install AutoTabPFN\n",
        "!git clone https://github.com/priorlabs/tabpfn-extensions.git\n",
        "!pip install -e tabpfn-extensions\n",
        "!pip install hyperopt\n",
        "\n",
        "#설치 후 세션 다시 시작"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFLjxw4Y19ps",
        "outputId": "ea89f4e3-7808-4518-b43d-999000fe9e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabpfn\n",
            "  Downloading tabpfn-2.1.3-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn<1.7,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (1.6.1)\n",
            "Requirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (4.14.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (1.16.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (2.2.2)\n",
            "Requirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (0.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (0.34.4)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (2.11.7)\n",
            "Requirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn) (2.10.1)\n",
            "Collecting eval-type-backport>=0.2.2 (from tabpfn)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn) (1.1.7)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.10.1->tabpfn) (1.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.1->tabpfn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn) (2025.8.3)\n",
            "Downloading tabpfn-2.1.3-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: eval-type-backport, tabpfn\n",
            "Successfully installed eval-type-backport-0.2.2 tabpfn-2.1.3\n",
            "Cloning into 'TabPFN'...\n",
            "remote: Enumerating objects: 4191, done.\u001b[K\n",
            "remote: Counting objects: 100% (520/520), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 4191 (delta 465), reused 295 (delta 295), pack-reused 3671 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4191/4191), 267.59 MiB | 51.10 MiB/s, done.\n",
            "Resolving deltas: 100% (2632/2632), done.\n",
            "Obtaining file:///content/TabPFN\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn<1.7,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (1.6.1)\n",
            "Requirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (4.14.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (1.16.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (2.2.2)\n",
            "Requirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (0.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (0.34.4)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (2.11.7)\n",
            "Requirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (2.10.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (0.2.2)\n",
            "Collecting pre-commit (from tabpfn==2.1.3)\n",
            "  Downloading pre_commit-4.3.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting ruff==0.8.6 (from tabpfn==2.1.3)\n",
            "  Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting mypy==1.17.1 (from tabpfn==2.1.3)\n",
            "  Downloading mypy-1.17.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (8.4.1)\n",
            "Collecting pytest-xdist (from tabpfn==2.1.3)\n",
            "  Downloading pytest_xdist-3.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnx (from tabpfn==2.1.3)\n",
            "  Downloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tabpfn==2.1.3) (5.9.5)\n",
            "Collecting mkdocs (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs-1.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting mkdocs-material (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs_material-9.6.18-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting mkdocs-autorefs (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs_autorefs-1.4.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting mkdocs-gen-files (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs_gen_files-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting mkdocs-literate-nav (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs_literate_nav-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting mkdocs-glightbox (from tabpfn==2.1.3)\n",
            "  Downloading mkdocs_glightbox-0.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting mkdocstrings[python] (from tabpfn==2.1.3)\n",
            "  Downloading mkdocstrings-0.30.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting markdown-exec[ansi] (from tabpfn==2.1.3)\n",
            "  Downloading markdown_exec-1.11.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting mike (from tabpfn==2.1.3)\n",
            "  Downloading mike-2.1.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting black (from tabpfn==2.1.3)\n",
            "  Downloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy_extensions>=1.0.0 (from mypy==1.17.1->tabpfn==2.1.3)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from mypy==1.17.1->tabpfn==2.1.3)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (1.1.7)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn==2.1.3) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn==2.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn==2.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn==2.1.3) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn==2.1.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn==2.1.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn==2.1.3) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.10.1->tabpfn==2.1.3) (1.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn==2.1.3) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn==2.1.3) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn==2.1.3) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->tabpfn==2.1.3) (8.2.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->tabpfn==2.1.3) (4.3.8)\n",
            "Collecting pymdown-extensions>=9 (from markdown-exec[ansi]; extra == \"dev\"->tabpfn==2.1.3)\n",
            "  Downloading pymdown_extensions-10.16.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pygments-ansi-color>=0.3 (from markdown-exec[ansi]; extra == \"dev\"->tabpfn==2.1.3)\n",
            "  Downloading pygments_ansi_color-0.3.0-py3-none-any.whl.metadata (506 bytes)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from mike->tabpfn==2.1.3) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from mike->tabpfn==2.1.3) (6.5.2)\n",
            "Requirement already satisfied: pyparsing>=3.0 in /usr/local/lib/python3.12/dist-packages (from mike->tabpfn==2.1.3) (3.2.3)\n",
            "Collecting pyyaml-env-tag (from mike->tabpfn==2.1.3)\n",
            "  Downloading pyyaml_env_tag-1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting verspec (from mike->tabpfn==2.1.3)\n",
            "  Downloading verspec-0.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ghp-import>=1.0 (from mkdocs->tabpfn==2.1.3)\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: markdown>=3.3.6 in /usr/local/lib/python3.12/dist-packages (from mkdocs->tabpfn==2.1.3) (3.8.2)\n",
            "Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs->tabpfn==2.1.3) (3.0.2)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs->tabpfn==2.1.3)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mkdocs-get-deps>=0.2.0 (from mkdocs->tabpfn==2.1.3)\n",
            "  Downloading mkdocs_get_deps-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs->tabpfn==2.1.3) (6.0.0)\n",
            "Requirement already satisfied: babel~=2.10 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->tabpfn==2.1.3) (2.17.0)\n",
            "Collecting backrefs~=5.7.post1 (from mkdocs-material->tabpfn==2.1.3)\n",
            "  Downloading backrefs-5.9-py312-none-any.whl.metadata (3.2 kB)\n",
            "Collecting colorama~=0.4 (from mkdocs-material->tabpfn==2.1.3)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting mkdocs-material-extensions~=1.3 (from mkdocs-material->tabpfn==2.1.3)\n",
            "  Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting paginate~=0.5 (from mkdocs-material->tabpfn==2.1.3)\n",
            "  Downloading paginate-0.5.7-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pygments~=2.16 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->tabpfn==2.1.3) (2.19.2)\n",
            "Collecting mkdocstrings-python>=1.16.2 (from mkdocstrings[python]; extra == \"dev\"->tabpfn==2.1.3)\n",
            "  Downloading mkdocstrings_python-1.17.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->tabpfn==2.1.3) (5.29.5)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->tabpfn==2.1.3)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->tabpfn==2.1.3)\n",
            "  Downloading identify-2.6.13-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->tabpfn==2.1.3)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->tabpfn==2.1.3)\n",
            "  Downloading virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->tabpfn==2.1.3) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->tabpfn==2.1.3) (1.6.0)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->tabpfn==2.1.3)\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting griffe>=1.12.1 (from mkdocstrings-python>=1.16.2->mkdocstrings[python]; extra == \"dev\"->tabpfn==2.1.3)\n",
            "  Downloading griffe-1.12.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn==2.1.3) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn==2.1.3) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.1->tabpfn==2.1.3) (1.3.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->tabpfn==2.1.3)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->mike->tabpfn==2.1.3) (3.23.0)\n",
            "Downloading mypy-1.17.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mike-2.1.3-py3-none-any.whl (33 kB)\n",
            "Downloading mkdocs-1.6.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mkdocs_autorefs-1.4.2-py3-none-any.whl (24 kB)\n",
            "Downloading mkdocs_gen_files-0.5.0-py3-none-any.whl (8.4 kB)\n",
            "Downloading mkdocs_glightbox-0.4.0-py3-none-any.whl (31 kB)\n",
            "Downloading mkdocs_literate_nav-0.6.2-py3-none-any.whl (13 kB)\n",
            "Downloading mkdocs_material-9.6.18-py3-none-any.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.3.0-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.0/221.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_xdist-3.8.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backrefs-5.9-py312-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.9/397.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading identify-2.6.13-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Downloading mkdocs_get_deps-0.2.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl (8.7 kB)\n",
            "Downloading mkdocstrings_python-1.17.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mkdocstrings-0.30.0-py3-none-any.whl (36 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading paginate-0.5.7-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pygments_ansi_color-0.3.0-py3-none-any.whl (10 kB)\n",
            "Downloading pymdown_extensions-10.16.1-py3-none-any.whl (266 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml_env_tag-1.1-py3-none-any.whl (4.7 kB)\n",
            "Downloading virtualenv-20.34.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m135.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_exec-1.11.0-py3-none-any.whl (34 kB)\n",
            "Downloading verspec-0.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.12.1-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tabpfn\n",
            "  Building editable for tabpfn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabpfn: filename=tabpfn-2.1.3-0.editable-py3-none-any.whl size=15183 sha256=6ba8689aa0983db85667c79ae3b985c8e471120f67cdd9f527f9e3fb75cdcb99\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qjp9615d/wheels/0a/b8/f3/ffa091d6655f101d6c59809f3a84b6d47e245e79cfc7fb1ba3\n",
            "Successfully built tabpfn\n",
            "Installing collected packages: verspec, paginate, mkdocs-glightbox, distlib, virtualenv, ruff, pyyaml-env-tag, pymdown-extensions, pygments-ansi-color, pathspec, onnx, nodeenv, mypy_extensions, mkdocs-material-extensions, mergedeep, identify, execnet, colorama, cfgv, backrefs, pytest-xdist, pre-commit, mypy, mkdocs-get-deps, markdown-exec, griffe, ghp-import, black, mkdocs, tabpfn, mkdocs-material, mkdocs-literate-nav, mkdocs-gen-files, mkdocs-autorefs, mike, mkdocstrings, mkdocstrings-python\n",
            "  Attempting uninstall: ruff\n",
            "    Found existing installation: ruff 0.12.9\n",
            "    Uninstalling ruff-0.12.9:\n",
            "      Successfully uninstalled ruff-0.12.9\n",
            "  Attempting uninstall: tabpfn\n",
            "    Found existing installation: tabpfn 2.1.3\n",
            "    Uninstalling tabpfn-2.1.3:\n",
            "      Successfully uninstalled tabpfn-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.42.0 requires ruff>=0.9.3; sys_platform != \"emscripten\", but you have ruff 0.8.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backrefs-5.9 black-25.1.0 cfgv-3.4.0 colorama-0.4.6 distlib-0.4.0 execnet-2.1.1 ghp-import-2.1.0 griffe-1.12.1 identify-2.6.13 markdown-exec-1.11.0 mergedeep-1.3.4 mike-2.1.3 mkdocs-1.6.1 mkdocs-autorefs-1.4.2 mkdocs-gen-files-0.5.0 mkdocs-get-deps-0.2.0 mkdocs-glightbox-0.4.0 mkdocs-literate-nav-0.6.2 mkdocs-material-9.6.18 mkdocs-material-extensions-1.3.1 mkdocstrings-0.30.0 mkdocstrings-python-1.17.0 mypy-1.17.1 mypy_extensions-1.1.0 nodeenv-1.9.1 onnx-1.18.0 paginate-0.5.7 pathspec-0.12.1 pre-commit-4.3.0 pygments-ansi-color-0.3.0 pymdown-extensions-10.16.1 pytest-xdist-3.8.0 pyyaml-env-tag-1.1 ruff-0.8.6 tabpfn-2.1.3 verspec-0.1.0 virtualenv-20.34.0\n",
            "Cloning into 'tabpfn-extensions'...\n",
            "remote: Enumerating objects: 2941, done.\u001b[K\n",
            "remote: Counting objects: 100% (895/895), done.\u001b[K\n",
            "remote: Compressing objects: 100% (283/283), done.\u001b[K\n",
            "remote: Total 2941 (delta 752), reused 620 (delta 605), pack-reused 2046 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2941/2941), 1.15 MiB | 8.95 MiB/s, done.\n",
            "Resolving deltas: 100% (1825/1825), done.\n",
            "Obtaining file:///content/tabpfn-extensions\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn-extensions==0.1.3) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn-extensions==0.1.3) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<1.7,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn-extensions==0.1.3) (1.6.1)\n",
            "Requirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn-extensions==0.1.3) (1.16.1)\n",
            "Requirement already satisfied: tabpfn<3,>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn-extensions==0.1.3) (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn-extensions==0.1.3) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn-extensions==0.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn-extensions==0.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->tabpfn-extensions==0.1.3) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn-extensions==0.1.3) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.7,>=1.2.0->tabpfn-extensions==0.1.3) (3.6.0)\n",
            "Requirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (4.14.1)\n",
            "Requirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (0.34.4)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2.11.7)\n",
            "Requirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2.10.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (0.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.1->tabpfn-extensions==0.1.3) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.10.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn-extensions==0.1.3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.1->tabpfn-extensions==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.1->tabpfn-extensions==0.1.3) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1,>=0.0.1->tabpfn<3,>=2.1.1->tabpfn-extensions==0.1.3) (2025.8.3)\n",
            "Building wheels for collected packages: tabpfn-extensions\n",
            "  Building editable for tabpfn-extensions (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabpfn-extensions: filename=tabpfn_extensions-0.1.3-0.editable-py3-none-any.whl size=13429 sha256=16ef8d14d2ede3663c431bbcfef1348394c227a13be8aec139c41a74764ea29a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c_rz_a9n/wheels/98/c2/45/45cebeb4cecdd8868f294d6425836d654c4787fc3a5f1db183\n",
            "Successfully built tabpfn-extensions\n",
            "Installing collected packages: tabpfn-extensions\n",
            "Successfully installed tabpfn-extensions-0.1.3\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.16.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hyperopt) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (from hyperopt) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything(42) # Seed 고정"
      ],
      "metadata": {
        "id": "eRt1Hn5j19n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data load, eda"
      ],
      "metadata": {
        "id": "VjLcOtYy2JVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/KUBIG/25_summer_contest/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/KUBIG/25_summer_contest/test.csv')\n",
        "building_df = pd.read_csv('/content/drive/MyDrive/KUBIG/25_summer_contest/building_info.csv')"
      ],
      "metadata": {
        "id": "C92_NXmQ19l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.replace(\"-\", np.nan)\n",
        "test_df = test_df.replace(\"-\", np.nan)\n",
        "building_df = building_df.replace(\"-\", np.nan)\n"
      ],
      "metadata": {
        "id": "TJDP8Dwr19kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "    \"건물번호\": \"building_id\",\n",
        "    \"일시\": \"datetime\",\n",
        "    \"기온(°C)\": \"temperature\",\n",
        "    \"강수량(mm)\": \"precipitation\",\n",
        "    \"풍속(m/s)\": \"wind_speed\",\n",
        "    \"습도(%)\": \"humidity\",\n",
        "    \"일조(hr)\": \"sunshine_hour\",\n",
        "    \"일사(MJ/m2)\": \"solar_radiation\",\n",
        "    \"전력소비량(kWh)\": \"consumption\",\n",
        "    \"month\": \"month\",\n",
        "    \"day\": \"day\",\n",
        "    \"time\": \"hour\",\n",
        "    \"log_consumption\": \"log_consumption\",\n",
        "    \"연면적(m2)\": \"total_area\",\n",
        "    \"냉방면적(m2)\": \"cooling_area\",\n",
        "    \"태양광용량(kW)\": \"solar_capacity\",\n",
        "    \"ESS저장용량(kWh)\": \"ess_capacity\",\n",
        "    \"PCS용량(kW)\": \"pcs_capacity\",\n",
        "    \"건물유형\" : \"building_type\"\n",
        "}\n",
        "\n",
        "train_df = train_df.rename(columns=column_mapping)\n",
        "test_df = test_df.rename(columns=column_mapping)\n",
        "building_df = building_df.rename(columns=column_mapping)\n",
        "\n"
      ],
      "metadata": {
        "id": "uWP3Sf_Z2NRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Building shape:\", building_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sekdt71t2NPs",
        "outputId": "d2b7a05b-536e-4db8-f106-6c931dd97faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (204000, 10)\n",
            "Test shape: (16800, 7)\n",
            "Building shape: (100, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTrain preview:\")\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByPeyjZp2NNy",
        "outputId": "e0a2d8b4-1432-4af1-a61f-bb8905ad400a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train preview:\n",
            "   num_date_time  building_id     datetime  temperature  precipitation  \\\n",
            "0  1_20240601 00            1  20240601 00         18.3            0.0   \n",
            "1  1_20240601 01            1  20240601 01         18.3            0.0   \n",
            "2  1_20240601 02            1  20240601 02         18.1            0.0   \n",
            "3  1_20240601 03            1  20240601 03         18.0            0.0   \n",
            "4  1_20240601 04            1  20240601 04         17.8            0.0   \n",
            "\n",
            "   wind_speed  humidity  sunshine_hour  solar_radiation  consumption  \n",
            "0         2.6      82.0            0.0              0.0      5794.80  \n",
            "1         2.7      82.0            0.0              0.0      5591.85  \n",
            "2         2.6      80.0            0.0              0.0      5338.17  \n",
            "3         2.6      81.0            0.0              0.0      4554.42  \n",
            "4         1.3      81.0            0.0              0.0      3602.25  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#시계열 특성을 학습에 반영하기 위해 일시를 월, 일, 시간으로 나눕니다\n",
        "train_df['month'] = train_df['datetime'].apply(lambda x : int(x[4:6]))\n",
        "train_df['day'] = train_df['datetime'].apply(lambda x : int(x[6:8]))\n",
        "train_df['time'] = train_df['datetime'].apply(lambda x : int(x[9:11]))"
      ],
      "metadata": {
        "id": "UYuK0v5F2NLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시계열 특성을 학습에 반영하기 위해 test 데이터도 동일하게 처리합니다\n",
        "test_df['month'] = test_df['datetime'].apply(lambda x : int(x[4:6]))\n",
        "test_df['day'] = test_df['datetime'].apply(lambda x : int(x[6:8]))\n",
        "test_df['time'] = test_df['datetime'].apply(lambda x : int(x[9:11]))"
      ],
      "metadata": {
        "id": "CAtKTR022NJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nData types:\")\n",
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqXsL-Ii19ht",
        "outputId": "cda4b177-57b4-4caa-d65b-65bd69dcf0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data types:\n",
            "num_date_time       object\n",
            "building_id          int64\n",
            "datetime            object\n",
            "temperature        float64\n",
            "precipitation      float64\n",
            "wind_speed         float64\n",
            "humidity           float64\n",
            "sunshine_hour      float64\n",
            "solar_radiation    float64\n",
            "consumption        float64\n",
            "month                int64\n",
            "day                  int64\n",
            "time                 int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDescriptive statistics:\")\n",
        "print(train_df.describe(include='all'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC66Iv792SjB",
        "outputId": "f4c5fd4d-3c29-4b91-ef6b-c3ea64b98b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Descriptive statistics:\n",
            "          num_date_time    building_id     datetime    temperature  \\\n",
            "count            204000  204000.000000       204000  204000.000000   \n",
            "unique           204000            NaN         2040            NaN   \n",
            "top     100_20240823 08            NaN  20240824 23            NaN   \n",
            "freq                  1            NaN          100            NaN   \n",
            "mean                NaN      50.500000          NaN      26.098130   \n",
            "std                 NaN      28.866141          NaN       4.052888   \n",
            "min                 NaN       1.000000          NaN       8.400000   \n",
            "25%                 NaN      25.750000          NaN      23.500000   \n",
            "50%                 NaN      50.500000          NaN      26.300000   \n",
            "75%                 NaN      75.250000          NaN      28.800000   \n",
            "max                 NaN     100.000000          NaN      38.700000   \n",
            "\n",
            "        precipitation     wind_speed       humidity  sunshine_hour  \\\n",
            "count   204000.000000  204000.000000  204000.000000  204000.000000   \n",
            "unique            NaN            NaN            NaN            NaN   \n",
            "top               NaN            NaN            NaN            NaN   \n",
            "freq              NaN            NaN            NaN            NaN   \n",
            "mean         0.304185       1.967977      75.206706       0.277459   \n",
            "std          2.052947       1.316102      16.378490       0.396476   \n",
            "min          0.000000       0.000000       0.000000       0.000000   \n",
            "25%          0.000000       1.000000      64.000000       0.000000   \n",
            "50%          0.000000       1.800000      78.000000       0.000000   \n",
            "75%          0.000000       2.700000      88.000000       0.600000   \n",
            "max        100.900000      15.200000     100.000000       1.000000   \n",
            "\n",
            "        solar_radiation    consumption          month            day  \\\n",
            "count     204000.000000  204000.000000  204000.000000  204000.000000   \n",
            "unique              NaN            NaN            NaN            NaN   \n",
            "top                 NaN            NaN            NaN            NaN   \n",
            "freq                NaN            NaN            NaN            NaN   \n",
            "mean           0.702752    3329.575857       6.929412      14.835294   \n",
            "std            1.017394    3689.102792       0.793923       8.446178   \n",
            "min            0.000000       0.000000       6.000000       1.000000   \n",
            "25%            0.000000    1176.120000       6.000000       8.000000   \n",
            "50%            0.050000    1935.720000       7.000000      15.000000   \n",
            "75%            1.230000    3726.765000       8.000000      22.000000   \n",
            "max            3.950000   27155.940000       8.000000      31.000000   \n",
            "\n",
            "                 time  \n",
            "count   204000.000000  \n",
            "unique            NaN  \n",
            "top               NaN  \n",
            "freq              NaN  \n",
            "mean        11.500000  \n",
            "std          6.922204  \n",
            "min          0.000000  \n",
            "25%          5.750000  \n",
            "50%         11.500000  \n",
            "75%         17.250000  \n",
            "max         23.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"고유 건물 유형 목록:\")\n",
        "print(building_df['building_type'].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxgwj8rS2ShR",
        "outputId": "e9d9409d-9d99-46cd-e995-8a8e4fafd9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "고유 건물 유형 목록:\n",
            "['호텔' '상용' '병원' '학교' '건물기타' '아파트' '연구소' '백화점' 'IDC(전화국)' '공공']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 건물유형별 그룹화 후 통계 요약\n",
        "grouped_summary = building_df.groupby('building_type').describe()\n",
        "display(grouped_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRu25H5i2SfQ",
        "outputId": "203d9ccd-ebd8-43c1-e9d1-ea0489bb2a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              building_id                                                  \\\n",
              "                    count       mean        std   min    25%   50%    75%   \n",
              "building_type                                                               \n",
              "IDC(전화국)              9.0  51.666667  17.117243  30.0  36.00  52.0  64.00   \n",
              "건물기타                 10.0  62.100000  29.152663   7.0  49.75  65.0  81.00   \n",
              "공공                    8.0  62.375000  20.479519  33.0  47.00  67.0  74.00   \n",
              "백화점                  16.0  51.812500  24.884316  18.0  31.25  49.5  73.25   \n",
              "병원                    9.0  42.111111  27.451978   3.0  21.00  42.0  48.00   \n",
              "상용                   10.0  45.300000  34.263846   2.0  17.00  46.0  71.00   \n",
              "아파트                   9.0  60.222222  30.388229  11.0  31.00  70.0  85.00   \n",
              "연구소                   9.0  47.666667  28.770645  13.0  23.00  49.0  62.00   \n",
              "학교                   10.0  33.300000  27.305067   5.0  12.50  23.0  52.75   \n",
              "호텔                   10.0  50.000000  42.807061   1.0   9.25  52.5  87.75   \n",
              "\n",
              "                     total_area                 ...                            \\\n",
              "                 max      count           mean  ...          75%          max   \n",
              "building_type                                   ...                             \n",
              "IDC(전화국)        81.0        9.0   42075.580000  ...   44676.6700    83432.350   \n",
              "건물기타            97.0       10.0  434137.068900  ...  202836.2550  3260213.410   \n",
              "공공              92.0        8.0  133799.302500  ...  139881.0000   373141.000   \n",
              "백화점             95.0       16.0  133947.159375  ...  152151.5000   338093.000   \n",
              "병원              90.0        9.0  157621.507778  ...  129583.7700   560431.000   \n",
              "상용              99.0       10.0   81677.582000  ...  119797.3325   329867.950   \n",
              "아파트             93.0        9.0  304948.496667  ...  340630.0000   492485.514   \n",
              "연구소             94.0        9.0   74157.081111  ...   86546.9400   124037.000   \n",
              "학교              87.0       10.0  351290.481300  ...  479186.3475   596251.000   \n",
              "호텔             100.0       10.0  148146.785000  ...  155741.9300   435993.500   \n",
              "\n",
              "              cooling_area                                          \\\n",
              "                     count           mean            std       min   \n",
              "building_type                                                        \n",
              "IDC(전화국)               9.0   19153.718889    9944.763916    337.84   \n",
              "건물기타                  10.0  262467.462500  599117.775430   3975.52   \n",
              "공공                     8.0   54304.158750   33158.002212   5628.00   \n",
              "백화점                   16.0   63127.165000   43775.877407  12066.86   \n",
              "병원                     9.0  117535.680000  116864.297249  45061.76   \n",
              "상용                    10.0   38479.131000   51003.120838   1089.28   \n",
              "아파트                    9.0  205633.047667  109890.140300  22210.39   \n",
              "연구소                    9.0   48866.634444   19657.424221  16014.23   \n",
              "학교                    10.0  190519.879000   86878.170779  82112.00   \n",
              "호텔                    10.0  103288.628000  107534.343707   5619.70   \n",
              "\n",
              "                                                                   \n",
              "                       25%         50%           75%          max  \n",
              "building_type                                                      \n",
              "IDC(전화국)        13487.0000   19232.000   26701.00000    34650.630  \n",
              "건물기타            25193.0000   60885.500  150446.88875  1956128.000  \n",
              "공공              32217.0525   54222.000   78534.75000   100746.200  \n",
              "백화점             33138.5000   53329.000   85812.45250   167868.560  \n",
              "병원              68513.4100   72627.980   96896.00000   418992.000  \n",
              "상용               8311.6475   21118.675   32077.67250   157835.000  \n",
              "아파트            131500.0000  233263.000  247380.00000   415124.587  \n",
              "연구소             35742.0000   44576.270   62015.99000    77267.000  \n",
              "학교             123331.5725  177814.750  237914.75000   339131.000  \n",
              "호텔              23631.1775   68037.655  141963.25000   341983.000  \n",
              "\n",
              "[10 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb4bf92e-f7b8-45b4-97aa-9c324073c2a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">building_id</th>\n",
              "      <th colspan=\"5\" halign=\"left\">total_area</th>\n",
              "      <th colspan=\"8\" halign=\"left\">cooling_area</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>...</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>building_type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>IDC(전화국)</th>\n",
              "      <td>9.0</td>\n",
              "      <td>51.666667</td>\n",
              "      <td>17.117243</td>\n",
              "      <td>30.0</td>\n",
              "      <td>36.00</td>\n",
              "      <td>52.0</td>\n",
              "      <td>64.00</td>\n",
              "      <td>81.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>42075.580000</td>\n",
              "      <td>...</td>\n",
              "      <td>44676.6700</td>\n",
              "      <td>83432.350</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19153.718889</td>\n",
              "      <td>9944.763916</td>\n",
              "      <td>337.84</td>\n",
              "      <td>13487.0000</td>\n",
              "      <td>19232.000</td>\n",
              "      <td>26701.00000</td>\n",
              "      <td>34650.630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>건물기타</th>\n",
              "      <td>10.0</td>\n",
              "      <td>62.100000</td>\n",
              "      <td>29.152663</td>\n",
              "      <td>7.0</td>\n",
              "      <td>49.75</td>\n",
              "      <td>65.0</td>\n",
              "      <td>81.00</td>\n",
              "      <td>97.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>434137.068900</td>\n",
              "      <td>...</td>\n",
              "      <td>202836.2550</td>\n",
              "      <td>3260213.410</td>\n",
              "      <td>10.0</td>\n",
              "      <td>262467.462500</td>\n",
              "      <td>599117.775430</td>\n",
              "      <td>3975.52</td>\n",
              "      <td>25193.0000</td>\n",
              "      <td>60885.500</td>\n",
              "      <td>150446.88875</td>\n",
              "      <td>1956128.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>공공</th>\n",
              "      <td>8.0</td>\n",
              "      <td>62.375000</td>\n",
              "      <td>20.479519</td>\n",
              "      <td>33.0</td>\n",
              "      <td>47.00</td>\n",
              "      <td>67.0</td>\n",
              "      <td>74.00</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>133799.302500</td>\n",
              "      <td>...</td>\n",
              "      <td>139881.0000</td>\n",
              "      <td>373141.000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>54304.158750</td>\n",
              "      <td>33158.002212</td>\n",
              "      <td>5628.00</td>\n",
              "      <td>32217.0525</td>\n",
              "      <td>54222.000</td>\n",
              "      <td>78534.75000</td>\n",
              "      <td>100746.200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>백화점</th>\n",
              "      <td>16.0</td>\n",
              "      <td>51.812500</td>\n",
              "      <td>24.884316</td>\n",
              "      <td>18.0</td>\n",
              "      <td>31.25</td>\n",
              "      <td>49.5</td>\n",
              "      <td>73.25</td>\n",
              "      <td>95.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>133947.159375</td>\n",
              "      <td>...</td>\n",
              "      <td>152151.5000</td>\n",
              "      <td>338093.000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>63127.165000</td>\n",
              "      <td>43775.877407</td>\n",
              "      <td>12066.86</td>\n",
              "      <td>33138.5000</td>\n",
              "      <td>53329.000</td>\n",
              "      <td>85812.45250</td>\n",
              "      <td>167868.560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>병원</th>\n",
              "      <td>9.0</td>\n",
              "      <td>42.111111</td>\n",
              "      <td>27.451978</td>\n",
              "      <td>3.0</td>\n",
              "      <td>21.00</td>\n",
              "      <td>42.0</td>\n",
              "      <td>48.00</td>\n",
              "      <td>90.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>157621.507778</td>\n",
              "      <td>...</td>\n",
              "      <td>129583.7700</td>\n",
              "      <td>560431.000</td>\n",
              "      <td>9.0</td>\n",
              "      <td>117535.680000</td>\n",
              "      <td>116864.297249</td>\n",
              "      <td>45061.76</td>\n",
              "      <td>68513.4100</td>\n",
              "      <td>72627.980</td>\n",
              "      <td>96896.00000</td>\n",
              "      <td>418992.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>상용</th>\n",
              "      <td>10.0</td>\n",
              "      <td>45.300000</td>\n",
              "      <td>34.263846</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.00</td>\n",
              "      <td>46.0</td>\n",
              "      <td>71.00</td>\n",
              "      <td>99.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>81677.582000</td>\n",
              "      <td>...</td>\n",
              "      <td>119797.3325</td>\n",
              "      <td>329867.950</td>\n",
              "      <td>10.0</td>\n",
              "      <td>38479.131000</td>\n",
              "      <td>51003.120838</td>\n",
              "      <td>1089.28</td>\n",
              "      <td>8311.6475</td>\n",
              "      <td>21118.675</td>\n",
              "      <td>32077.67250</td>\n",
              "      <td>157835.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>아파트</th>\n",
              "      <td>9.0</td>\n",
              "      <td>60.222222</td>\n",
              "      <td>30.388229</td>\n",
              "      <td>11.0</td>\n",
              "      <td>31.00</td>\n",
              "      <td>70.0</td>\n",
              "      <td>85.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>304948.496667</td>\n",
              "      <td>...</td>\n",
              "      <td>340630.0000</td>\n",
              "      <td>492485.514</td>\n",
              "      <td>9.0</td>\n",
              "      <td>205633.047667</td>\n",
              "      <td>109890.140300</td>\n",
              "      <td>22210.39</td>\n",
              "      <td>131500.0000</td>\n",
              "      <td>233263.000</td>\n",
              "      <td>247380.00000</td>\n",
              "      <td>415124.587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>연구소</th>\n",
              "      <td>9.0</td>\n",
              "      <td>47.666667</td>\n",
              "      <td>28.770645</td>\n",
              "      <td>13.0</td>\n",
              "      <td>23.00</td>\n",
              "      <td>49.0</td>\n",
              "      <td>62.00</td>\n",
              "      <td>94.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>74157.081111</td>\n",
              "      <td>...</td>\n",
              "      <td>86546.9400</td>\n",
              "      <td>124037.000</td>\n",
              "      <td>9.0</td>\n",
              "      <td>48866.634444</td>\n",
              "      <td>19657.424221</td>\n",
              "      <td>16014.23</td>\n",
              "      <td>35742.0000</td>\n",
              "      <td>44576.270</td>\n",
              "      <td>62015.99000</td>\n",
              "      <td>77267.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>학교</th>\n",
              "      <td>10.0</td>\n",
              "      <td>33.300000</td>\n",
              "      <td>27.305067</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.50</td>\n",
              "      <td>23.0</td>\n",
              "      <td>52.75</td>\n",
              "      <td>87.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>351290.481300</td>\n",
              "      <td>...</td>\n",
              "      <td>479186.3475</td>\n",
              "      <td>596251.000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>190519.879000</td>\n",
              "      <td>86878.170779</td>\n",
              "      <td>82112.00</td>\n",
              "      <td>123331.5725</td>\n",
              "      <td>177814.750</td>\n",
              "      <td>237914.75000</td>\n",
              "      <td>339131.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>호텔</th>\n",
              "      <td>10.0</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>42.807061</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.25</td>\n",
              "      <td>52.5</td>\n",
              "      <td>87.75</td>\n",
              "      <td>100.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>148146.785000</td>\n",
              "      <td>...</td>\n",
              "      <td>155741.9300</td>\n",
              "      <td>435993.500</td>\n",
              "      <td>10.0</td>\n",
              "      <td>103288.628000</td>\n",
              "      <td>107534.343707</td>\n",
              "      <td>5619.70</td>\n",
              "      <td>23631.1775</td>\n",
              "      <td>68037.655</td>\n",
              "      <td>141963.25000</td>\n",
              "      <td>341983.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb4bf92e-f7b8-45b4-97aa-9c324073c2a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb4bf92e-f7b8-45b4-97aa-9c324073c2a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb4bf92e-f7b8-45b4-97aa-9c324073c2a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d2ea314e-7841-44ae-970c-91ec7c261608\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2ea314e-7841-44ae-970c-91ec7c261608')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d2ea314e-7841-44ae-970c-91ec7c261608 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_091fc581-f312-44d8-8590-6abc61ddade4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('grouped_summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_091fc581-f312-44d8-8590-6abc61ddade4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('grouped_summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "grouped_summary"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 건물유형별 건물번호 목록 출력\n",
        "grouped = building_df.groupby('building_type')['building_id'].apply(list)\n",
        "\n",
        "# 보기 좋게 출력\n",
        "for building_type, building_ids in grouped.items():\n",
        "    print(f\"[{building_type}]\")\n",
        "    print(sorted(building_ids))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo8kZ4AW2SdM",
        "outputId": "db964f24-7ef5-4769-c48c-efa2bf63ba09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IDC(전화국)]\n",
            "[30, 35, 36, 43, 52, 57, 64, 67, 81]\n",
            "\n",
            "[건물기타]\n",
            "[7, 26, 47, 58, 61, 69, 78, 82, 96, 97]\n",
            "\n",
            "[공공]\n",
            "[33, 38, 50, 66, 68, 72, 80, 92]\n",
            "\n",
            "[백화점]\n",
            "[18, 19, 27, 29, 32, 34, 40, 45, 54, 59, 63, 73, 74, 79, 88, 95]\n",
            "\n",
            "[병원]\n",
            "[3, 17, 21, 39, 42, 44, 48, 75, 90]\n",
            "\n",
            "[상용]\n",
            "[2, 6, 16, 20, 41, 51, 56, 76, 86, 99]\n",
            "\n",
            "[아파트]\n",
            "[11, 25, 31, 65, 70, 71, 85, 91, 93]\n",
            "\n",
            "[연구소]\n",
            "[13, 15, 23, 37, 49, 53, 62, 83, 94]\n",
            "\n",
            "[학교]\n",
            "[5, 8, 12, 14, 22, 24, 46, 55, 60, 87]\n",
            "\n",
            "[호텔]\n",
            "[1, 4, 9, 10, 28, 77, 84, 89, 98, 100]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 건물유형 정보 병합\n",
        "train_df_merged = pd.merge(train_df, building_df[['building_id', 'building_type']], on='building_id', how='left')\n",
        "test_df_merged = pd.merge(test_df, building_df[['building_id', 'building_type']], on='building_id', how='left')\n"
      ],
      "metadata": {
        "id": "6JTEgfnt2SbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 데이터\n",
        "train_counts = train_df_merged['building_type'].value_counts().sort_index()\n",
        "print(\"Train 데이터 - 건물유형별 관측값 개수:\")\n",
        "print(train_counts)\n",
        "\n",
        "# test 데이터\n",
        "test_counts = test_df_merged['building_type'].value_counts().sort_index()\n",
        "print(\"\\nTest 데이터 - 건물유형별 관측값 개수:\")\n",
        "print(test_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH9zSmuI2XLN",
        "outputId": "24bee754-6f23-43f9-a43a-eb2acac0444a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 데이터 - 건물유형별 관측값 개수:\n",
            "building_type\n",
            "IDC(전화국)    18360\n",
            "건물기타        20400\n",
            "공공          16320\n",
            "백화점         32640\n",
            "병원          18360\n",
            "상용          20400\n",
            "아파트         18360\n",
            "연구소         18360\n",
            "학교          20400\n",
            "호텔          20400\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test 데이터 - 건물유형별 관측값 개수:\n",
            "building_type\n",
            "IDC(전화국)    1512\n",
            "건물기타        1680\n",
            "공공          1344\n",
            "백화점         2688\n",
            "병원          1512\n",
            "상용          1680\n",
            "아파트         1512\n",
            "연구소         1512\n",
            "학교          1680\n",
            "호텔          1680\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 건물유형별 평균 전력소비량 계산\n",
        "type_mean = (\n",
        "    train_df_merged\n",
        "    .groupby('building_type')['consumption']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .round(2)\n",
        ")\n",
        "\n",
        "# DataFrame으로 보기 좋게 출력\n",
        "type_mean_df = type_mean.reset_index()\n",
        "type_mean_df.columns = ['building_type', 'consumption']\n",
        "display(type_mean_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj-0-MJu2XJN",
        "outputId": "8dbd4661-ec96-4de1-91a9-ffc98bec77e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  building_type  consumption\n",
              "0      IDC(전화국)     10316.94\n",
              "1            병원      4454.06\n",
              "2            학교      3462.68\n",
              "3            호텔      3175.02\n",
              "4           백화점      2729.74\n",
              "5            상용      2513.70\n",
              "6          건물기타      2285.96\n",
              "7           연구소      2111.67\n",
              "8            공공      1625.91\n",
              "9           아파트      1106.31"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99ad43a6-13e7-40dc-81b0-dec3d85f83f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>building_type</th>\n",
              "      <th>consumption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IDC(전화국)</td>\n",
              "      <td>10316.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>병원</td>\n",
              "      <td>4454.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>학교</td>\n",
              "      <td>3462.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>호텔</td>\n",
              "      <td>3175.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>백화점</td>\n",
              "      <td>2729.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>상용</td>\n",
              "      <td>2513.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>건물기타</td>\n",
              "      <td>2285.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>연구소</td>\n",
              "      <td>2111.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>공공</td>\n",
              "      <td>1625.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>아파트</td>\n",
              "      <td>1106.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99ad43a6-13e7-40dc-81b0-dec3d85f83f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99ad43a6-13e7-40dc-81b0-dec3d85f83f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99ad43a6-13e7-40dc-81b0-dec3d85f83f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4816c6b4-3565-4be5-acf9-d543f5200f48\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4816c6b4-3565-4be5-acf9-d543f5200f48')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4816c6b4-3565-4be5-acf9-d543f5200f48 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8c3dbc0a-9625-4e5c-af90-418756d5bf11\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('type_mean_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8c3dbc0a-9625-4e5c-af90-418756d5bf11 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('type_mean_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "type_mean_df",
              "summary": "{\n  \"name\": \"type_mean_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"building_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\uacf5\\uacf5\",\n          \"\\ubcd1\\uc6d0\",\n          \"\\uc0c1\\uc6a9\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"consumption\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2615.104799393325,\n        \"min\": 1106.31,\n        \"max\": 10316.94,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1625.91,\n          4454.06,\n          2513.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델링"
      ],
      "metadata": {
        "id": "jJQm5idk2dXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 설정 ---\n",
        "RANDOM_SEED = 2025\n",
        "N_SPLITS = 10          # Walk-forward 폴드 수\n",
        "EMBARGO  = 24         # 검증 직전 금지 구간(시간)\n",
        "USE_LOG1P = True\n",
        "DO_BLEND_SEASONAL = True\n",
        "BLEND_ALPHA = 0.2     # pred = (1-α)*xgb + α*seasonal(t-168)\n",
        "AUTO_DROP_IF_MISSING = {'sunshine_hour', 'solar_radiation', '일조', '일사'}  # 테스트에 없으면 제거\n",
        "\n",
        "def set_seed(seed=RANDOM_SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "set_seed()\n",
        "\n",
        "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = np.abs(y_true) + np.abs(y_pred)\n",
        "    denom[denom == 0] = 1e-12\n",
        "    return 100.0 * np.mean(np.abs(y_pred - y_true) / denom)\n",
        "\n",
        "# --- GPU 파라미터 ---\n",
        "XGB_PARAMS = dict(\n",
        "    n_estimators=10000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    min_child_weight=3,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=0.5,\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1,\n",
        "    tree_method=\"gpu_hist\",      # GPU\n",
        "    predictor=\"gpu_predictor\",\n",
        "    max_bin=256,\n",
        ")"
      ],
      "metadata": {
        "id": "4kkbKSlD2XHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple\n",
        "\n",
        "# =================================================================\n",
        "# --- 필수 컬럼 점검/정규화 ---\n",
        "# =================================================================\n",
        "# datetime\n",
        "if 'datetime' not in train_df.columns:\n",
        "    if 'date_time' in train_df.columns:\n",
        "        train_df = train_df.rename(columns={'date_time': 'datetime'})\n",
        "    else:\n",
        "        raise KeyError(\"train_df에 datetime 컬럼이 없습니다.\")\n",
        "if 'datetime' not in test_df.columns:\n",
        "    if 'date_time' in test_df.columns:\n",
        "        test_df = test_df.rename(columns={'date_time': 'datetime'})\n",
        "    else:\n",
        "        raise KeyError(\"test_df에 datetime 컬럼이 없습니다.\")\n",
        "\n",
        "train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
        "test_df['datetime']  = pd.to_datetime(test_df['datetime'])\n",
        "\n",
        "# building_id\n",
        "if 'building_id' not in train_df.columns:\n",
        "    raise KeyError(\"train_df에 building_id가 없습니다.\")\n",
        "if 'building_id' not in test_df.columns:\n",
        "    # 보통 test의 num_date_time에서 추출 가능\n",
        "    if 'num_date_time' in test_df.columns:\n",
        "        test_df['building_id'] = test_df['num_date_time'].astype(str).str.extract(r'^(\\d+)').astype(int)\n",
        "    else:\n",
        "        raise KeyError(\"test_df에 building_id가 없습니다.\")\n",
        "\n",
        "# target\n",
        "if 'target' not in train_df.columns:\n",
        "    # 대회 원본명 매핑 예시\n",
        "    for cand in ['consumption', 'load', 'target_kwh']:\n",
        "        if cand in train_df.columns:\n",
        "            train_df = train_df.rename(columns={cand: 'target'})\n",
        "            break\n",
        "if 'target' not in train_df.columns:\n",
        "    raise KeyError(\"train_df에 target(전력소비량) 컬럼이 없습니다.\")\n",
        "\n",
        "# 원본 코드 재현을 위해 사본 생성\n",
        "train_df = train_df.copy()\n",
        "test_df  = test_df.copy()\n",
        "\n",
        "# 수치화\n",
        "train_df['target'] = pd.to_numeric(train_df['target'], errors='coerce')\n",
        "\n",
        "# FIX: downstream 호환을 위해 'consumption' 컬럼이 없으면 target을 복제\n",
        "if 'consumption' not in train_df.columns:\n",
        "    train_df['consumption'] = train_df['target']\n",
        "\n",
        "# FIX: 기존 코드의 'log_consumption' 계산이 'consumption' 유무에 의존 → target 기준으로 안전하게 계산\n",
        "train_df['log_consumption'] = np.log1p(train_df['consumption'])\n",
        "\n",
        "# =================================================================\n",
        "# --- 피처 엔지니어링 함수들 ---\n",
        "# =================================================================\n",
        "\n",
        "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"datetime 기반 캘린더/주기 피처 (df만 수정; train/test 외부 참조 금지)\"\"\"\n",
        "    dt = df['datetime']\n",
        "    df = df.copy()\n",
        "    df['hour'] = dt.dt.hour\n",
        "    df['dayofweek'] = dt.dt.dayofweek\n",
        "    df['week'] = dt.dt.isocalendar().week.astype(int)\n",
        "    df['month'] = dt.dt.month\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    df['sin_hour'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['cos_hour'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    df['sin_dow']  = np.sin(2*np.pi*df['dayofweek']/7.0)\n",
        "    df['cos_dow']  = np.cos(2*np.pi*df['dayofweek']/7.0)\n",
        "    df['summer_cos'] = np.cos((df['month']-6) * np.pi/3)\n",
        "\n",
        "    # FIX: weekday/weekend 계산을 df 기준으로만 수행\n",
        "    df['weekday'] = df['datetime'].dt.weekday\n",
        "    df['weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_weather_features(df: pd.DataFrame, group_keys: Tuple[str, ...] = ('building_id',)) -> pd.DataFrame:\n",
        "    \"\"\"온도/습도/풍속 기반 파생, 차분/휴일 플래그 등 (df만 수정)\"\"\"\n",
        "    df = df.sort_values(list(group_keys) + ['datetime']).copy() if set(group_keys).issubset(df.columns) else df.copy()\n",
        "\n",
        "    has_T  = 'temperature' in df.columns\n",
        "    has_RH = 'humidity' in df.columns\n",
        "    has_W  = 'wind_speed' in df.columns\n",
        "\n",
        "    # 1) 기온·습도 기반 (THI, apparent temp)\n",
        "    if has_T and has_RH:\n",
        "        # 불쾌지수 (THI)\n",
        "        df['discomfort_index'] = (\n",
        "            0.81*df['temperature']\n",
        "            + 0.01*df['humidity'] * (0.99*df['temperature'] - 14.3)\n",
        "            + 46.3\n",
        "        )\n",
        "    if has_T and has_W:\n",
        "        # 체감온도 (Wind Chill 유사식; 고온 영역에서는 heat index 사용을 권장)\n",
        "        df['apparent_temp'] = (\n",
        "            13.12 + 0.6215*df['temperature']\n",
        "            - 11.37*(df['wind_speed'].clip(lower=0.1)**0.16)\n",
        "            + 0.3965*df['temperature']*(df['wind_speed'].clip(lower=0.1)**0.16)\n",
        "        )\n",
        "\n",
        "    # 2) 날씨 차이 변수 (그룹별 diff)\n",
        "    def _by_group(s):\n",
        "        if set(group_keys).issubset(df.columns):\n",
        "            return s.groupby(list(group_keys))\n",
        "        return s\n",
        "\n",
        "    if has_T:\n",
        "        df['temp_diff'] = _by_group(df['temperature']).diff()\n",
        "    if has_RH:\n",
        "        df['hum_diff'] = _by_group(df['humidity']).diff()\n",
        "    if has_W:\n",
        "        df['wind_diff'] = _by_group(df['wind_speed']).diff()\n",
        "\n",
        "    # 3) 공휴일 여부 (미국 연방휴일 예시; 필요시 한국 공휴일로 대체)\n",
        "    #    참고: from pandas.tseries.holiday import Holiday, AbstractHolidayCalendar 로 커스텀 가능\n",
        "    try:\n",
        "        from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "        cal = USFederalHolidayCalendar()\n",
        "        holidays = cal.holidays(start=df['datetime'].min(), end=df['datetime'].max())\n",
        "        df['is_holiday'] = df['datetime'].dt.normalize().isin(holidays).astype(int)\n",
        "    except Exception:\n",
        "        # 라이브러리/환경 문제 시 0으로 대체\n",
        "        df['is_holiday'] = 0\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_summer_features(df: pd.DataFrame, group_keys: Tuple[str, ...] = ('building_id',)) -> pd.DataFrame:\n",
        "    \"\"\"여름(6~8월) 특화 보강 피처 (상호작용/런-길이 등)\"\"\"\n",
        "    if 'datetime' not in df.columns:\n",
        "        raise AssertionError(\"datetime 컬럼이 필요합니다.\")\n",
        "\n",
        "    if not np.issubdtype(df['datetime'].dtype, np.datetime64):\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    df = df.sort_values(list(group_keys) + ['datetime']).copy() if set(group_keys).issubset(df.columns) else df.copy()\n",
        "\n",
        "    # 0) 여름 내 위치/캘린더 보강\n",
        "    df['date'] = df['datetime'].dt.date\n",
        "    df['doy'] = df['datetime'].dt.dayofyear\n",
        "    # summer_day_idx: 해당 연도 6/1 기준 일수\n",
        "    start_summer = pd.Timestamp(df['datetime'].dt.year.min(), 6, 1)\n",
        "    df['summer_day_idx'] = (df['datetime'].dt.normalize() - start_summer).dt.days.clip(lower=0) + 1\n",
        "\n",
        "    # 영업/출퇴근 시간\n",
        "    df['business_hours'] = df['hour'].between(9, 18).astype(int) if 'hour' in df.columns else 0\n",
        "    df['rush_morning'] = df['hour'].between(7, 9).astype(int) if 'hour' in df.columns else 0\n",
        "    df['rush_evening'] = df['hour'].between(18, 20).astype(int) if 'hour' in df.columns else 0\n",
        "\n",
        "    # 방학/성수기 플래그(한국 감각; 7/20~8/20)\n",
        "    if 'month' in df.columns:\n",
        "        m = df['month']\n",
        "    else:\n",
        "        m = df['datetime'].dt.month\n",
        "    d = df['datetime'].dt.day\n",
        "    df['vacation_peak'] = (((m == 7) & (d >= 20)) | ((m == 8) & (d <= 20))).astype(int)\n",
        "\n",
        "    has_T  = 'temperature' in df.columns\n",
        "    has_RH = 'humidity' in df.columns\n",
        "    has_W  = 'wind_speed' in df.columns\n",
        "\n",
        "    # 1) 더위/습도 스트레스 지표\n",
        "    if has_T:\n",
        "        df['CDD24'] = np.maximum(df['temperature'] - 24.0, 0.0)\n",
        "        df['CDD26'] = np.maximum(df['temperature'] - 26.0, 0.0)\n",
        "        df['temp_above26'] = (df['temperature'] - 26.0).clip(lower=0.0)\n",
        "\n",
        "    if has_T and has_RH:\n",
        "        # 이슬점\n",
        "        a, b = 17.27, 237.7\n",
        "        RHc = df['humidity'].clip(1, 100) / 100.0\n",
        "        alpha = ((a * df['temperature']) / (b + df['temperature'])) + np.log(RHc)\n",
        "        df['dew_point'] = (b * alpha) / (a - alpha)\n",
        "\n",
        "        # 습구온도 (Stull 2011 근사)\n",
        "        RH = df['humidity'].clip(1, 100)\n",
        "        T  = df['temperature']\n",
        "        df['wet_bulb'] = (\n",
        "            T*np.arctan(0.151977*np.sqrt(RH+8.313659))\n",
        "            + np.arctan(T+RH)\n",
        "            - np.arctan(RH-1.676331)\n",
        "            + 0.00391838*(RH**1.5)*np.arctan(0.023101*RH)\n",
        "            - 4.686035\n",
        "        )\n",
        "\n",
        "        # Humidex\n",
        "        dewK = (df['dew_point'] + 273.15).clip(lower=200, upper=350)\n",
        "        expo = 5417.7530 * (1/273.16 - 1/dewK)\n",
        "        df['humidex'] = T + 0.5555*((6.11*np.exp(expo)) - 10.0)\n",
        "\n",
        "        # Heat Index(간이)\n",
        "        R = df['humidity'].clip(1, 100)\n",
        "        df['heat_index'] = (\n",
        "            -8.784695 + 1.61139411*T + 2.338549*R\n",
        "            - 0.14611605*T*R - 0.012308094*(T**2)\n",
        "            - 0.016424828*(R**2) + 0.002211732*(T**2)*R\n",
        "            + 0.00072546*T*(R**2) - 0.000003582*(T**2)*(R**2)\n",
        "        )\n",
        "\n",
        "    if has_T and has_W and 'apparent_temp' not in df.columns:\n",
        "        df['apparent_temp'] = (\n",
        "            13.12 + 0.6215*df['temperature']\n",
        "            - 11.37*(df['wind_speed'].clip(lower=0.1)**0.16)\n",
        "            + 0.3965*df['temperature']*(df['wind_speed'].clip(lower=0.1)**0.16)\n",
        "        )\n",
        "\n",
        "    # 2) 상호작용 항\n",
        "    if has_T:\n",
        "        df['temp_x_business']   = df['temperature'] * df['business_hours']\n",
        "        df['temp_x_weekend']    = df['temperature'] * (df['dayofweek'] >= 5).astype(int) if 'dayofweek' in df.columns else 0\n",
        "        df['temp_x_peakvac']    = df['temperature'] * df['vacation_peak']\n",
        "        df['CDD26_x_business']  = df['CDD26'] * df['business_hours']\n",
        "\n",
        "    if has_RH:\n",
        "        df['rh_x_business'] = df['humidity'] * df['business_hours']\n",
        "\n",
        "    # 3) 날씨 lag/rolling (그룹 기준)\n",
        "    def _by_group(s):\n",
        "        if set(group_keys).issubset(df.columns):\n",
        "            return s.groupby(list(group_keys))\n",
        "        return s\n",
        "\n",
        "    for col in ['temperature', 'humidity', 'wind_speed']:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_lag1']     = _by_group(df[col]).shift(1)\n",
        "            df[f'{col}_lag24']    = _by_group(df[col]).shift(24)\n",
        "            df[f'{col}_ma3']      = _by_group(df[col]).apply(lambda x: x.rolling(3,  min_periods=1).mean())\n",
        "            df[f'{col}_ma24']     = _by_group(df[col]).apply(lambda x: x.rolling(24, min_periods=1).mean())\n",
        "            df[f'{col}_dev_ma24'] = df[col] - df[f'{col}_ma24']\n",
        "\n",
        "    # 4) 목표변수 lag/rolling (학습 시점)\n",
        "    if 'target' in df.columns:\n",
        "        df['y_lag1']    = _by_group(df['target']).shift(1)\n",
        "        df['y_lag24']   = _by_group(df['target']).shift(24)\n",
        "        df['y_lag168']  = _by_group(df['target']).shift(168)\n",
        "        df['y_ma3']     = _by_group(df['target']).apply(lambda x: x.shift(1).rolling(3,  min_periods=1).mean())\n",
        "        df['y_ma24']    = _by_group(df['target']).apply(lambda x: x.shift(1).rolling(24, min_periods=1).mean())\n",
        "        df['y_ema24']   = _by_group(df['target']).apply(lambda x: x.shift(1).ewm(span=24, adjust=False, min_periods=1).mean())\n",
        "        df['y_diff1']   = _by_group(df['target']).diff(1)\n",
        "        df['y_diff24']  = _by_group(df['target']).diff(24)\n",
        "        df['y_vs_ma24'] = df['target'] - df['y_ma24']\n",
        "\n",
        "    # 5) Heatwave 런-길이 (일단위 집계)\n",
        "    if has_T:\n",
        "        by = list(group_keys) if set(group_keys).issubset(df.columns) else []\n",
        "        daily = df.groupby(by + ['date'], as_index=False)['temperature'].max()\n",
        "        daily['hot_day'] = (daily['temperature'] >= 33).astype(int)\n",
        "\n",
        "        def _runlen(x):\n",
        "            r = []\n",
        "            cnt = 0\n",
        "            for v in x:\n",
        "                cnt = cnt + 1 if v == 1 else 0\n",
        "                r.append(cnt)\n",
        "            return pd.Series(r, index=x.index)\n",
        "\n",
        "        if by:\n",
        "            daily['hot_runlen'] = daily.groupby(by)['hot_day'].apply(_runlen).reset_index(level=by, drop=True)\n",
        "        else:\n",
        "            daily['hot_runlen'] = _runlen(daily['hot_day'])\n",
        "\n",
        "        df = df.merge(daily[by + ['date', 'hot_day', 'hot_runlen']], on=by + ['date'], how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_lag_roll(df: pd.DataFrame, group_keys: List[str], lag_hours=(1, 24, 168), roll_hours=(24, 168)) -> pd.DataFrame:\n",
        "    df = df.sort_values(group_keys + ['datetime']).copy()\n",
        "    for lh in lag_hours:\n",
        "        df[f'lag_{lh}'] = df.groupby(group_keys)['target'].shift(lh)\n",
        "    for rh in roll_hours:\n",
        "        df[f'roll{rh}_mean'] = (\n",
        "            df.groupby(group_keys)['target']\n",
        "              .shift(1)\n",
        "              .rolling(rh, min_periods=int(rh*0.5))\n",
        "              .mean()\n",
        "              .reset_index(level=0, drop=True)\n",
        "        )\n",
        "    return df\n",
        "\n",
        "\n",
        "# 추후 컬럼 정렬 시 누락해도 되는 컬럼 목록(예: 학습 시점에만 있는 lag 등)을 필요에 따라 지정\n",
        "AUTO_DROP_IF_MISSING = set()\n",
        "\n",
        "def align_train_test_columns(X_tr: pd.DataFrame, X_te: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    common = [c for c in X_tr.columns if c in X_te.columns]\n",
        "    common = [c for c in common if c not in AUTO_DROP_IF_MISSING]\n",
        "    return X_tr[common].copy(), X_te[common].copy()\n",
        "\n",
        "\n",
        "def seasonal_naive(series: pd.Series, horizon: int, period: int = 168) -> np.ndarray:\n",
        "    if len(series) < period:\n",
        "        return np.repeat(series.iloc[-1], horizon)\n",
        "    base = series.iloc[-period:]\n",
        "    return base.values[:horizon]"
      ],
      "metadata": {
        "id": "Ialc4V4U32zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##k fold"
      ],
      "metadata": {
        "id": "LEx4u52YjpPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 단일 건물 학습/예측 ---\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    smape: float\n",
        "    n_train: int\n",
        "    n_valid: int\n",
        "\n",
        "def walk_forward_building(df_b: pd.DataFrame, df_b_te: pd.DataFrame) -> np.ndarray:\n",
        "    df_b = df_b.sort_values('datetime').reset_index(drop=True)\n",
        "    df_b_te = df_b_te.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "    # 피처 선택: 누출/키/식별자 제거\n",
        "    exclude = {'target', 'datetime', 'num_date_time', 'building_id'}\n",
        "    feat_cols = [c for c in df_b.columns if c not in exclude and pd.api.types.is_numeric_dtype(df_b[c])]\n",
        "\n",
        "    if len(feat_cols) == 0:\n",
        "        # 피처가 전혀 없으면 시즌-나이브로 대체\n",
        "        log(f\"[B{int(df_b['building_id'].iloc[0])}] 사용 가능한 피처가 없어 seasonal naive 사용\")\n",
        "        return seasonal_naive(df_b['target'], horizon=len(df_b_te), period=168)\n",
        "\n",
        "    # 학습/테스트 행렬\n",
        "    X = df_b[feat_cols].to_numpy()\n",
        "    X_te = df_b_te.reindex(columns=feat_cols, fill_value=0).to_numpy()\n",
        "\n",
        "    # 타깃\n",
        "    y = df_b['target'].astype(float).values\n",
        "    y_trn = np.log1p(y) if USE_LOG1P else y\n",
        "\n",
        "    # NaN/inf 방지\n",
        "    X = np.nan_to_num(X, copy=False, posinf=None, neginf=None)\n",
        "    X_te = np.nan_to_num(X_te, copy=False, posinf=None, neginf=None)\n",
        "\n",
        "    n = len(df_b)\n",
        "    fold_sizes = np.linspace(0.6, 0.95, N_SPLITS)  # 앞쪽 학습 비율\n",
        "    oof_pred = np.zeros(n, dtype=float)\n",
        "\n",
        "    for i, frac in enumerate(fold_sizes, start=1):\n",
        "        split = int(n * float(frac))\n",
        "        tr_end = max(0, split - EMBARGO)\n",
        "\n",
        "        # 안전장치: 최소 학습/검증 길이 확보\n",
        "        if tr_end < 16 or tr_end >= n-1:\n",
        "            continue\n",
        "        va_start, va_end = tr_end, split\n",
        "        if va_end - va_start < 8:\n",
        "            continue\n",
        "\n",
        "        X_tr, y_tr = X[:tr_end], y_trn[:tr_end]\n",
        "        X_va, y_va = X[va_start:va_end], y[va_start:va_end]\n",
        "\n",
        "        model = XGBRegressor(**XGB_PARAMS)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        va_pred = model.predict(X_va)\n",
        "        va_pred = np.expm1(va_pred) if USE_LOG1P else va_pred\n",
        "        va_pred = np.clip(va_pred, 0, None)\n",
        "        oof_pred[va_start:va_end] = va_pred\n",
        "\n",
        "        s = smape(y_va, va_pred)\n",
        "        log(f\"[B{int(df_b['building_id'].iloc[0])}] Fold {i}/{N_SPLITS} SMAPE={s:.3f} \"\n",
        "            f\"(train {len(y_tr)}, valid {len(y_va)})\")\n",
        "        del model; gc.collect()\n",
        "\n",
        "    # 최종 학습\n",
        "    final_model = XGBRegressor(**XGB_PARAMS)\n",
        "    final_model.fit(X, y_trn)\n",
        "    pred_te = final_model.predict(X_te)\n",
        "    pred_te = np.expm1(pred_te) if USE_LOG1P else pred_te\n",
        "    pred_te = np.clip(pred_te, 0, None)\n",
        "\n",
        "    # 시즌-나이브 블렌딩\n",
        "    if DO_BLEND_SEASONAL:\n",
        "        horizon = len(df_b_te)\n",
        "        s_pred = seasonal_naive(df_b['target'], horizon=horizon, period=168)\n",
        "        pred_te = (1.0 - BLEND_ALPHA) * pred_te + BLEND_ALPHA * s_pred\n",
        "\n",
        "    return pred_te\n",
        "\n",
        "\n",
        "# --- 메인 파이프라인 (중복 building_id 제거/안전 groupby) ---\n",
        "def build_features_and_predict(train_df: pd.DataFrame, test_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    tr = train_df.copy()\n",
        "    te = test_df.copy()\n",
        "\n",
        "    # dtype 정규화(안전)\n",
        "    if 'building_id' in tr.columns:\n",
        "        tr['building_id'] = pd.to_numeric(tr['building_id'], errors='coerce').astype('Int64')\n",
        "    if 'building_id' in te.columns:\n",
        "        te['building_id'] = pd.to_numeric(te['building_id'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # 시간 피처 (반환형 함수이므로 대입 필수)  <-- FIX\n",
        "    tr = add_time_features(tr)\n",
        "    te = add_time_features(te)\n",
        "\n",
        "    # lag/rolling은 building 단위로 생성\n",
        "    tr = add_lag_roll(tr, group_keys=['building_id'], lag_hours=(1, 24, 168), roll_hours=(24, 168))\n",
        "\n",
        "    # 수치/결측 정리\n",
        "    tr.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    te.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    tr = tr.fillna(0)\n",
        "    te = te.fillna(0)\n",
        "\n",
        "    # 라그로 생기는 초기 구간 제거(정보누출 방지)  <-- 짧은 그룹 보호\n",
        "    def _cut_head(g, cut=200):\n",
        "        if len(g) <= cut:\n",
        "            return g.iloc[0:0]  # 빈 프레임\n",
        "        return g.iloc[cut:].copy()\n",
        "\n",
        "    tr = (\n",
        "        tr.sort_values(['building_id', 'datetime'])\n",
        "          .groupby('building_id', group_keys=False)\n",
        "          .apply(lambda g: _cut_head(g, 200))\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # 드랍 컬럼 설정\n",
        "    drop_cols = ['building_id', 'target', 'datetime', 'num_date_time']\n",
        "\n",
        "    tr_feat = tr.drop(columns=[c for c in drop_cols if c in tr.columns], errors='ignore')\n",
        "    te_feat = te.drop(columns=[c for c in drop_cols if c in te.columns], errors='ignore')\n",
        "\n",
        "    # 컬럼 정렬\n",
        "    tr_feat, te_feat = align_train_test_columns(tr_feat, te_feat)\n",
        "\n",
        "    # 재조합 (주의: building_id는 학습용 df에는 남기되, 모델 입력에서는 제외함)\n",
        "    tr = pd.concat(\n",
        "        [tr[['building_id', 'datetime', 'target']].reset_index(drop=True),\n",
        "         tr_feat.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "    te = pd.concat(\n",
        "        [te[['building_id', 'datetime', 'num_date_time']].reset_index(drop=True),\n",
        "         te_feat.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 혹시 모를 중복 컬럼 최종 방지\n",
        "    tr = tr.loc[:, ~tr.columns.duplicated(keep='first')]\n",
        "    te = te.loc[:, ~te.columns.duplicated(keep='first')]\n",
        "\n",
        "    preds = []\n",
        "    for b_id, g_tr in tr.groupby('building_id', sort=False):\n",
        "        g_te = te[te['building_id'] == b_id].copy()\n",
        "        if g_te.empty or g_tr.empty:\n",
        "            continue\n",
        "        try:\n",
        "            pred = walk_forward_building(g_tr, g_te)\n",
        "        except Exception as e:\n",
        "            log(f\"[B{b_id}] 오류 발생({e}); 시즌 나이브로 대체\")\n",
        "            pred = seasonal_naive(g_tr['target'].reset_index(drop=True), len(g_te), period=168)\n",
        "\n",
        "        preds.append(pd.DataFrame({\n",
        "            'num_date_time': g_te['num_date_time'].values,\n",
        "            'answer': pred\n",
        "        }))\n",
        "\n",
        "    pred_all = pd.concat(preds, axis=0, ignore_index=True) if preds else pd.DataFrame(columns=['num_date_time', 'answer'])\n",
        "    submission = test_df[['num_date_time']].merge(pred_all, on='num_date_time', how='left')\n",
        "    if submission['answer'].isna().any():\n",
        "        log(\"누락 예측 0으로 채움\")\n",
        "        submission['answer'] = submission['answer'].fillna(0.0)\n",
        "    submission['answer'] = submission['answer'].astype(float).clip(lower=0)\n",
        "    return submission[['num_date_time', 'answer']]"
      ],
      "metadata": {
        "id": "Fg1GnTN4h3g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = build_features_and_predict(train_df, test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eb04SY_7i1MN",
        "outputId": "8bbe0d50-a16e-443c-a3b1-0b6a68505d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09:24:52] [B1] Fold 1/10 SMAPE=10.690 (train 1080, valid 24)\n",
            "[09:24:57] [B1] Fold 2/10 SMAPE=5.810 (train 1151, valid 24)\n",
            "[09:25:03] [B1] Fold 3/10 SMAPE=2.823 (train 1223, valid 24)\n",
            "[09:25:09] [B1] Fold 4/10 SMAPE=3.986 (train 1294, valid 24)\n",
            "[09:25:15] [B1] Fold 5/10 SMAPE=5.140 (train 1366, valid 24)\n",
            "[09:25:21] [B1] Fold 6/10 SMAPE=4.276 (train 1437, valid 24)\n",
            "[09:25:27] [B1] Fold 7/10 SMAPE=5.826 (train 1509, valid 24)\n",
            "[09:25:33] [B1] Fold 8/10 SMAPE=2.906 (train 1580, valid 24)\n",
            "[09:25:39] [B1] Fold 9/10 SMAPE=5.041 (train 1652, valid 24)\n",
            "[09:25:45] [B1] Fold 10/10 SMAPE=4.303 (train 1724, valid 24)\n",
            "[09:25:57] [B2] Fold 1/10 SMAPE=4.926 (train 1080, valid 24)\n",
            "[09:26:03] [B2] Fold 2/10 SMAPE=4.415 (train 1151, valid 24)\n",
            "[09:26:08] [B2] Fold 3/10 SMAPE=2.514 (train 1223, valid 24)\n",
            "[09:26:14] [B2] Fold 4/10 SMAPE=3.508 (train 1294, valid 24)\n",
            "[09:26:20] [B2] Fold 5/10 SMAPE=3.362 (train 1366, valid 24)\n",
            "[09:26:26] [B2] Fold 6/10 SMAPE=3.636 (train 1437, valid 24)\n",
            "[09:26:32] [B2] Fold 7/10 SMAPE=2.663 (train 1509, valid 24)\n",
            "[09:26:38] [B2] Fold 8/10 SMAPE=2.419 (train 1580, valid 24)\n",
            "[09:26:44] [B2] Fold 9/10 SMAPE=3.162 (train 1652, valid 24)\n",
            "[09:26:50] [B2] Fold 10/10 SMAPE=2.473 (train 1724, valid 24)\n",
            "[09:27:01] [B3] Fold 1/10 SMAPE=3.019 (train 1080, valid 24)\n",
            "[09:27:07] [B3] Fold 2/10 SMAPE=2.966 (train 1151, valid 24)\n",
            "[09:27:12] [B3] Fold 3/10 SMAPE=1.731 (train 1223, valid 24)\n",
            "[09:27:17] [B3] Fold 4/10 SMAPE=1.494 (train 1294, valid 24)\n",
            "[09:27:23] [B3] Fold 5/10 SMAPE=3.509 (train 1366, valid 24)\n",
            "[09:27:28] [B3] Fold 6/10 SMAPE=1.342 (train 1437, valid 24)\n",
            "[09:27:34] [B3] Fold 7/10 SMAPE=1.507 (train 1509, valid 24)\n",
            "[09:27:40] [B3] Fold 8/10 SMAPE=1.209 (train 1580, valid 24)\n",
            "[09:27:45] [B3] Fold 9/10 SMAPE=0.908 (train 1652, valid 24)\n",
            "[09:27:51] [B3] Fold 10/10 SMAPE=1.393 (train 1724, valid 24)\n",
            "[09:28:02] [B4] Fold 1/10 SMAPE=3.906 (train 1080, valid 24)\n",
            "[09:28:08] [B4] Fold 2/10 SMAPE=2.960 (train 1151, valid 24)\n",
            "[09:28:14] [B4] Fold 3/10 SMAPE=2.557 (train 1223, valid 24)\n",
            "[09:28:19] [B4] Fold 4/10 SMAPE=3.139 (train 1294, valid 24)\n",
            "[09:28:25] [B4] Fold 5/10 SMAPE=2.816 (train 1366, valid 24)\n",
            "[09:28:31] [B4] Fold 6/10 SMAPE=2.285 (train 1437, valid 24)\n",
            "[09:28:37] [B4] Fold 7/10 SMAPE=2.374 (train 1509, valid 24)\n",
            "[09:28:43] [B4] Fold 8/10 SMAPE=3.412 (train 1580, valid 24)\n",
            "[09:28:49] [B4] Fold 9/10 SMAPE=1.883 (train 1652, valid 24)\n",
            "[09:28:55] [B4] Fold 10/10 SMAPE=4.608 (train 1724, valid 24)\n",
            "[09:29:06] [B5] Fold 1/10 SMAPE=1.352 (train 1080, valid 24)\n",
            "[09:29:11] [B5] Fold 2/10 SMAPE=3.291 (train 1151, valid 24)\n",
            "[09:29:16] [B5] Fold 3/10 SMAPE=1.581 (train 1223, valid 24)\n",
            "[09:29:22] [B5] Fold 4/10 SMAPE=0.950 (train 1294, valid 24)\n",
            "[09:29:27] [B5] Fold 5/10 SMAPE=2.765 (train 1366, valid 24)\n",
            "[09:29:33] [B5] Fold 6/10 SMAPE=1.315 (train 1437, valid 24)\n",
            "[09:29:38] [B5] Fold 7/10 SMAPE=1.034 (train 1509, valid 24)\n",
            "[09:29:43] [B5] Fold 8/10 SMAPE=0.915 (train 1580, valid 24)\n",
            "[09:29:49] [B5] Fold 9/10 SMAPE=1.490 (train 1652, valid 24)\n",
            "[09:29:55] [B5] Fold 10/10 SMAPE=1.307 (train 1724, valid 24)\n",
            "[09:30:06] [B6] Fold 1/10 SMAPE=5.896 (train 1080, valid 24)\n",
            "[09:30:11] [B6] Fold 2/10 SMAPE=8.560 (train 1151, valid 24)\n",
            "[09:30:17] [B6] Fold 3/10 SMAPE=2.988 (train 1223, valid 24)\n",
            "[09:30:22] [B6] Fold 4/10 SMAPE=2.802 (train 1294, valid 24)\n",
            "[09:30:28] [B6] Fold 5/10 SMAPE=4.265 (train 1366, valid 24)\n",
            "[09:30:34] [B6] Fold 6/10 SMAPE=2.773 (train 1437, valid 24)\n",
            "[09:30:40] [B6] Fold 7/10 SMAPE=7.401 (train 1509, valid 24)\n",
            "[09:30:46] [B6] Fold 8/10 SMAPE=4.952 (train 1580, valid 24)\n",
            "[09:30:52] [B6] Fold 9/10 SMAPE=11.708 (train 1652, valid 24)\n",
            "[09:30:58] [B6] Fold 10/10 SMAPE=3.339 (train 1724, valid 24)\n",
            "[09:31:09] [B7] Fold 1/10 SMAPE=6.518 (train 1080, valid 24)\n",
            "[09:31:15] [B7] Fold 2/10 SMAPE=5.637 (train 1151, valid 24)\n",
            "[09:31:21] [B7] Fold 3/10 SMAPE=3.499 (train 1223, valid 24)\n",
            "[09:31:27] [B7] Fold 4/10 SMAPE=3.517 (train 1294, valid 24)\n",
            "[09:31:32] [B7] Fold 5/10 SMAPE=13.061 (train 1366, valid 24)\n",
            "[09:31:38] [B7] Fold 6/10 SMAPE=8.824 (train 1437, valid 24)\n",
            "[09:31:44] [B7] Fold 7/10 SMAPE=11.235 (train 1509, valid 24)\n",
            "[09:31:51] [B7] Fold 8/10 SMAPE=7.850 (train 1580, valid 24)\n",
            "[09:31:57] [B7] Fold 9/10 SMAPE=5.454 (train 1652, valid 24)\n",
            "[09:32:03] [B7] Fold 10/10 SMAPE=9.968 (train 1724, valid 24)\n",
            "[09:32:14] [B8] Fold 1/10 SMAPE=2.092 (train 1080, valid 24)\n",
            "[09:32:20] [B8] Fold 2/10 SMAPE=21.504 (train 1151, valid 24)\n",
            "[09:32:26] [B8] Fold 3/10 SMAPE=1.009 (train 1223, valid 24)\n",
            "[09:32:31] [B8] Fold 4/10 SMAPE=5.638 (train 1294, valid 24)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-579315503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_features_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2482683642.py\u001b[0m in \u001b[0;36mbuild_features_and_predict\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk_forward_building\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[B{b_id}] 오류 발생({e}); 시즌 나이브로 대체\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2482683642.py\u001b[0m in \u001b[0;36mwalk_forward_building\u001b[0;34m(df_b, df_b_te)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mXGB_PARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mva_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1248\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             _check_call(\n\u001b[0;32m-> 2247\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2248\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 경로 설정 (필요 시 수정)\n",
        "DATA_DIR = \"/content/drive/MyDrive/KUBIG/25_summer_contest\"\n",
        "SAMPLE_SUB_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 2) 제출 DataFrame 확보\n",
        "if \"submission\" not in globals():\n",
        "    raise RuntimeError(\"submission DataFrame이 없습니다. 위 파이프라인 실행 후 다시 시도하세요.\")\n",
        "\n",
        "sub = submission.copy()\n",
        "\n",
        "# 2-1) 기본 컬럼/타입 안전화\n",
        "required_cols = {\"num_date_time\", \"answer\"}\n",
        "missing = required_cols - set(sub.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"submission에 누락된 컬럼: {missing}\")\n",
        "\n",
        "# num_date_time은 문자열로 고정(대회 포맷 오류 방지)\n",
        "sub[\"num_date_time\"] = sub[\"num_date_time\"].astype(str)\n",
        "\n",
        "# 혹시 중복 num_date_time이 있으면 평균으로 집계(또는 마지막값 사용하려면 tail(1))\n",
        "if sub[\"num_date_time\"].duplicated().any():\n",
        "    sub = (sub.groupby(\"num_date_time\", as_index=False)[\"answer\"]\n",
        "              .mean())  # 필요시 .last() 로 변경 가능\n",
        "\n",
        "# 3) 포맷 정렬: sample_submission.csv가 있으면 순서/행 일치시킴\n",
        "if os.path.exists(SAMPLE_SUB_CSV):\n",
        "    sample_sub = pd.read_csv(SAMPLE_SUB_CSV)\n",
        "    if \"num_date_time\" not in sample_sub.columns:\n",
        "        raise KeyError(\"sample_submission.csv에 num_date_time 컬럼이 없습니다.\")\n",
        "\n",
        "    # dtype 통일\n",
        "    sample_sub[\"num_date_time\"] = sample_sub[\"num_date_time\"].astype(str)\n",
        "\n",
        "    # 병합(누락은 NaN)\n",
        "    sub = sample_sub[[\"num_date_time\"]].merge(\n",
        "        sub[[\"num_date_time\", \"answer\"]], on=\"num_date_time\", how=\"left\"\n",
        "    )\n",
        "else:\n",
        "    # sample 미제공 시, 최소 컬럼 형태 보장\n",
        "    sub = sub[[\"num_date_time\", \"answer\"]].copy()\n",
        "\n",
        "# 4) 값 정리: NaN/inf 처리 및 하한 0\n",
        "sub[\"answer\"] = pd.to_numeric(sub[\"answer\"], errors=\"coerce\")\n",
        "sub.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "missing_cnt = int(sub[\"answer\"].isna().sum())\n",
        "if missing_cnt > 0:\n",
        "    print(f\"[경고] 예측 누락 {missing_cnt}건 → 0으로 대체합니다.\")\n",
        "    sub[\"answer\"] = sub[\"answer\"].fillna(0.0)\n",
        "\n",
        "# 음수 방지 및 소수점 자리수 제한(파일 크기/평가 안전)\n",
        "sub[\"answer\"] = sub[\"answer\"].astype(float).clip(lower=0)\n",
        "sub[\"answer\"] = sub[\"answer\"].round(6)\n",
        "\n",
        "# 5) 파일 저장: 타임스탬프 포함\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_path = os.path.join(OUTPUT_DIR, f\"submission_gpu_xgb_{ts}.csv\")\n",
        "sub.to_csv(out_path, index=False)\n",
        "print(f\"[완료] 제출 파일 저장: {out_path}\")\n",
        "\n",
        "# 6) 상위 몇 줄 미리보기 + 간단 검증\n",
        "print(sub.head())\n",
        "if os.path.exists(SAMPLE_SUB_CSV):\n",
        "    sample_rows = len(pd.read_csv(SAMPLE_SUB_CSV))\n",
        "    if len(sub) != sample_rows:\n",
        "        print(f\"[경고] 제출 행수({len(sub)})가 sample({sample_rows})과 다릅니다. 포맷 확인 필요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "fA7avNRFh3eS",
        "outputId": "e0f128ba-585b-45a5-e038-cefdb6375665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "submission DataFrame이 없습니다. 위 파이프라인 실행 후 다시 시도하세요.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3507680629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2) 제출 DataFrame 확보\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"submission\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission DataFrame이 없습니다. 위 파이프라인 실행 후 다시 시도하세요.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: submission DataFrame이 없습니다. 위 파이프라인 실행 후 다시 시도하세요."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##time split\n"
      ],
      "metadata": {
        "id": "0WBgI41MjgCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# 전역 기본값\n",
        "N_SPLITS = globals().get('N_SPLITS', 7)          # 권장: 5\n",
        "EMBARGO  = globals().get('EMBARGO', 48)          # 24~48 추천\n",
        "USE_LOG1P = globals().get('USE_LOG1P', True)\n",
        "DO_BLEND_SEASONAL = globals().get('DO_BLEND_SEASONAL', True)\n",
        "BLEND_ALPHA = globals().get('BLEND_ALPHA', 0.2)\n",
        "\n",
        "def _iter_timeseries_splits(n_samples: int, n_splits: int, test_size: int):\n",
        "    \"\"\"\n",
        "    sklearn 버전 상관없이 '확장형(Expanding) + 고정 test_size' 스플릿 생성기.\n",
        "    학습: [0 : train_end), 검증: [train_end + EMBARGO : train_end + EMBARGO + test_size)\n",
        "    \"\"\"\n",
        "    if n_samples <= test_size + 16:\n",
        "        return  # 너무 짧으면 폴드 생성 불가\n",
        "\n",
        "    # 균등하게 n_splits개 지점에서 검증 세그먼트 생성\n",
        "    # 각 fold의 검증 시작점 후보를 만들어준다.\n",
        "    # 검증 구간: [va_start, va_start + test_size)\n",
        "    # 학습 구간: [0, va_start - EMBARGO)\n",
        "    max_start = n_samples - test_size\n",
        "    # 시작 후보를 n_splits개 균등 분할(초기 학습 최소 16 보장)\n",
        "    starts = np.linspace(16 + EMBARGO, max_start, num=n_splits, dtype=int)\n",
        "    # 단조 증가/유효성 보장\n",
        "    starts = np.unique(starts[starts + test_size <= n_samples])\n",
        "    for va_start in starts:\n",
        "        va_end = va_start + test_size\n",
        "        tr_end = max(0, va_start - EMBARGO)\n",
        "        if tr_end < 16 or (va_end - va_start) < 8:\n",
        "            continue\n",
        "        tr_idx = np.arange(0, tr_end, dtype=int)\n",
        "        va_idx = np.arange(va_start, va_end, dtype=int)\n",
        "        if len(tr_idx) >= 16 and len(va_idx) >= 8:\n",
        "            yield tr_idx, va_idx\n",
        "\n",
        "def walk_forward_building(df_b: pd.DataFrame, df_b_te: pd.DataFrame) -> np.ndarray:\n",
        "    df_b = df_b.sort_values('datetime').reset_index(drop=True)\n",
        "    df_b_te = df_b_te.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "    # 피처 선택: 식별자/키 제외 + 숫자형만\n",
        "    exclude = {'target', 'datetime', 'num_date_time', 'building_id'}\n",
        "    feat_cols = [c for c in df_b.columns if c not in exclude and pd.api.types.is_numeric_dtype(df_b[c])]\n",
        "\n",
        "    if len(feat_cols) == 0:\n",
        "        log(f\"[B{int(df_b['building_id'].iloc[0])}] 사용 가능한 피처가 없어 seasonal naive 사용\")\n",
        "        return seasonal_naive(df_b['target'], horizon=len(df_b_te), period=168)\n",
        "\n",
        "    X = df_b[feat_cols].to_numpy()\n",
        "    X_te = df_b_te.reindex(columns=feat_cols, fill_value=0).to_numpy()\n",
        "    y = df_b['target'].astype(float).values\n",
        "    y_trn = np.log1p(y) if USE_LOG1P else y\n",
        "\n",
        "    # 안전값 처리\n",
        "    X = np.nan_to_num(X, copy=False, posinf=None, neginf=None)\n",
        "    X_te = np.nan_to_num(X_te, copy=False, posinf=None, neginf=None)\n",
        "\n",
        "    n = len(df_b)\n",
        "\n",
        "    # === TimeSeriesSplit 설정 ===\n",
        "    # 기본: 예측 호라이즌/주기로 168(1주) 권장. 데이터가 너무 짧으면 자동 축소.\n",
        "    TEST_SIZE = min(336, max(8, 168 if n >= 168*3 else int(n*0.1)))\n",
        "\n",
        "    # 우선 sklearn TimeSeriesSplit(test_size=...) 시도, 안 되면 커스텀 분할 사용\n",
        "    folds = []\n",
        "    try:\n",
        "        tscv = TimeSeriesSplit(n_splits=N_SPLITS, test_size=TEST_SIZE)\n",
        "        for tr_idx, va_idx in tscv.split(X):\n",
        "            # Embargo 적용\n",
        "            if EMBARGO > 0:\n",
        "                va_start = va_idx[0]\n",
        "                tr_keep = tr_idx[tr_idx < max(0, va_start - EMBARGO)]\n",
        "            else:\n",
        "                tr_keep = tr_idx\n",
        "            if len(tr_keep) >= 16 and len(va_idx) >= 8:\n",
        "                folds.append((tr_keep, va_idx))\n",
        "    except TypeError:\n",
        "        # test_size 인자가 없는 sklearn 버전 대비\n",
        "        for tr_idx, va_idx in _iter_timeseries_splits(n, N_SPLITS, TEST_SIZE):\n",
        "            folds.append((tr_idx, va_idx))\n",
        "\n",
        "    # 만약 위 시도에서 폴드가 거의 안 나왔다면 커스텀 분할로 보강\n",
        "    if len(folds) < max(2, int(N_SPLITS*0.6)):\n",
        "        folds = list(_iter_timeseries_splits(n, N_SPLITS, TEST_SIZE))\n",
        "\n",
        "    oof_pred = np.zeros(n, dtype=float)\n",
        "    any_fold = False\n",
        "\n",
        "    for i, (tr_idx, va_idx) in enumerate(folds, start=1):\n",
        "        X_tr, y_tr = X[tr_idx], y_trn[tr_idx]\n",
        "        X_va, y_va = X[va_idx], y[va_idx]\n",
        "\n",
        "        model = XGBRegressor(**XGB_PARAMS)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        va_pred = model.predict(X_va)\n",
        "        va_pred = np.expm1(va_pred) if USE_LOG1P else va_pred\n",
        "        va_pred = np.clip(va_pred, 0, None)\n",
        "        oof_pred[va_idx] = va_pred\n",
        "\n",
        "        s = smape(y_va, va_pred)\n",
        "        log(f\"[B{int(df_b['building_id'].iloc[0])}] Fold {i}/{len(folds)} \"\n",
        "            f\"(train {len(tr_idx)}, valid {len(va_idx)}), SMAPE={s:.3f}\")\n",
        "        del model; gc.collect()\n",
        "        any_fold = True\n",
        "\n",
        "    # 최종 학습(전체 구간)\n",
        "    final_model = XGBRegressor(**XGB_PARAMS)\n",
        "    final_model.fit(X, y_trn)\n",
        "    pred_te = final_model.predict(X_te)\n",
        "    pred_te = np.expm1(pred_te) if USE_LOG1P else pred_te\n",
        "    pred_te = np.clip(pred_te, 0, None)\n",
        "\n",
        "    # 시즌-나이브 블렌딩(옵션)\n",
        "    if DO_BLEND_SEASONAL:\n",
        "        horizon = len(df_b_te)\n",
        "        s_pred = seasonal_naive(df_b['target'], horizon=horizon, period=168)\n",
        "        pred_te = (1.0 - BLEND_ALPHA) * pred_te + BLEND_ALPHA * s_pred\n",
        "\n",
        "    if not any_fold:\n",
        "        log(f\"[B{int(df_b['building_id'].iloc[0])}] 유효한 폴드가 없어 seasonal naive 사용\")\n",
        "        return seasonal_naive(df_b['target'], horizon=len(df_b_te), period=168)\n",
        "\n",
        "    return pred_te\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 메인 파이프라인 (중복 building_id 제거/안전 groupby) ---\n",
        "def build_features_and_predict(train_df: pd.DataFrame, test_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    tr = train_df.copy()\n",
        "    te = test_df.copy()\n",
        "\n",
        "    # dtype 정규화(안전)\n",
        "    if 'building_id' in tr.columns:\n",
        "        tr['building_id'] = pd.to_numeric(tr['building_id'], errors='coerce').astype('Int64')\n",
        "    if 'building_id' in te.columns:\n",
        "        te['building_id'] = pd.to_numeric(te['building_id'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # 시간 피처 (반환형 함수이므로 대입 필수)  <-- FIX\n",
        "    tr = add_time_features(tr)\n",
        "    te = add_time_features(te)\n",
        "\n",
        "    # lag/rolling은 building 단위로 생성\n",
        "    tr = add_lag_roll(tr, group_keys=['building_id'], lag_hours=(1, 24, 168), roll_hours=(24, 168))\n",
        "\n",
        "    # 수치/결측 정리\n",
        "    tr.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    te.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    tr = tr.fillna(0)\n",
        "    te = te.fillna(0)\n",
        "\n",
        "    # 라그로 생기는 초기 구간 제거(정보누출 방지)  <-- 짧은 그룹 보호\n",
        "    def _cut_head(g, cut=200):\n",
        "        if len(g) <= cut:\n",
        "            return g.iloc[0:0]  # 빈 프레임\n",
        "        return g.iloc[cut:].copy()\n",
        "\n",
        "    tr = (\n",
        "        tr.sort_values(['building_id', 'datetime'])\n",
        "          .groupby('building_id', group_keys=False)\n",
        "          .apply(lambda g: _cut_head(g, 200))\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # 드랍 컬럼 설정\n",
        "    drop_cols = ['building_id', 'target', 'datetime', 'num_date_time']\n",
        "\n",
        "    tr_feat = tr.drop(columns=[c for c in drop_cols if c in tr.columns], errors='ignore')\n",
        "    te_feat = te.drop(columns=[c for c in drop_cols if c in te.columns], errors='ignore')\n",
        "\n",
        "    # 컬럼 정렬\n",
        "    tr_feat, te_feat = align_train_test_columns(tr_feat, te_feat)\n",
        "\n",
        "    # 재조합 (주의: building_id는 학습용 df에는 남기되, 모델 입력에서는 제외함)\n",
        "    tr = pd.concat(\n",
        "        [tr[['building_id', 'datetime', 'target']].reset_index(drop=True),\n",
        "         tr_feat.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "    te = pd.concat(\n",
        "        [te[['building_id', 'datetime', 'num_date_time']].reset_index(drop=True),\n",
        "         te_feat.reset_index(drop=True)],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 혹시 모를 중복 컬럼 최종 방지\n",
        "    tr = tr.loc[:, ~tr.columns.duplicated(keep='first')]\n",
        "    te = te.loc[:, ~te.columns.duplicated(keep='first')]\n",
        "\n",
        "    preds = []\n",
        "    for b_id, g_tr in tr.groupby('building_id', sort=False):\n",
        "        g_te = te[te['building_id'] == b_id].copy()\n",
        "        if g_te.empty or g_tr.empty:\n",
        "            continue\n",
        "        try:\n",
        "            pred = walk_forward_building(g_tr, g_te)\n",
        "        except Exception as e:\n",
        "            log(f\"[B{b_id}] 오류 발생({e}); 시즌 나이브로 대체\")\n",
        "            pred = seasonal_naive(g_tr['target'].reset_index(drop=True), len(g_te), period=168)\n",
        "\n",
        "        preds.append(pd.DataFrame({\n",
        "            'num_date_time': g_te['num_date_time'].values,\n",
        "            'answer': pred\n",
        "        }))\n",
        "\n",
        "    pred_all = pd.concat(preds, axis=0, ignore_index=True) if preds else pd.DataFrame(columns=['num_date_time', 'answer'])\n",
        "    submission = test_df[['num_date_time']].merge(pred_all, on='num_date_time', how='left')\n",
        "    if submission['answer'].isna().any():\n",
        "        log(\"누락 예측 0으로 채움\")\n",
        "        submission['answer'] = submission['answer'].fillna(0.0)\n",
        "    submission['answer'] = submission['answer'].astype(float).clip(lower=0)\n",
        "    return submission[['num_date_time', 'answer']]"
      ],
      "metadata": {
        "id": "HJuOP08jici4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = build_features_and_predict(train_df, test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "ZoH9-EuHicgq",
        "outputId": "11f9550a-b915-400a-c9c0-b1d29f4b6fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23:44:07] [B1] Fold 1/10 (train 136, valid 168), SMAPE=6.292\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-579315503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_features_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-974093415.py\u001b[0m in \u001b[0;36mbuild_features_and_predict\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk_forward_building\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[B{b_id}] 오류 발생({e}); 시즌 나이브로 대체\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-974093415.py\u001b[0m in \u001b[0;36mwalk_forward_building\u001b[0;34m(df_b, df_b_te)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mXGB_PARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mva_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1248\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m             _check_call(\n\u001b[0;32m-> 2247\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2248\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 경로 설정 (필요 시 수정)\n",
        "DATA_DIR = \"/content/drive/MyDrive/KUBIG/25_summer_contest\"\n",
        "SAMPLE_SUB_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 2) 제출 DataFrame 확보\n",
        "if \"submission\" not in globals():\n",
        "    raise RuntimeError(\"submission DataFrame이 없습니다. 위 파이프라인 실행 후 다시 시도하세요.\")\n",
        "\n",
        "sub = submission.copy()\n",
        "\n",
        "# 2-1) 기본 컬럼/타입 안전화\n",
        "required_cols = {\"num_date_time\", \"answer\"}\n",
        "missing = required_cols - set(sub.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"submission에 누락된 컬럼: {missing}\")\n",
        "\n",
        "# num_date_time은 문자열로 고정(대회 포맷 오류 방지)\n",
        "sub[\"num_date_time\"] = sub[\"num_date_time\"].astype(str)\n",
        "\n",
        "# 혹시 중복 num_date_time이 있으면 평균으로 집계(또는 마지막값 사용하려면 tail(1))\n",
        "if sub[\"num_date_time\"].duplicated().any():\n",
        "    sub = (sub.groupby(\"num_date_time\", as_index=False)[\"answer\"]\n",
        "              .mean())  # 필요시 .last() 로 변경 가능\n",
        "\n",
        "# 3) 포맷 정렬: sample_submission.csv가 있으면 순서/행 일치시킴\n",
        "if os.path.exists(SAMPLE_SUB_CSV):\n",
        "    sample_sub = pd.read_csv(SAMPLE_SUB_CSV)\n",
        "    if \"num_date_time\" not in sample_sub.columns:\n",
        "        raise KeyError(\"sample_submission.csv에 num_date_time 컬럼이 없습니다.\")\n",
        "\n",
        "    # dtype 통일\n",
        "    sample_sub[\"num_date_time\"] = sample_sub[\"num_date_time\"].astype(str)\n",
        "\n",
        "    # 병합(누락은 NaN)\n",
        "    sub = sample_sub[[\"num_date_time\"]].merge(\n",
        "        sub[[\"num_date_time\", \"answer\"]], on=\"num_date_time\", how=\"left\"\n",
        "    )\n",
        "else:\n",
        "    # sample 미제공 시, 최소 컬럼 형태 보장\n",
        "    sub = sub[[\"num_date_time\", \"answer\"]].copy()\n",
        "\n",
        "# 4) 값 정리: NaN/inf 처리 및 하한 0\n",
        "sub[\"answer\"] = pd.to_numeric(sub[\"answer\"], errors=\"coerce\")\n",
        "sub.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "missing_cnt = int(sub[\"answer\"].isna().sum())\n",
        "if missing_cnt > 0:\n",
        "    print(f\"[경고] 예측 누락 {missing_cnt}건 → 0으로 대체합니다.\")\n",
        "    sub[\"answer\"] = sub[\"answer\"].fillna(0.0)\n",
        "\n",
        "# 음수 방지 및 소수점 자리수 제한(파일 크기/평가 안전)\n",
        "sub[\"answer\"] = sub[\"answer\"].astype(float).clip(lower=0)\n",
        "sub[\"answer\"] = sub[\"answer\"].round(6)\n",
        "\n",
        "# 5) 파일 저장: 타임스탬프 포함\n",
        "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "out_path = os.path.join(OUTPUT_DIR, f\"submission_gpu_xgb_{ts}.csv\")\n",
        "sub.to_csv(out_path, index=False)\n",
        "print(f\"[완료] 제출 파일 저장: {out_path}\")\n",
        "\n",
        "# 6) 상위 몇 줄 미리보기 + 간단 검증\n",
        "print(sub.head())\n",
        "if os.path.exists(SAMPLE_SUB_CSV):\n",
        "    sample_rows = len(pd.read_csv(SAMPLE_SUB_CSV))\n",
        "    if len(sub) != sample_rows:\n",
        "        print(f\"[경고] 제출 행수({len(sub)})가 sample({sample_rows})과 다릅니다. 포맷 확인 필요.\")"
      ],
      "metadata": {
        "id": "M7v_O7YficeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##개선"
      ],
      "metadata": {
        "id": "yt0ftF2JIUVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# GPU XGBoost (SMAPE 개선판, 단일 파일 실행용)\n",
        "# - 도메인 파생변수(THI/불쾌지수/체감온도, wind_diff/hum_diff 등)\n",
        "# - 누출 방지 lag/rolling (shift(1)), t-24/t-168 계열\n",
        "# - Gap Walk‑Forward CV (시간 간격 비우기)\n",
        "# - Seed 앙상블 + Seasonal‑Naive 블렌딩(소량)\n",
        "# - 건물별 스케일 캘리브레이션(OOF 기반)\n",
        "# - 제출 파일 생성\n",
        "# 전제: train_df, test_df 가 이미 메모리에 존재 (Colab/Notebook)\n",
        "# ============================================================\n",
        "\n",
        "import os, gc, math, json, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# =========================\n",
        "# 설정\n",
        "# =========================\n",
        "SEED_LIST         = [2025, 1029, 77]      # 시드 앙상블\n",
        "N_SPLITS          = 10                     # Walk-forward 폴드 수\n",
        "GAP_HOURS         = 24                     # 폴드 사이 비우는 간격(누출 방지)\n",
        "USE_LOG1P_TARGET  = True                   # 타깃 로그스케일 학습\n",
        "CLIP_MIN          = 0.0                    # 음수 예측 방지\n",
        "BLEND_SEASONAL    = 0.10                   # Seasonal naive 소량 블렌딩 비율 (0~0.2 권장)\n",
        "BUILDING_RANGE    = None                   # 예: range(1, 101); None이면 데이터에서 자동\n",
        "SUBMISSION_IN     = '/content/drive/MyDrive/KUBIG/25_summer_contest/sample_submission.csv'\n",
        "SUBMISSION_OUT    = f'/content/drive/MyDrive/KUBIG/25_summer_contest/submission_gpu_xgb_smape_tuned.csv'\n",
        "\n",
        "# XGBoost 기본 하이퍼파라미터(성능 위주, 과적합 방지 강화)\n",
        "XGB_BASE = dict(\n",
        "    tree_method=\"hist\",        # xgboost>=2.0 에서 device='cuda'와 함께 사용\n",
        "    device=\"cuda\",\n",
        "    n_estimators=1800,\n",
        "    learning_rate=0.06,\n",
        "    max_depth=7,\n",
        "    min_child_weight=6.0,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=8.0,\n",
        "    gamma=0.0,\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 유틸\n",
        "# =========================\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    diff = np.abs(y_true - y_pred)\n",
        "    mask = denom != 0\n",
        "    return 100.0 * np.mean(diff[mask] / denom[mask])\n",
        "\n",
        "def seasonal_naive(series: pd.Series, horizon: int, period: int = 168) -> np.ndarray:\n",
        "    # 마지막 period 구간을 그대로 반복\n",
        "    if len(series) < period:\n",
        "        return np.repeat(series.iloc[-1], horizon).astype(float)\n",
        "    ref = series.iloc[-period:].values\n",
        "    reps = int(np.ceil(horizon / period))\n",
        "    return np.tile(ref, reps)[:horizon].astype(float)\n",
        "\n",
        "# =========================\n",
        "# 파생변수\n",
        "# =========================\n",
        "def add_time_features(df: pd.DataFrame):\n",
        "    dt = pd.to_datetime(df['datetime'])\n",
        "    df['hour']       = dt.dt.hour\n",
        "    df['dayofweek']  = dt.dt.dayofweek\n",
        "    df['week']       = dt.dt.isocalendar().week.astype(int)\n",
        "    df['month']      = dt.dt.month\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    # 주기 인코딩\n",
        "    df['sin_hour'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['cos_hour'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    df['sin_dow']  = np.sin(2*np.pi*df['dayofweek']/7.0)\n",
        "    df['cos_dow']  = np.cos(2*np.pi*df['dayofweek']/7.0)\n",
        "    # 여름 특화(6,7,8월만이지만 약한 위상 보정)\n",
        "    df['summer_cos'] = np.cos((df['month']-6) * np.pi/3)\n",
        "\n",
        "def add_weather_features(df: pd.DataFrame):\n",
        "    # 컬럼 표준화 가정:\n",
        "    # temperature(°C), humidity(%), wind_speed(m/s), precipitation(mm) 등 이름은 사전에 준비되어 있다고 가정\n",
        "    # 없는 컬럼은 생성\n",
        "    for col, default in [('temperature', np.nan), ('humidity', np.nan),\n",
        "                         ('wind_speed', np.nan), ('precipitation', 0.0)]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = default\n",
        "\n",
        "    # 온습도지수 THI (간단식): THI = T - (0.55 - 0.0055*RH)*(T - 14.5)\n",
        "    T  = pd.to_numeric(df['temperature'], errors='coerce')\n",
        "    RH = pd.to_numeric(df['humidity'], errors='coerce')\n",
        "    df['thi'] = T - (0.55 - 0.0055*RH) * (T - 14.5)\n",
        "\n",
        "    # 불쾌지수 DI (섭씨, %): DI = 0.81*T + 0.01*RH*(0.99*T - 14.3) + 46.3\n",
        "    df['discomfort_idx'] = 0.81*T + 0.01*RH*(0.99*T - 14.3) + 46.3\n",
        "\n",
        "    # 체감온도(간이식): AT = T + 0.2*(0.348*RH/100*(T-4) + 0.70*wind + 0.7)\n",
        "    W  = pd.to_numeric(df['wind_speed'], errors='coerce')\n",
        "    df['apparent_temp'] = T + 0.2*(0.348*(RH/100.0)*(T-4.0) + 0.70*W + 0.7)\n",
        "\n",
        "    # 차이형 파생\n",
        "    df['wind_diff'] = W.diff().fillna(0.0)\n",
        "    df['hum_diff']  = RH.diff().fillna(0.0)\n",
        "\n",
        "    # 강수 플래그\n",
        "    P  = pd.to_numeric(df['precipitation'], errors='coerce').fillna(0.0)\n",
        "    df['is_rain'] = (P > 0).astype(int)\n",
        "\n",
        "def _group_lag_roll(\n",
        "    df: pd.DataFrame,\n",
        "    group_key: str,\n",
        "    target_col: str,\n",
        "    lags: List[int],\n",
        "    rolls: List[Tuple[int, str]]\n",
        "):\n",
        "    g = df.groupby(group_key, sort=False)\n",
        "    for L in lags:\n",
        "        df[f'{target_col}_lag{L}'] = g[target_col].shift(1).shift(L-1)  # total L with leakage-safe shift(1)\n",
        "    for win, how in rolls:\n",
        "        if how == 'mean':\n",
        "            df[f'{target_col}_roll{win}_mean'] = g[target_col].shift(1).rolling(win).mean()\n",
        "        elif how == 'median':\n",
        "            df[f'{target_col}_roll{win}_median'] = g[target_col].shift(1).rolling(win).median()\n",
        "        elif how == 'std':\n",
        "            df[f'{target_col}_roll{win}_std'] = g[target_col].shift(1).rolling(win).std()\n",
        "\n",
        "def add_lag_roll(df: pd.DataFrame):\n",
        "    # target 컬럼 준비 (train에서는 이미 존재, test에는 없음 → skip)\n",
        "    if 'target' not in df.columns and 'consumption' in df.columns:\n",
        "        df['target'] = pd.to_numeric(df['consumption'], errors='coerce')\n",
        "\n",
        "    if 'target' in df.columns:\n",
        "        _group_lag_roll(\n",
        "            df=df,\n",
        "            group_key='building_id',\n",
        "            target_col='target',\n",
        "            lags=[1, 2, 3, 24, 48, 72, 168],\n",
        "            rolls=[(6,'mean'), (12,'mean'), (24,'mean'), (24,'std'), (168,'mean')]\n",
        "        )\n",
        "\n",
        "def build_feature_matrix(tr: pd.DataFrame, te: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
        "    # 공통 파생\n",
        "    add_time_features(tr); add_time_features(te)\n",
        "    add_weather_features(tr); add_weather_features(te)\n",
        "\n",
        "    # lag/roll (train만)\n",
        "    add_lag_roll(tr)\n",
        "\n",
        "    # test에 train에만 있는 열 생성 (일단 NaN으로)\n",
        "    for c in tr.columns:\n",
        "        if c not in te.columns:\n",
        "            te[c] = np.nan\n",
        "\n",
        "    # 절대 피처에 넣지 말아야 할 열들\n",
        "    hard_exclude = {\n",
        "        'datetime', 'num_date_time', 'answer',\n",
        "        'target', 'consumption', 'building_id',\n",
        "        'sunshine_hour', 'solar_radiation'\n",
        "    }\n",
        "\n",
        "    # 숫자형만, hard_exclude 제거\n",
        "    feat_cols = []\n",
        "    for c in tr.columns:\n",
        "        if c in hard_exclude:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(tr[c]):\n",
        "            feat_cols.append(c)\n",
        "    feat_cols = sorted(set(feat_cols))\n",
        "\n",
        "    # 결측 대체 (lag/roll에서 생긴 NaN 포함)\n",
        "    tr[feat_cols] = tr[feat_cols].fillna(0.0)\n",
        "    te[feat_cols] = te[feat_cols].fillna(0.0)\n",
        "\n",
        "    return tr, te, feat_cols\n",
        "\n",
        "# =========================\n",
        "# 모델링 (건물 단위)\n",
        "# =========================\n",
        "def train_predict_building(\n",
        "    df_b: pd.DataFrame,\n",
        "    df_b_te: pd.DataFrame,\n",
        "    feat_cols: List[str],\n",
        "    n_splits: int = N_SPLITS,\n",
        "    gap_hours: int = GAP_HOURS\n",
        ") -> Dict[str, Any]:\n",
        "    df_b = df_b.sort_values('datetime').reset_index(drop=True)\n",
        "    df_b_te = df_b_te.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "    # 사용할 피처 교차집합(안전)\n",
        "    use_cols = [c for c in feat_cols if c in df_b.columns]\n",
        "\n",
        "    if ('target' not in df_b.columns) or (len(use_cols) == 0):\n",
        "        log(\"  [Fallback] target/feature 없음 → seasonal naive 사용\")\n",
        "        y_tr = df_b['consumption'] if 'consumption' in df_b.columns else pd.Series([], dtype=float)\n",
        "        horizon = len(df_b_te)\n",
        "        pred = seasonal_naive(y_tr.reset_index(drop=True), horizon=horizon, period=168) if len(y_tr) > 0 else np.zeros(horizon)\n",
        "        return dict(oof_idx=np.array([], dtype=int), oof_pred=np.array([], dtype=float),\n",
        "                    test_pred=pred, fold_smape=[], oof_smape=np.nan)\n",
        "\n",
        "    # 항상 reindex로 안전하게 매칭 (KeyError 원천 차단)\n",
        "    X   = df_b.reindex(columns=use_cols,   fill_value=0.0).to_numpy(dtype=float)\n",
        "    X_te= df_b_te.reindex(columns=use_cols,fill_value=0.0).to_numpy(dtype=float)\n",
        "    y   = df_b['target'].astype(float).values\n",
        "\n",
        "    if USE_LOG1P_TARGET:\n",
        "        y_fit = np.log1p(y.clip(min=0.0))\n",
        "    else:\n",
        "        y_fit = y\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=None)\n",
        "    n = len(df_b)\n",
        "\n",
        "    log(f\"  [Info] samples={n}, features={len(use_cols)}, test_horizon={len(df_b_te)}\")\n",
        "\n",
        "    oof_pred = np.zeros(n, dtype=float)\n",
        "    fold_metrics = []\n",
        "    used_idx = []\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(tscv.split(X), start=1):\n",
        "        # gap 처리\n",
        "        if gap_hours > 0:\n",
        "            va_start = va_idx.min()\n",
        "            gap_start = max(0, va_start - gap_hours)\n",
        "            tr_idx = tr_idx[tr_idx < gap_start]\n",
        "\n",
        "        if len(tr_idx) == 0 or len(va_idx) == 0:\n",
        "            log(f\"  [Fold {fold}/{n_splits}] 건너뜀 (train={len(tr_idx)}, val={len(va_idx)})\")\n",
        "            continue\n",
        "\n",
        "        log(f\"  [Fold {fold}/{n_splits}] train={len(tr_idx)}, val={len(va_idx)}\")\n",
        "\n",
        "        X_tr, y_tr = X[tr_idx], y_fit[tr_idx]\n",
        "        X_va, y_va = X[va_idx], y_fit[va_idx]\n",
        "\n",
        "        # 시드 앙상블\n",
        "        va_pred_ens = np.zeros_like(y_va, dtype=float)\n",
        "        for sd in SEED_LIST:\n",
        "            set_seed(sd)\n",
        "            params = dict(XGB_BASE)\n",
        "            params.update(dict(random_state=sd))\n",
        "            model = XGBRegressor(**params)\n",
        "            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
        "            p = model.predict(X_va)\n",
        "            va_pred_ens += p / len(SEED_LIST)\n",
        "\n",
        "        # 역변환\n",
        "        va_pred = np.expm1(va_pred_ens) if USE_LOG1P_TARGET else va_pred_ens\n",
        "        va_pred = np.clip(va_pred, CLIP_MIN, None)\n",
        "        oof_pred[va_idx] = va_pred\n",
        "        used_idx.extend(list(va_idx))\n",
        "\n",
        "        fold_sm = smape(np.expm1(y_va) if USE_LOG1P_TARGET else y_va, va_pred)\n",
        "        fold_metrics.append(float(fold_sm))\n",
        "        log(f\"    → Fold SMAPE: {fold_sm:.3f}%\")\n",
        "\n",
        "    used_idx = np.array(sorted(set(used_idx)), dtype=int)\n",
        "    if len(used_idx) > 0:\n",
        "        y_oof_true = y[used_idx]\n",
        "        y_oof_true_lin = np.expm1(y_oof_true) if USE_LOG1P_TARGET else y_oof_true\n",
        "        oof_sm = smape(y_oof_true_lin, oof_pred[used_idx])\n",
        "    else:\n",
        "        oof_sm = np.nan\n",
        "\n",
        "    # 스케일 보정\n",
        "    scale_factor = 1.0\n",
        "    if len(used_idx) > 0:\n",
        "        eps = 1e-6\n",
        "        mean_true = float(np.mean(y_oof_true_lin) + eps)\n",
        "        mean_pred = float(np.mean(oof_pred[used_idx]) + eps)\n",
        "        scale_factor = (mean_true / mean_pred)\n",
        "\n",
        "    # 풀데이터 재학습 → 테스트 예측\n",
        "    test_pred_ens = np.zeros(len(df_b_te), dtype=float)\n",
        "    for sd in SEED_LIST:\n",
        "        set_seed(sd)\n",
        "        params = dict(XGB_BASE)\n",
        "        params.update(dict(random_state=sd))\n",
        "        model = XGBRegressor(**params)\n",
        "        model.fit(X, y_fit, verbose=False)\n",
        "        test_pred_ens += model.predict(X_te) / len(SEED_LIST)\n",
        "\n",
        "    test_pred = np.expm1(test_pred_ens) if USE_LOG1P_TARGET else test_pred_ens\n",
        "    test_pred = np.clip(test_pred * scale_factor, CLIP_MIN, None)\n",
        "\n",
        "    # seasonal-naive 블렌딩\n",
        "    if 'consumption' in df_b.columns:\n",
        "        sn = seasonal_naive(df_b['consumption'].reset_index(drop=True), horizon=len(df_b_te), period=168)\n",
        "        test_pred = (1.0 - BLEND_SEASONAL) * test_pred + BLEND_SEASONAL * sn\n",
        "\n",
        "    log(f\"  [Done] OOF SMAPE={oof_sm:.3f}%  scale={scale_factor:.4f}\")\n",
        "    return dict(\n",
        "        oof_idx=used_idx,\n",
        "        oof_pred=oof_pred,\n",
        "        test_pred=test_pred,\n",
        "        fold_smape=fold_metrics,\n",
        "        oof_smape=float(oof_sm)\n",
        "    )\n",
        "# =========================\n",
        "# 메인 파이프라인\n",
        "# =========================\n",
        "def run_pipeline(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "    tr = train_df.copy()\n",
        "    te = test_df.copy()\n",
        "\n",
        "    # 키/타깃 표준화\n",
        "    # train: 'consumption' 컬럼이 타깃이라고 가정\n",
        "    if 'target' not in tr.columns:\n",
        "        tr['target'] = pd.to_numeric(tr.get('consumption', np.nan), errors='coerce')\n",
        "\n",
        "    # ID/시간 정렬 및 타입 정리\n",
        "    if 'building_id' not in tr.columns:\n",
        "        # 원래 '건물번호'였다면 매핑 필요\n",
        "        if '건물번호' in tr.columns: tr = tr.rename(columns={'건물번호':'building_id'})\n",
        "        if '건물번호' in te.columns: te = te.rename(columns={'건물번호':'building_id'})\n",
        "    tr['building_id'] = pd.to_numeric(tr['building_id'], errors='coerce').astype(int)\n",
        "    te['building_id'] = pd.to_numeric(te['building_id'], errors='coerce').astype(int)\n",
        "\n",
        "    tr['datetime'] = pd.to_datetime(tr['datetime'])\n",
        "    te['datetime'] = pd.to_datetime(te['datetime'])\n",
        "\n",
        "    tr = tr.sort_values(['building_id','datetime']).reset_index(drop=True)\n",
        "    te = te.sort_values(['building_id','datetime']).reset_index(drop=True)\n",
        "\n",
        "    # 피처 구성\n",
        "    tr_feat, te_feat, feat_cols = build_feature_matrix(tr, te)\n",
        "\n",
        "    # test에 target 생성 방지\n",
        "    if 'target' in te_feat.columns:\n",
        "        te_feat = te_feat.drop(columns=['target'])\n",
        "\n",
        "    # 대상 건물 목록\n",
        "    if BUILDING_RANGE is None:\n",
        "        b_list = sorted(tr_feat['building_id'].unique().tolist())\n",
        "    else:\n",
        "        b_list = list(BUILDING_RANGE)\n",
        "\n",
        "    # 결과 저장\n",
        "    results = {}\n",
        "    all_oof_idx = []\n",
        "    all_oof_pred = []\n",
        "    all_oof_true = []\n",
        "\n",
        "    # 제출 템플릿\n",
        "    sub = pd.read_csv(SUBMISSION_IN)\n",
        "    # 제출 키가 'num_date_time' 기반이면 그대로 사용\n",
        "    # test와의 매핑을 위해 보조 키 생성 (building_id + datetime)\n",
        "    te_feat['key'] = te_feat['building_id'].astype(str) + '_' + te_feat['datetime'].dt.strftime('%Y%m%d_%H')\n",
        "    # sample_submission에도 동일 키 생성 시도\n",
        "    if 'num_date_time' in sub.columns:\n",
        "        # 예: \"3_20210801_00\"\n",
        "        sub['key'] = sub['num_date_time'].astype(str)\n",
        "\n",
        "    # 건물 루프\n",
        "    for b in b_list:\n",
        "        df_b = tr_feat[tr_feat['building_id'] == b].copy()\n",
        "        df_b_te = te_feat[te_feat['building_id'] == b].copy()\n",
        "\n",
        "        if len(df_b_te) == 0:\n",
        "            continue\n",
        "\n",
        "        res = train_predict_building(df_b, df_b_te, feat_cols)\n",
        "        results[b] = res\n",
        "\n",
        "        # OOF 수집\n",
        "        if len(res['oof_idx']) > 0:\n",
        "            sel = df_b.iloc[res['oof_idx']]\n",
        "            all_oof_idx.append(sel.index.values)\n",
        "            all_oof_pred.append(res['oof_pred'][res['oof_idx']])\n",
        "            # y_true (lin scale)\n",
        "            y_true = df_b['target'].values\n",
        "            if USE_LOG1P_TARGET:\n",
        "                all_oof_true.append(np.expm1(y_true[res['oof_idx']]))\n",
        "            else:\n",
        "                all_oof_true.append(y_true[res['oof_idx']])\n",
        "\n",
        "        # 테스트 예측 매핑\n",
        "        pred_b = res['test_pred']\n",
        "        keys_b = df_b_te['key'].values\n",
        "        m = pd.DataFrame({'key': keys_b, 'answer': pred_b})\n",
        "        sub = sub.merge(m, on='key', how='left', suffixes=('', f'_b{b}'))\n",
        "        # answer 채우기\n",
        "        sub['answer'] = sub['answer'].fillna(sub[f'answer_b{b}'])\n",
        "        sub = sub.drop(columns=[c for c in sub.columns if c.startswith('answer_b')])\n",
        "\n",
        "        # 메모리 정리\n",
        "        del df_b, df_b_te, res, m\n",
        "        gc.collect()\n",
        "\n",
        "    # OOF SMAPE 출력\n",
        "    if len(all_oof_idx) > 0:\n",
        "        y_true_all = np.concatenate(all_oof_true)\n",
        "        y_pred_all = np.concatenate(all_oof_pred)\n",
        "        oof_sm = smape(y_true_all, y_pred_all)\n",
        "    else:\n",
        "        oof_sm = np.nan\n",
        "\n",
        "    print(f\"[Global] OOF SMAPE: {oof_sm:.4f}%\")\n",
        "    # 폴드별은 각 건물 res['fold_smape'] 참고 가능\n",
        "\n",
        "    # 제출 마무리\n",
        "    if 'answer' not in sub.columns:\n",
        "        sub['answer'] = 0.0\n",
        "    sub_final = sub[['num_date_time','answer']].copy()\n",
        "    sub_final['answer'] = sub_final['answer'].fillna(0.0).clip(CLIP_MIN, None)\n",
        "\n",
        "    # 저장\n",
        "    os.makedirs(os.path.dirname(SUBMISSION_OUT), exist_ok=True)\n",
        "    sub_final.to_csv(SUBMISSION_OUT, index=False)\n",
        "    print(f\"Saved submission → {SUBMISSION_OUT}\")\n",
        "\n",
        "    return dict(\n",
        "        submission=sub_final,\n",
        "        results=results,\n",
        "        oof_smape=oof_sm\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# 실행\n",
        "# =========================\n",
        "out = run_pipeline(train_df, test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-Fl8GZ4h3XJ",
        "outputId": "5adcbabf-cbd7-4590-f34e-d020445bd38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23:44:14]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:44:14]   [Fold 1/10] train=166, val=185\n",
            "[23:44:19]     → Fold SMAPE: 2.678%\n",
            "[23:44:19]   [Fold 2/10] train=351, val=185\n",
            "[23:44:24]     → Fold SMAPE: 2.966%\n",
            "[23:44:24]   [Fold 3/10] train=536, val=185\n",
            "[23:44:29]     → Fold SMAPE: 1.330%\n",
            "[23:44:29]   [Fold 4/10] train=721, val=185\n",
            "[23:44:34]     → Fold SMAPE: 2.483%\n",
            "[23:44:34]   [Fold 5/10] train=906, val=185\n",
            "[23:44:40]     → Fold SMAPE: 1.745%\n",
            "[23:44:40]   [Fold 6/10] train=1091, val=185\n",
            "[23:44:45]     → Fold SMAPE: 1.509%\n",
            "[23:44:45]   [Fold 7/10] train=1276, val=185\n",
            "[23:44:50]     → Fold SMAPE: 1.418%\n",
            "[23:44:50]   [Fold 8/10] train=1461, val=185\n",
            "[23:44:56]     → Fold SMAPE: 1.807%\n",
            "[23:44:56]   [Fold 9/10] train=1646, val=185\n",
            "[23:45:02]     → Fold SMAPE: 0.962%\n",
            "[23:45:02]   [Fold 10/10] train=1831, val=185\n",
            "[23:45:07]     → Fold SMAPE: 1.631%\n",
            "[23:45:11]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:45:11]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:45:11]   [Fold 1/10] train=166, val=185\n",
            "[23:45:16]     → Fold SMAPE: 3.414%\n",
            "[23:45:16]   [Fold 2/10] train=351, val=185\n",
            "[23:45:21]     → Fold SMAPE: 2.373%\n",
            "[23:45:21]   [Fold 3/10] train=536, val=185\n",
            "[23:45:27]     → Fold SMAPE: 1.474%\n",
            "[23:45:27]   [Fold 4/10] train=721, val=185\n",
            "[23:45:32]     → Fold SMAPE: 1.341%\n",
            "[23:45:32]   [Fold 5/10] train=906, val=185\n",
            "[23:45:37]     → Fold SMAPE: 1.288%\n",
            "[23:45:37]   [Fold 6/10] train=1091, val=185\n",
            "[23:45:42]     → Fold SMAPE: 1.030%\n",
            "[23:45:42]   [Fold 7/10] train=1276, val=185\n",
            "[23:45:48]     → Fold SMAPE: 0.834%\n",
            "[23:45:48]   [Fold 8/10] train=1461, val=185\n",
            "[23:45:53]     → Fold SMAPE: 0.862%\n",
            "[23:45:53]   [Fold 9/10] train=1646, val=185\n",
            "[23:45:59]     → Fold SMAPE: 1.000%\n",
            "[23:45:59]   [Fold 10/10] train=1831, val=185\n",
            "[23:46:04]     → Fold SMAPE: 0.729%\n",
            "[23:46:08]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:46:08]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:46:08]   [Fold 1/10] train=166, val=185\n",
            "[23:46:13]     → Fold SMAPE: 4.873%\n",
            "[23:46:13]   [Fold 2/10] train=351, val=185\n",
            "[23:46:18]     → Fold SMAPE: 2.449%\n",
            "[23:46:18]   [Fold 3/10] train=536, val=185\n",
            "[23:46:23]     → Fold SMAPE: 1.028%\n",
            "[23:46:23]   [Fold 4/10] train=721, val=185\n",
            "[23:46:28]     → Fold SMAPE: 1.749%\n",
            "[23:46:28]   [Fold 5/10] train=906, val=185\n",
            "[23:46:32]     → Fold SMAPE: 1.770%\n",
            "[23:46:32]   [Fold 6/10] train=1091, val=185\n",
            "[23:46:37]     → Fold SMAPE: 2.351%\n",
            "[23:46:37]   [Fold 7/10] train=1276, val=185\n",
            "[23:46:42]     → Fold SMAPE: 2.893%\n",
            "[23:46:42]   [Fold 8/10] train=1461, val=185\n",
            "[23:46:48]     → Fold SMAPE: 1.866%\n",
            "[23:46:48]   [Fold 9/10] train=1646, val=185\n",
            "[23:46:53]     → Fold SMAPE: 1.755%\n",
            "[23:46:53]   [Fold 10/10] train=1831, val=185\n",
            "[23:46:58]     → Fold SMAPE: 1.301%\n",
            "[23:47:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:47:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:47:01]   [Fold 1/10] train=166, val=185\n",
            "[23:47:06]     → Fold SMAPE: 9.548%\n",
            "[23:47:06]   [Fold 2/10] train=351, val=185\n",
            "[23:47:11]     → Fold SMAPE: 4.180%\n",
            "[23:47:11]   [Fold 3/10] train=536, val=185\n",
            "[23:47:16]     → Fold SMAPE: 1.492%\n",
            "[23:47:16]   [Fold 4/10] train=721, val=185\n",
            "[23:47:21]     → Fold SMAPE: 1.766%\n",
            "[23:47:21]   [Fold 5/10] train=906, val=185\n",
            "[23:47:26]     → Fold SMAPE: 2.006%\n",
            "[23:47:26]   [Fold 6/10] train=1091, val=185\n",
            "[23:47:31]     → Fold SMAPE: 1.352%\n",
            "[23:47:31]   [Fold 7/10] train=1276, val=185\n",
            "[23:47:36]     → Fold SMAPE: 2.488%\n",
            "[23:47:36]   [Fold 8/10] train=1461, val=185\n",
            "[23:47:42]     → Fold SMAPE: 1.887%\n",
            "[23:47:42]   [Fold 9/10] train=1646, val=185\n",
            "[23:47:47]     → Fold SMAPE: 0.965%\n",
            "[23:47:47]   [Fold 10/10] train=1831, val=185\n",
            "[23:47:53]     → Fold SMAPE: 1.046%\n",
            "[23:47:56]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:47:56]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:47:56]   [Fold 1/10] train=166, val=185\n",
            "[23:48:01]     → Fold SMAPE: 5.454%\n",
            "[23:48:01]   [Fold 2/10] train=351, val=185\n",
            "[23:48:06]     → Fold SMAPE: 2.350%\n",
            "[23:48:06]   [Fold 3/10] train=536, val=185\n",
            "[23:48:11]     → Fold SMAPE: 0.784%\n",
            "[23:48:11]   [Fold 4/10] train=721, val=185\n",
            "[23:48:15]     → Fold SMAPE: 0.525%\n",
            "[23:48:15]   [Fold 5/10] train=906, val=185\n",
            "[23:48:20]     → Fold SMAPE: 0.482%\n",
            "[23:48:20]   [Fold 6/10] train=1091, val=185\n",
            "[23:48:25]     → Fold SMAPE: 0.427%\n",
            "[23:48:25]   [Fold 7/10] train=1276, val=185\n",
            "[23:48:31]     → Fold SMAPE: 0.700%\n",
            "[23:48:31]   [Fold 8/10] train=1461, val=185\n",
            "[23:48:36]     → Fold SMAPE: 1.653%\n",
            "[23:48:36]   [Fold 9/10] train=1646, val=185\n",
            "[23:48:41]     → Fold SMAPE: 1.106%\n",
            "[23:48:41]   [Fold 10/10] train=1831, val=185\n",
            "[23:48:46]     → Fold SMAPE: 1.491%\n",
            "[23:48:49]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:48:49]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:48:49]   [Fold 1/10] train=166, val=185\n",
            "[23:48:54]     → Fold SMAPE: 5.102%\n",
            "[23:48:54]   [Fold 2/10] train=351, val=185\n",
            "[23:48:59]     → Fold SMAPE: 3.115%\n",
            "[23:48:59]   [Fold 3/10] train=536, val=185\n",
            "[23:49:04]     → Fold SMAPE: 2.246%\n",
            "[23:49:04]   [Fold 4/10] train=721, val=185\n",
            "[23:49:09]     → Fold SMAPE: 2.178%\n",
            "[23:49:09]   [Fold 5/10] train=906, val=185\n",
            "[23:49:14]     → Fold SMAPE: 2.548%\n",
            "[23:49:14]   [Fold 6/10] train=1091, val=185\n",
            "[23:49:20]     → Fold SMAPE: 2.362%\n",
            "[23:49:20]   [Fold 7/10] train=1276, val=185\n",
            "[23:49:25]     → Fold SMAPE: 3.540%\n",
            "[23:49:25]   [Fold 8/10] train=1461, val=185\n",
            "[23:49:31]     → Fold SMAPE: 1.926%\n",
            "[23:49:31]   [Fold 9/10] train=1646, val=185\n",
            "[23:49:36]     → Fold SMAPE: 2.421%\n",
            "[23:49:36]   [Fold 10/10] train=1831, val=185\n",
            "[23:49:42]     → Fold SMAPE: 1.291%\n",
            "[23:49:46]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:49:46]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:49:46]   [Fold 1/10] train=166, val=185\n",
            "[23:49:51]     → Fold SMAPE: 5.079%\n",
            "[23:49:51]   [Fold 2/10] train=351, val=185\n",
            "[23:49:56]     → Fold SMAPE: 3.003%\n",
            "[23:49:56]   [Fold 3/10] train=536, val=185\n",
            "[23:50:00]     → Fold SMAPE: 1.360%\n",
            "[23:50:00]   [Fold 4/10] train=721, val=185\n",
            "[23:50:05]     → Fold SMAPE: 17.570%\n",
            "[23:50:05]   [Fold 5/10] train=906, val=185\n",
            "[23:50:10]     → Fold SMAPE: 8.225%\n",
            "[23:50:10]   [Fold 6/10] train=1091, val=185\n",
            "[23:50:16]     → Fold SMAPE: 3.819%\n",
            "[23:50:16]   [Fold 7/10] train=1276, val=185\n",
            "[23:50:21]     → Fold SMAPE: 1.527%\n",
            "[23:50:21]   [Fold 8/10] train=1461, val=185\n",
            "[23:50:27]     → Fold SMAPE: 6.656%\n",
            "[23:50:27]   [Fold 9/10] train=1646, val=185\n",
            "[23:50:32]     → Fold SMAPE: 7.196%\n",
            "[23:50:32]   [Fold 10/10] train=1831, val=185\n",
            "[23:50:38]     → Fold SMAPE: 1.721%\n",
            "[23:50:42]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:50:42]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:50:42]   [Fold 1/10] train=166, val=185\n",
            "[23:50:46]     → Fold SMAPE: 6.851%\n",
            "[23:50:46]   [Fold 2/10] train=351, val=185\n",
            "[23:50:51]     → Fold SMAPE: 3.411%\n",
            "[23:50:51]   [Fold 3/10] train=536, val=185\n",
            "[23:50:56]     → Fold SMAPE: 1.486%\n",
            "[23:50:56]   [Fold 4/10] train=721, val=185\n",
            "[23:51:01]     → Fold SMAPE: 1.300%\n",
            "[23:51:01]   [Fold 5/10] train=906, val=185\n",
            "[23:51:07]     → Fold SMAPE: 1.055%\n",
            "[23:51:07]   [Fold 6/10] train=1091, val=185\n",
            "[23:51:12]     → Fold SMAPE: 4.295%\n",
            "[23:51:12]   [Fold 7/10] train=1276, val=185\n",
            "[23:51:18]     → Fold SMAPE: 1.298%\n",
            "[23:51:18]   [Fold 8/10] train=1461, val=185\n",
            "[23:51:24]     → Fold SMAPE: 1.319%\n",
            "[23:51:24]   [Fold 9/10] train=1646, val=185\n",
            "[23:51:30]     → Fold SMAPE: 1.032%\n",
            "[23:51:30]   [Fold 10/10] train=1831, val=185\n",
            "[23:51:36]     → Fold SMAPE: 0.970%\n",
            "[23:51:40]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:51:40]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:51:40]   [Fold 1/10] train=166, val=185\n",
            "[23:51:45]     → Fold SMAPE: 6.830%\n",
            "[23:51:45]   [Fold 2/10] train=351, val=185\n",
            "[23:51:50]     → Fold SMAPE: 3.081%\n",
            "[23:51:50]   [Fold 3/10] train=536, val=185\n",
            "[23:51:55]     → Fold SMAPE: 1.260%\n",
            "[23:51:55]   [Fold 4/10] train=721, val=185\n",
            "[23:51:59]     → Fold SMAPE: 1.351%\n",
            "[23:51:59]   [Fold 5/10] train=906, val=185\n",
            "[23:52:04]     → Fold SMAPE: 3.928%\n",
            "[23:52:04]   [Fold 6/10] train=1091, val=185\n",
            "[23:52:10]     → Fold SMAPE: 1.411%\n",
            "[23:52:10]   [Fold 7/10] train=1276, val=185\n",
            "[23:52:15]     → Fold SMAPE: 5.013%\n",
            "[23:52:15]   [Fold 8/10] train=1461, val=185\n",
            "[23:52:20]     → Fold SMAPE: 3.715%\n",
            "[23:52:20]   [Fold 9/10] train=1646, val=185\n",
            "[23:52:26]     → Fold SMAPE: 0.985%\n",
            "[23:52:26]   [Fold 10/10] train=1831, val=185\n",
            "[23:52:31]     → Fold SMAPE: 0.860%\n",
            "[23:52:35]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:52:35]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:52:35]   [Fold 1/10] train=166, val=185\n",
            "[23:52:40]     → Fold SMAPE: 3.603%\n",
            "[23:52:40]   [Fold 2/10] train=351, val=185\n",
            "[23:52:45]     → Fold SMAPE: 3.368%\n",
            "[23:52:45]   [Fold 3/10] train=536, val=185\n",
            "[23:52:50]     → Fold SMAPE: 1.908%\n",
            "[23:52:50]   [Fold 4/10] train=721, val=185\n",
            "[23:52:55]     → Fold SMAPE: 24.753%\n",
            "[23:52:55]   [Fold 5/10] train=906, val=185\n",
            "[23:53:00]     → Fold SMAPE: 4.741%\n",
            "[23:53:00]   [Fold 6/10] train=1091, val=185\n",
            "[23:53:05]     → Fold SMAPE: 1.225%\n",
            "[23:53:05]   [Fold 7/10] train=1276, val=185\n",
            "[23:53:11]     → Fold SMAPE: 4.756%\n",
            "[23:53:11]   [Fold 8/10] train=1461, val=185\n",
            "[23:53:16]     → Fold SMAPE: 3.775%\n",
            "[23:53:16]   [Fold 9/10] train=1646, val=185\n",
            "[23:53:21]     → Fold SMAPE: 1.218%\n",
            "[23:53:21]   [Fold 10/10] train=1831, val=185\n",
            "[23:53:27]     → Fold SMAPE: 1.142%\n",
            "[23:53:30]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:53:31]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:53:31]   [Fold 1/10] train=166, val=185\n",
            "[23:53:35]     → Fold SMAPE: 6.957%\n",
            "[23:53:35]   [Fold 2/10] train=351, val=185\n",
            "[23:53:40]     → Fold SMAPE: 5.387%\n",
            "[23:53:40]   [Fold 3/10] train=536, val=185\n",
            "[23:53:45]     → Fold SMAPE: 1.755%\n",
            "[23:53:45]   [Fold 4/10] train=721, val=185\n",
            "[23:53:50]     → Fold SMAPE: 5.191%\n",
            "[23:53:50]   [Fold 5/10] train=906, val=185\n",
            "[23:53:55]     → Fold SMAPE: 1.106%\n",
            "[23:53:55]   [Fold 6/10] train=1091, val=185\n",
            "[23:54:00]     → Fold SMAPE: 3.161%\n",
            "[23:54:00]   [Fold 7/10] train=1276, val=185\n",
            "[23:54:06]     → Fold SMAPE: 5.055%\n",
            "[23:54:06]   [Fold 8/10] train=1461, val=185\n",
            "[23:54:11]     → Fold SMAPE: 3.267%\n",
            "[23:54:11]   [Fold 9/10] train=1646, val=185\n",
            "[23:54:16]     → Fold SMAPE: 1.489%\n",
            "[23:54:16]   [Fold 10/10] train=1831, val=185\n",
            "[23:54:22]     → Fold SMAPE: 1.502%\n",
            "[23:54:25]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:54:25]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:54:25]   [Fold 1/10] train=166, val=185\n",
            "[23:54:30]     → Fold SMAPE: 5.270%\n",
            "[23:54:30]   [Fold 2/10] train=351, val=185\n",
            "[23:54:35]     → Fold SMAPE: 1.920%\n",
            "[23:54:35]   [Fold 3/10] train=536, val=185\n",
            "[23:54:40]     → Fold SMAPE: 0.784%\n",
            "[23:54:40]   [Fold 4/10] train=721, val=185\n",
            "[23:54:44]     → Fold SMAPE: 0.482%\n",
            "[23:54:44]   [Fold 5/10] train=906, val=185\n",
            "[23:54:49]     → Fold SMAPE: 0.496%\n",
            "[23:54:49]   [Fold 6/10] train=1091, val=185\n",
            "[23:54:54]     → Fold SMAPE: 2.053%\n",
            "[23:54:54]   [Fold 7/10] train=1276, val=185\n",
            "[23:55:00]     → Fold SMAPE: 2.455%\n",
            "[23:55:00]   [Fold 8/10] train=1461, val=185\n",
            "[23:55:05]     → Fold SMAPE: 1.986%\n",
            "[23:55:05]   [Fold 9/10] train=1646, val=185\n",
            "[23:55:10]     → Fold SMAPE: 0.724%\n",
            "[23:55:10]   [Fold 10/10] train=1831, val=185\n",
            "[23:55:15]     → Fold SMAPE: 1.198%\n",
            "[23:55:19]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:55:19]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:55:19]   [Fold 1/10] train=166, val=185\n",
            "[23:55:24]     → Fold SMAPE: 3.460%\n",
            "[23:55:24]   [Fold 2/10] train=351, val=185\n",
            "[23:55:29]     → Fold SMAPE: 3.539%\n",
            "[23:55:29]   [Fold 3/10] train=536, val=185\n",
            "[23:55:34]     → Fold SMAPE: 1.609%\n",
            "[23:55:34]   [Fold 4/10] train=721, val=185\n",
            "[23:55:39]     → Fold SMAPE: 1.855%\n",
            "[23:55:39]   [Fold 5/10] train=906, val=185\n",
            "[23:55:44]     → Fold SMAPE: 1.396%\n",
            "[23:55:44]   [Fold 6/10] train=1091, val=185\n",
            "[23:55:49]     → Fold SMAPE: 3.767%\n",
            "[23:55:49]   [Fold 7/10] train=1276, val=185\n",
            "[23:55:54]     → Fold SMAPE: 2.215%\n",
            "[23:55:54]   [Fold 8/10] train=1461, val=185\n",
            "[23:56:00]     → Fold SMAPE: 1.533%\n",
            "[23:56:00]   [Fold 9/10] train=1646, val=185\n",
            "[23:56:05]     → Fold SMAPE: 1.027%\n",
            "[23:56:05]   [Fold 10/10] train=1831, val=185\n",
            "[23:56:11]     → Fold SMAPE: 1.250%\n",
            "[23:56:14]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:56:14]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:56:14]   [Fold 1/10] train=166, val=185\n",
            "[23:56:19]     → Fold SMAPE: 4.900%\n",
            "[23:56:19]   [Fold 2/10] train=351, val=185\n",
            "[23:56:24]     → Fold SMAPE: 1.545%\n",
            "[23:56:24]   [Fold 3/10] train=536, val=185\n",
            "[23:56:29]     → Fold SMAPE: 1.853%\n",
            "[23:56:29]   [Fold 4/10] train=721, val=185\n",
            "[23:56:33]     → Fold SMAPE: 0.889%\n",
            "[23:56:33]   [Fold 5/10] train=906, val=185\n",
            "[23:56:38]     → Fold SMAPE: 0.962%\n",
            "[23:56:38]   [Fold 6/10] train=1091, val=185\n",
            "[23:56:43]     → Fold SMAPE: 2.135%\n",
            "[23:56:43]   [Fold 7/10] train=1276, val=185\n",
            "[23:56:49]     → Fold SMAPE: 3.849%\n",
            "[23:56:49]   [Fold 8/10] train=1461, val=185\n",
            "[23:56:54]     → Fold SMAPE: 2.003%\n",
            "[23:56:54]   [Fold 9/10] train=1646, val=185\n",
            "[23:56:59]     → Fold SMAPE: 1.204%\n",
            "[23:56:59]   [Fold 10/10] train=1831, val=185\n",
            "[23:57:04]     → Fold SMAPE: 1.110%\n",
            "[23:57:07]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:57:08]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:57:08]   [Fold 1/10] train=166, val=185\n",
            "[23:57:12]     → Fold SMAPE: 6.522%\n",
            "[23:57:12]   [Fold 2/10] train=351, val=185\n",
            "[23:57:17]     → Fold SMAPE: 3.428%\n",
            "[23:57:17]   [Fold 3/10] train=536, val=185\n",
            "[23:57:22]     → Fold SMAPE: 1.087%\n",
            "[23:57:22]   [Fold 4/10] train=721, val=185\n",
            "[23:57:27]     → Fold SMAPE: 4.531%\n",
            "[23:57:27]   [Fold 5/10] train=906, val=185\n",
            "[23:57:32]     → Fold SMAPE: 1.768%\n",
            "[23:57:32]   [Fold 6/10] train=1091, val=185\n",
            "[23:57:37]     → Fold SMAPE: 3.801%\n",
            "[23:57:37]   [Fold 7/10] train=1276, val=185\n",
            "[23:57:42]     → Fold SMAPE: 1.479%\n",
            "[23:57:42]   [Fold 8/10] train=1461, val=185\n",
            "[23:57:48]     → Fold SMAPE: 0.918%\n",
            "[23:57:48]   [Fold 9/10] train=1646, val=185\n",
            "[23:57:53]     → Fold SMAPE: 0.690%\n",
            "[23:57:53]   [Fold 10/10] train=1831, val=185\n",
            "[23:57:58]     → Fold SMAPE: 1.138%\n",
            "[23:58:02]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:58:02]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:58:02]   [Fold 1/10] train=166, val=185\n",
            "[23:58:07]     → Fold SMAPE: 4.225%\n",
            "[23:58:07]   [Fold 2/10] train=351, val=185\n",
            "[23:58:12]     → Fold SMAPE: 2.601%\n",
            "[23:58:12]   [Fold 3/10] train=536, val=185\n",
            "[23:58:17]     → Fold SMAPE: 1.639%\n",
            "[23:58:17]   [Fold 4/10] train=721, val=185\n",
            "[23:58:21]     → Fold SMAPE: 1.047%\n",
            "[23:58:21]   [Fold 5/10] train=906, val=185\n",
            "[23:58:26]     → Fold SMAPE: 1.865%\n",
            "[23:58:26]   [Fold 6/10] train=1091, val=185\n",
            "[23:58:31]     → Fold SMAPE: 1.441%\n",
            "[23:58:31]   [Fold 7/10] train=1276, val=185\n",
            "[23:58:36]     → Fold SMAPE: 2.895%\n",
            "[23:58:36]   [Fold 8/10] train=1461, val=185\n",
            "[23:58:41]     → Fold SMAPE: 1.986%\n",
            "[23:58:41]   [Fold 9/10] train=1646, val=185\n",
            "[23:58:46]     → Fold SMAPE: 1.854%\n",
            "[23:58:46]   [Fold 10/10] train=1831, val=185\n",
            "[23:58:51]     → Fold SMAPE: 1.119%\n",
            "[23:58:55]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:58:55]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:58:55]   [Fold 1/10] train=166, val=185\n",
            "[23:59:00]     → Fold SMAPE: 4.773%\n",
            "[23:59:00]   [Fold 2/10] train=351, val=185\n",
            "[23:59:04]     → Fold SMAPE: 2.644%\n",
            "[23:59:04]   [Fold 3/10] train=536, val=185\n",
            "[23:59:09]     → Fold SMAPE: 19.651%\n",
            "[23:59:09]   [Fold 4/10] train=721, val=185\n",
            "[23:59:15]     → Fold SMAPE: 1.786%\n",
            "[23:59:15]   [Fold 5/10] train=906, val=185\n",
            "[23:59:21]     → Fold SMAPE: 3.008%\n",
            "[23:59:21]   [Fold 6/10] train=1091, val=185\n",
            "[23:59:27]     → Fold SMAPE: 2.483%\n",
            "[23:59:27]   [Fold 7/10] train=1276, val=185\n",
            "[23:59:33]     → Fold SMAPE: 2.457%\n",
            "[23:59:33]   [Fold 8/10] train=1461, val=185\n",
            "[23:59:39]     → Fold SMAPE: 1.948%\n",
            "[23:59:39]   [Fold 9/10] train=1646, val=185\n",
            "[23:59:45]     → Fold SMAPE: 1.706%\n",
            "[23:59:45]   [Fold 10/10] train=1831, val=185\n",
            "[23:59:51]     → Fold SMAPE: 1.102%\n",
            "[23:59:55]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[23:59:55]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[23:59:55]   [Fold 1/10] train=166, val=185\n",
            "[00:00:00]     → Fold SMAPE: 4.402%\n",
            "[00:00:00]   [Fold 2/10] train=351, val=185\n",
            "[00:00:04]     → Fold SMAPE: 3.264%\n",
            "[00:00:04]   [Fold 3/10] train=536, val=185\n",
            "[00:00:09]     → Fold SMAPE: 1.485%\n",
            "[00:00:09]   [Fold 4/10] train=721, val=185\n",
            "[00:00:14]     → Fold SMAPE: 1.167%\n",
            "[00:00:14]   [Fold 5/10] train=906, val=185\n",
            "[00:00:19]     → Fold SMAPE: 1.009%\n",
            "[00:00:19]   [Fold 6/10] train=1091, val=185\n",
            "[00:00:25]     → Fold SMAPE: 1.429%\n",
            "[00:00:25]   [Fold 7/10] train=1276, val=185\n",
            "[00:00:30]     → Fold SMAPE: 1.876%\n",
            "[00:00:30]   [Fold 8/10] train=1461, val=185\n",
            "[00:00:35]     → Fold SMAPE: 1.300%\n",
            "[00:00:35]   [Fold 9/10] train=1646, val=185\n",
            "[00:00:41]     → Fold SMAPE: 0.976%\n",
            "[00:00:41]   [Fold 10/10] train=1831, val=185\n",
            "[00:00:46]     → Fold SMAPE: 1.230%\n",
            "[00:00:50]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:00:50]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:00:50]   [Fold 1/10] train=166, val=185\n",
            "[00:00:55]     → Fold SMAPE: 10.140%\n",
            "[00:00:55]   [Fold 2/10] train=351, val=185\n",
            "[00:01:00]     → Fold SMAPE: 4.125%\n",
            "[00:01:00]   [Fold 3/10] train=536, val=185\n",
            "[00:01:05]     → Fold SMAPE: 1.948%\n",
            "[00:01:05]   [Fold 4/10] train=721, val=185\n",
            "[00:01:10]     → Fold SMAPE: 2.745%\n",
            "[00:01:10]   [Fold 5/10] train=906, val=185\n",
            "[00:01:15]     → Fold SMAPE: 1.639%\n",
            "[00:01:15]   [Fold 6/10] train=1091, val=185\n",
            "[00:01:20]     → Fold SMAPE: 1.703%\n",
            "[00:01:20]   [Fold 7/10] train=1276, val=185\n",
            "[00:01:26]     → Fold SMAPE: 1.457%\n",
            "[00:01:26]   [Fold 8/10] train=1461, val=185\n",
            "[00:01:31]     → Fold SMAPE: 0.999%\n",
            "[00:01:31]   [Fold 9/10] train=1646, val=185\n",
            "[00:01:37]     → Fold SMAPE: 0.911%\n",
            "[00:01:37]   [Fold 10/10] train=1831, val=185\n",
            "[00:01:42]     → Fold SMAPE: 1.378%\n",
            "[00:01:46]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:01:46]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:01:46]   [Fold 1/10] train=166, val=185\n",
            "[00:01:51]     → Fold SMAPE: 3.101%\n",
            "[00:01:51]   [Fold 2/10] train=351, val=185\n",
            "[00:01:56]     → Fold SMAPE: 1.786%\n",
            "[00:01:56]   [Fold 3/10] train=536, val=185\n",
            "[00:02:01]     → Fold SMAPE: 1.127%\n",
            "[00:02:01]   [Fold 4/10] train=721, val=185\n",
            "[00:02:05]     → Fold SMAPE: 1.372%\n",
            "[00:02:05]   [Fold 5/10] train=906, val=185\n",
            "[00:02:10]     → Fold SMAPE: 0.717%\n",
            "[00:02:10]   [Fold 6/10] train=1091, val=185\n",
            "[00:02:15]     → Fold SMAPE: 0.706%\n",
            "[00:02:15]   [Fold 7/10] train=1276, val=185\n",
            "[00:02:20]     → Fold SMAPE: 1.112%\n",
            "[00:02:20]   [Fold 8/10] train=1461, val=185\n",
            "[00:02:25]     → Fold SMAPE: 1.123%\n",
            "[00:02:25]   [Fold 9/10] train=1646, val=185\n",
            "[00:02:30]     → Fold SMAPE: 0.328%\n",
            "[00:02:30]   [Fold 10/10] train=1831, val=185\n",
            "[00:02:35]     → Fold SMAPE: 0.696%\n",
            "[00:02:37]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:02:37]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:02:37]   [Fold 1/10] train=166, val=185\n",
            "[00:02:42]     → Fold SMAPE: 4.037%\n",
            "[00:02:42]   [Fold 2/10] train=351, val=185\n",
            "[00:02:47]     → Fold SMAPE: 1.300%\n",
            "[00:02:47]   [Fold 3/10] train=536, val=185\n",
            "[00:02:52]     → Fold SMAPE: 1.254%\n",
            "[00:02:52]   [Fold 4/10] train=721, val=185\n",
            "[00:02:57]     → Fold SMAPE: 2.991%\n",
            "[00:02:57]   [Fold 5/10] train=906, val=185\n",
            "[00:03:02]     → Fold SMAPE: 1.010%\n",
            "[00:03:02]   [Fold 6/10] train=1091, val=185\n",
            "[00:03:07]     → Fold SMAPE: 2.169%\n",
            "[00:03:07]   [Fold 7/10] train=1276, val=185\n",
            "[00:03:12]     → Fold SMAPE: 2.820%\n",
            "[00:03:12]   [Fold 8/10] train=1461, val=185\n",
            "[00:03:17]     → Fold SMAPE: 1.693%\n",
            "[00:03:17]   [Fold 9/10] train=1646, val=185\n",
            "[00:03:22]     → Fold SMAPE: 0.498%\n",
            "[00:03:22]   [Fold 10/10] train=1831, val=185\n",
            "[00:03:27]     → Fold SMAPE: 0.743%\n",
            "[00:03:30]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:03:30]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:03:30]   [Fold 1/10] train=166, val=185\n",
            "[00:03:35]     → Fold SMAPE: 7.030%\n",
            "[00:03:35]   [Fold 2/10] train=351, val=185\n",
            "[00:03:40]     → Fold SMAPE: 2.132%\n",
            "[00:03:40]   [Fold 3/10] train=536, val=185\n",
            "[00:03:44]     → Fold SMAPE: 0.835%\n",
            "[00:03:44]   [Fold 4/10] train=721, val=185\n",
            "[00:03:49]     → Fold SMAPE: 0.682%\n",
            "[00:03:49]   [Fold 5/10] train=906, val=185\n",
            "[00:03:55]     → Fold SMAPE: 0.737%\n",
            "[00:03:55]   [Fold 6/10] train=1091, val=185\n",
            "[00:04:00]     → Fold SMAPE: 1.202%\n",
            "[00:04:00]   [Fold 7/10] train=1276, val=185\n",
            "[00:04:05]     → Fold SMAPE: 2.484%\n",
            "[00:04:05]   [Fold 8/10] train=1461, val=185\n",
            "[00:04:10]     → Fold SMAPE: 1.175%\n",
            "[00:04:10]   [Fold 9/10] train=1646, val=185\n",
            "[00:04:16]     → Fold SMAPE: 0.571%\n",
            "[00:04:16]   [Fold 10/10] train=1831, val=185\n",
            "[00:04:21]     → Fold SMAPE: 0.508%\n",
            "[00:04:25]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:04:25]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:04:25]   [Fold 1/10] train=166, val=185\n",
            "[00:04:29]     → Fold SMAPE: 5.494%\n",
            "[00:04:29]   [Fold 2/10] train=351, val=185\n",
            "[00:04:34]     → Fold SMAPE: 2.470%\n",
            "[00:04:34]   [Fold 3/10] train=536, val=185\n",
            "[00:04:39]     → Fold SMAPE: 1.610%\n",
            "[00:04:39]   [Fold 4/10] train=721, val=185\n",
            "[00:04:45]     → Fold SMAPE: 2.350%\n",
            "[00:04:45]   [Fold 5/10] train=906, val=185\n",
            "[00:04:50]     → Fold SMAPE: 2.445%\n",
            "[00:04:50]   [Fold 6/10] train=1091, val=185\n",
            "[00:04:55]     → Fold SMAPE: 1.397%\n",
            "[00:04:55]   [Fold 7/10] train=1276, val=185\n",
            "[00:05:01]     → Fold SMAPE: 1.722%\n",
            "[00:05:01]   [Fold 8/10] train=1461, val=185\n",
            "[00:05:06]     → Fold SMAPE: 1.113%\n",
            "[00:05:06]   [Fold 9/10] train=1646, val=185\n",
            "[00:05:12]     → Fold SMAPE: 1.253%\n",
            "[00:05:12]   [Fold 10/10] train=1831, val=185\n",
            "[00:05:18]     → Fold SMAPE: 1.001%\n",
            "[00:05:22]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:05:22]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:05:22]   [Fold 1/10] train=166, val=185\n",
            "[00:05:27]     → Fold SMAPE: 6.869%\n",
            "[00:05:27]   [Fold 2/10] train=351, val=185\n",
            "[00:05:32]     → Fold SMAPE: 2.564%\n",
            "[00:05:32]   [Fold 3/10] train=536, val=185\n",
            "[00:05:37]     → Fold SMAPE: 1.140%\n",
            "[00:05:37]   [Fold 4/10] train=721, val=185\n",
            "[00:05:42]     → Fold SMAPE: 1.120%\n",
            "[00:05:42]   [Fold 5/10] train=906, val=185\n",
            "[00:05:48]     → Fold SMAPE: 0.859%\n",
            "[00:05:48]   [Fold 6/10] train=1091, val=185\n",
            "[00:05:53]     → Fold SMAPE: 1.284%\n",
            "[00:05:53]   [Fold 7/10] train=1276, val=185\n",
            "[00:05:58]     → Fold SMAPE: 1.167%\n",
            "[00:05:58]   [Fold 8/10] train=1461, val=185\n",
            "[00:06:04]     → Fold SMAPE: 1.397%\n",
            "[00:06:04]   [Fold 9/10] train=1646, val=185\n",
            "[00:06:09]     → Fold SMAPE: 0.901%\n",
            "[00:06:09]   [Fold 10/10] train=1831, val=185\n",
            "[00:06:15]     → Fold SMAPE: 0.997%\n",
            "[00:06:19]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:06:19]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:06:19]   [Fold 1/10] train=166, val=185\n",
            "[00:06:24]     → Fold SMAPE: 4.141%\n",
            "[00:06:24]   [Fold 2/10] train=351, val=185\n",
            "[00:06:29]     → Fold SMAPE: 5.159%\n",
            "[00:06:29]   [Fold 3/10] train=536, val=185\n",
            "[00:06:34]     → Fold SMAPE: 2.379%\n",
            "[00:06:34]   [Fold 4/10] train=721, val=185\n",
            "[00:06:39]     → Fold SMAPE: 10.591%\n",
            "[00:06:39]   [Fold 5/10] train=906, val=185\n",
            "[00:06:45]     → Fold SMAPE: 2.310%\n",
            "[00:06:45]   [Fold 6/10] train=1091, val=185\n",
            "[00:06:51]     → Fold SMAPE: 5.381%\n",
            "[00:06:51]   [Fold 7/10] train=1276, val=185\n",
            "[00:06:57]     → Fold SMAPE: 9.624%\n",
            "[00:06:57]   [Fold 8/10] train=1461, val=185\n",
            "[00:07:03]     → Fold SMAPE: 7.269%\n",
            "[00:07:03]   [Fold 9/10] train=1646, val=185\n",
            "[00:07:10]     → Fold SMAPE: 0.958%\n",
            "[00:07:10]   [Fold 10/10] train=1831, val=185\n",
            "[00:07:16]     → Fold SMAPE: 1.799%\n",
            "[00:07:20]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:07:21]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:07:21]   [Fold 1/10] train=166, val=185\n",
            "[00:07:26]     → Fold SMAPE: 5.056%\n",
            "[00:07:26]   [Fold 2/10] train=351, val=185\n",
            "[00:07:31]     → Fold SMAPE: 10.797%\n",
            "[00:07:31]   [Fold 3/10] train=536, val=185\n",
            "[00:07:36]     → Fold SMAPE: 7.938%\n",
            "[00:07:36]   [Fold 4/10] train=721, val=185\n",
            "[00:07:41]     → Fold SMAPE: 1.865%\n",
            "[00:07:41]   [Fold 5/10] train=906, val=185\n",
            "[00:07:46]     → Fold SMAPE: 1.517%\n",
            "[00:07:46]   [Fold 6/10] train=1091, val=185\n",
            "[00:07:52]     → Fold SMAPE: 1.059%\n",
            "[00:07:52]   [Fold 7/10] train=1276, val=185\n",
            "[00:07:57]     → Fold SMAPE: 1.170%\n",
            "[00:07:57]   [Fold 8/10] train=1461, val=185\n",
            "[00:08:03]     → Fold SMAPE: 1.124%\n",
            "[00:08:03]   [Fold 9/10] train=1646, val=185\n",
            "[00:08:09]     → Fold SMAPE: 1.815%\n",
            "[00:08:09]   [Fold 10/10] train=1831, val=185\n",
            "[00:08:15]     → Fold SMAPE: 1.380%\n",
            "[00:08:18]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:08:18]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:08:18]   [Fold 1/10] train=166, val=185\n",
            "[00:08:23]     → Fold SMAPE: 8.915%\n",
            "[00:08:23]   [Fold 2/10] train=351, val=185\n",
            "[00:08:28]     → Fold SMAPE: 3.420%\n",
            "[00:08:28]   [Fold 3/10] train=536, val=185\n",
            "[00:08:33]     → Fold SMAPE: 2.061%\n",
            "[00:08:33]   [Fold 4/10] train=721, val=185\n",
            "[00:08:38]     → Fold SMAPE: 2.171%\n",
            "[00:08:38]   [Fold 5/10] train=906, val=185\n",
            "[00:08:43]     → Fold SMAPE: 2.415%\n",
            "[00:08:43]   [Fold 6/10] train=1091, val=185\n",
            "[00:08:48]     → Fold SMAPE: 2.594%\n",
            "[00:08:48]   [Fold 7/10] train=1276, val=185\n",
            "[00:08:53]     → Fold SMAPE: 4.401%\n",
            "[00:08:53]   [Fold 8/10] train=1461, val=185\n",
            "[00:08:59]     → Fold SMAPE: 2.647%\n",
            "[00:08:59]   [Fold 9/10] train=1646, val=185\n",
            "[00:09:04]     → Fold SMAPE: 2.110%\n",
            "[00:09:04]   [Fold 10/10] train=1831, val=185\n",
            "[00:09:09]     → Fold SMAPE: 1.067%\n",
            "[00:09:13]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:09:13]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:09:13]   [Fold 1/10] train=166, val=185\n",
            "[00:09:18]     → Fold SMAPE: 9.446%\n",
            "[00:09:18]   [Fold 2/10] train=351, val=185\n",
            "[00:09:23]     → Fold SMAPE: 4.625%\n",
            "[00:09:23]   [Fold 3/10] train=536, val=185\n",
            "[00:09:28]     → Fold SMAPE: 2.363%\n",
            "[00:09:28]   [Fold 4/10] train=721, val=185\n",
            "[00:09:32]     → Fold SMAPE: 2.323%\n",
            "[00:09:32]   [Fold 5/10] train=906, val=185\n",
            "[00:09:38]     → Fold SMAPE: 3.154%\n",
            "[00:09:38]   [Fold 6/10] train=1091, val=185\n",
            "[00:09:43]     → Fold SMAPE: 4.773%\n",
            "[00:09:43]   [Fold 7/10] train=1276, val=185\n",
            "[00:09:48]     → Fold SMAPE: 6.827%\n",
            "[00:09:48]   [Fold 8/10] train=1461, val=185\n",
            "[00:09:53]     → Fold SMAPE: 1.926%\n",
            "[00:09:53]   [Fold 9/10] train=1646, val=185\n",
            "[00:09:58]     → Fold SMAPE: 0.536%\n",
            "[00:09:58]   [Fold 10/10] train=1831, val=185\n",
            "[00:10:04]     → Fold SMAPE: 0.530%\n",
            "[00:10:07]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:10:08]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:10:08]   [Fold 1/10] train=166, val=185\n",
            "[00:10:12]     → Fold SMAPE: 10.690%\n",
            "[00:10:12]   [Fold 2/10] train=351, val=185\n",
            "[00:10:17]     → Fold SMAPE: 1.817%\n",
            "[00:10:17]   [Fold 3/10] train=536, val=185\n",
            "[00:10:23]     → Fold SMAPE: 12.069%\n",
            "[00:10:23]   [Fold 4/10] train=721, val=185\n",
            "[00:10:29]     → Fold SMAPE: 5.201%\n",
            "[00:10:29]   [Fold 5/10] train=906, val=185\n",
            "[00:10:35]     → Fold SMAPE: 4.466%\n",
            "[00:10:35]   [Fold 6/10] train=1091, val=185\n",
            "[00:10:41]     → Fold SMAPE: 3.108%\n",
            "[00:10:41]   [Fold 7/10] train=1276, val=185\n",
            "[00:10:47]     → Fold SMAPE: 2.788%\n",
            "[00:10:47]   [Fold 8/10] train=1461, val=185\n",
            "[00:10:53]     → Fold SMAPE: 2.016%\n",
            "[00:10:53]   [Fold 9/10] train=1646, val=185\n",
            "[00:11:00]     → Fold SMAPE: 2.031%\n",
            "[00:11:00]   [Fold 10/10] train=1831, val=185\n",
            "[00:11:06]     → Fold SMAPE: 0.806%\n",
            "[00:11:10]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:11:10]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:11:10]   [Fold 1/10] train=166, val=185\n",
            "[00:11:15]     → Fold SMAPE: 0.700%\n",
            "[00:11:15]   [Fold 2/10] train=351, val=185\n",
            "[00:11:20]     → Fold SMAPE: 0.775%\n",
            "[00:11:20]   [Fold 3/10] train=536, val=185\n",
            "[00:11:24]     → Fold SMAPE: 0.535%\n",
            "[00:11:24]   [Fold 4/10] train=721, val=185\n",
            "[00:11:29]     → Fold SMAPE: 0.930%\n",
            "[00:11:29]   [Fold 5/10] train=906, val=185\n",
            "[00:11:34]     → Fold SMAPE: 1.345%\n",
            "[00:11:34]   [Fold 6/10] train=1091, val=185\n",
            "[00:11:39]     → Fold SMAPE: 0.787%\n",
            "[00:11:39]   [Fold 7/10] train=1276, val=185\n",
            "[00:11:43]     → Fold SMAPE: 1.218%\n",
            "[00:11:43]   [Fold 8/10] train=1461, val=185\n",
            "[00:11:48]     → Fold SMAPE: 2.843%\n",
            "[00:11:48]   [Fold 9/10] train=1646, val=185\n",
            "[00:11:53]     → Fold SMAPE: 0.308%\n",
            "[00:11:53]   [Fold 10/10] train=1831, val=185\n",
            "[00:11:58]     → Fold SMAPE: 0.056%\n",
            "[00:12:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:12:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:12:01]   [Fold 1/10] train=166, val=185\n",
            "[00:12:06]     → Fold SMAPE: 6.360%\n",
            "[00:12:06]   [Fold 2/10] train=351, val=185\n",
            "[00:12:11]     → Fold SMAPE: 5.810%\n",
            "[00:12:11]   [Fold 3/10] train=536, val=185\n",
            "[00:12:16]     → Fold SMAPE: 1.481%\n",
            "[00:12:16]   [Fold 4/10] train=721, val=185\n",
            "[00:12:20]     → Fold SMAPE: 1.565%\n",
            "[00:12:20]   [Fold 5/10] train=906, val=185\n",
            "[00:12:25]     → Fold SMAPE: 2.614%\n",
            "[00:12:25]   [Fold 6/10] train=1091, val=185\n",
            "[00:12:30]     → Fold SMAPE: 2.544%\n",
            "[00:12:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:12:35]     → Fold SMAPE: 6.530%\n",
            "[00:12:35]   [Fold 8/10] train=1461, val=185\n",
            "[00:12:41]     → Fold SMAPE: 5.290%\n",
            "[00:12:41]   [Fold 9/10] train=1646, val=185\n",
            "[00:12:46]     → Fold SMAPE: 1.538%\n",
            "[00:12:46]   [Fold 10/10] train=1831, val=185\n",
            "[00:12:51]     → Fold SMAPE: 1.309%\n",
            "[00:12:55]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:12:55]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:12:55]   [Fold 1/10] train=166, val=185\n",
            "[00:12:59]     → Fold SMAPE: 6.633%\n",
            "[00:12:59]   [Fold 2/10] train=351, val=185\n",
            "[00:13:04]     → Fold SMAPE: 4.990%\n",
            "[00:13:04]   [Fold 3/10] train=536, val=185\n",
            "[00:13:09]     → Fold SMAPE: 3.140%\n",
            "[00:13:09]   [Fold 4/10] train=721, val=185\n",
            "[00:13:14]     → Fold SMAPE: 4.209%\n",
            "[00:13:14]   [Fold 5/10] train=906, val=185\n",
            "[00:13:19]     → Fold SMAPE: 1.247%\n",
            "[00:13:19]   [Fold 6/10] train=1091, val=185\n",
            "[00:13:25]     → Fold SMAPE: 1.807%\n",
            "[00:13:25]   [Fold 7/10] train=1276, val=185\n",
            "[00:13:30]     → Fold SMAPE: 2.829%\n",
            "[00:13:30]   [Fold 8/10] train=1461, val=185\n",
            "[00:13:35]     → Fold SMAPE: 2.152%\n",
            "[00:13:35]   [Fold 9/10] train=1646, val=185\n",
            "[00:13:41]     → Fold SMAPE: 1.295%\n",
            "[00:13:41]   [Fold 10/10] train=1831, val=185\n",
            "[00:13:46]     → Fold SMAPE: 0.907%\n",
            "[00:13:50]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:13:50]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:13:50]   [Fold 1/10] train=166, val=185\n",
            "[00:13:55]     → Fold SMAPE: 10.522%\n",
            "[00:13:55]   [Fold 2/10] train=351, val=185\n",
            "[00:14:00]     → Fold SMAPE: 4.366%\n",
            "[00:14:00]   [Fold 3/10] train=536, val=185\n",
            "[00:14:05]     → Fold SMAPE: 2.162%\n",
            "[00:14:05]   [Fold 4/10] train=721, val=185\n",
            "[00:14:10]     → Fold SMAPE: 4.962%\n",
            "[00:14:10]   [Fold 5/10] train=906, val=185\n",
            "[00:14:15]     → Fold SMAPE: 2.334%\n",
            "[00:14:15]   [Fold 6/10] train=1091, val=185\n",
            "[00:14:21]     → Fold SMAPE: 5.274%\n",
            "[00:14:21]   [Fold 7/10] train=1276, val=185\n",
            "[00:14:26]     → Fold SMAPE: 8.345%\n",
            "[00:14:26]   [Fold 8/10] train=1461, val=185\n",
            "[00:14:32]     → Fold SMAPE: 3.384%\n",
            "[00:14:32]   [Fold 9/10] train=1646, val=185\n",
            "[00:14:38]     → Fold SMAPE: 2.306%\n",
            "[00:14:38]   [Fold 10/10] train=1831, val=185\n",
            "[00:14:44]     → Fold SMAPE: 1.764%\n",
            "[00:14:48]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:14:48]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:14:48]   [Fold 1/10] train=166, val=185\n",
            "[00:14:52]     → Fold SMAPE: 6.655%\n",
            "[00:14:52]   [Fold 2/10] train=351, val=185\n",
            "[00:14:57]     → Fold SMAPE: 3.700%\n",
            "[00:14:57]   [Fold 3/10] train=536, val=185\n",
            "[00:15:02]     → Fold SMAPE: 1.893%\n",
            "[00:15:02]   [Fold 4/10] train=721, val=185\n",
            "[00:15:07]     → Fold SMAPE: 2.456%\n",
            "[00:15:07]   [Fold 5/10] train=906, val=185\n",
            "[00:15:12]     → Fold SMAPE: 2.209%\n",
            "[00:15:12]   [Fold 6/10] train=1091, val=185\n",
            "[00:15:17]     → Fold SMAPE: 2.989%\n",
            "[00:15:17]   [Fold 7/10] train=1276, val=185\n",
            "[00:15:22]     → Fold SMAPE: 3.902%\n",
            "[00:15:22]   [Fold 8/10] train=1461, val=185\n",
            "[00:15:28]     → Fold SMAPE: 2.636%\n",
            "[00:15:28]   [Fold 9/10] train=1646, val=185\n",
            "[00:15:33]     → Fold SMAPE: 1.546%\n",
            "[00:15:33]   [Fold 10/10] train=1831, val=185\n",
            "[00:15:38]     → Fold SMAPE: 0.770%\n",
            "[00:15:42]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:15:42]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:15:42]   [Fold 1/10] train=166, val=185\n",
            "[00:15:46]     → Fold SMAPE: 1.407%\n",
            "[00:15:46]   [Fold 2/10] train=351, val=185\n",
            "[00:15:51]     → Fold SMAPE: 1.366%\n",
            "[00:15:51]   [Fold 3/10] train=536, val=185\n",
            "[00:15:56]     → Fold SMAPE: 1.329%\n",
            "[00:15:56]   [Fold 4/10] train=721, val=185\n",
            "[00:16:00]     → Fold SMAPE: 1.267%\n",
            "[00:16:00]   [Fold 5/10] train=906, val=185\n",
            "[00:16:05]     → Fold SMAPE: 1.384%\n",
            "[00:16:05]   [Fold 6/10] train=1091, val=185\n",
            "[00:16:10]     → Fold SMAPE: 1.479%\n",
            "[00:16:10]   [Fold 7/10] train=1276, val=185\n",
            "[00:16:15]     → Fold SMAPE: 1.241%\n",
            "[00:16:15]   [Fold 8/10] train=1461, val=185\n",
            "[00:16:20]     → Fold SMAPE: 0.877%\n",
            "[00:16:20]   [Fold 9/10] train=1646, val=185\n",
            "[00:16:25]     → Fold SMAPE: 0.661%\n",
            "[00:16:25]   [Fold 10/10] train=1831, val=185\n",
            "[00:16:30]     → Fold SMAPE: 0.545%\n",
            "[00:16:33]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:16:33]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:16:33]   [Fold 1/10] train=166, val=185\n",
            "[00:16:37]     → Fold SMAPE: 3.149%\n",
            "[00:16:37]   [Fold 2/10] train=351, val=185\n",
            "[00:16:42]     → Fold SMAPE: 1.030%\n",
            "[00:16:42]   [Fold 3/10] train=536, val=185\n",
            "[00:16:47]     → Fold SMAPE: 0.680%\n",
            "[00:16:47]   [Fold 4/10] train=721, val=185\n",
            "[00:16:52]     → Fold SMAPE: 0.594%\n",
            "[00:16:52]   [Fold 5/10] train=906, val=185\n",
            "[00:16:56]     → Fold SMAPE: 1.080%\n",
            "[00:16:56]   [Fold 6/10] train=1091, val=185\n",
            "[00:17:01]     → Fold SMAPE: 1.816%\n",
            "[00:17:01]   [Fold 7/10] train=1276, val=185\n",
            "[00:17:06]     → Fold SMAPE: 2.134%\n",
            "[00:17:06]   [Fold 8/10] train=1461, val=185\n",
            "[00:17:11]     → Fold SMAPE: 0.981%\n",
            "[00:17:11]   [Fold 9/10] train=1646, val=185\n",
            "[00:17:16]     → Fold SMAPE: 0.747%\n",
            "[00:17:16]   [Fold 10/10] train=1831, val=185\n",
            "[00:17:21]     → Fold SMAPE: 0.439%\n",
            "[00:17:23]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:17:23]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:17:23]   [Fold 1/10] train=166, val=185\n",
            "[00:17:28]     → Fold SMAPE: 6.551%\n",
            "[00:17:28]   [Fold 2/10] train=351, val=185\n",
            "[00:17:33]     → Fold SMAPE: 2.404%\n",
            "[00:17:33]   [Fold 3/10] train=536, val=185\n",
            "[00:17:38]     → Fold SMAPE: 1.638%\n",
            "[00:17:38]   [Fold 4/10] train=721, val=185\n",
            "[00:17:43]     → Fold SMAPE: 1.078%\n",
            "[00:17:43]   [Fold 5/10] train=906, val=185\n",
            "[00:17:48]     → Fold SMAPE: 2.160%\n",
            "[00:17:48]   [Fold 6/10] train=1091, val=185\n",
            "[00:17:53]     → Fold SMAPE: 2.295%\n",
            "[00:17:53]   [Fold 7/10] train=1276, val=185\n",
            "[00:17:58]     → Fold SMAPE: 3.047%\n",
            "[00:17:58]   [Fold 8/10] train=1461, val=185\n",
            "[00:18:03]     → Fold SMAPE: 1.946%\n",
            "[00:18:03]   [Fold 9/10] train=1646, val=185\n",
            "[00:18:09]     → Fold SMAPE: 1.575%\n",
            "[00:18:09]   [Fold 10/10] train=1831, val=185\n",
            "[00:18:14]     → Fold SMAPE: 1.439%\n",
            "[00:18:17]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:18:18]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:18:18]   [Fold 1/10] train=166, val=185\n",
            "[00:18:22]     → Fold SMAPE: 4.423%\n",
            "[00:18:22]   [Fold 2/10] train=351, val=185\n",
            "[00:18:27]     → Fold SMAPE: 1.891%\n",
            "[00:18:27]   [Fold 3/10] train=536, val=185\n",
            "[00:18:32]     → Fold SMAPE: 1.103%\n",
            "[00:18:32]   [Fold 4/10] train=721, val=185\n",
            "[00:18:37]     → Fold SMAPE: 1.148%\n",
            "[00:18:37]   [Fold 5/10] train=906, val=185\n",
            "[00:18:42]     → Fold SMAPE: 0.963%\n",
            "[00:18:42]   [Fold 6/10] train=1091, val=185\n",
            "[00:18:47]     → Fold SMAPE: 1.145%\n",
            "[00:18:47]   [Fold 7/10] train=1276, val=185\n",
            "[00:18:52]     → Fold SMAPE: 1.582%\n",
            "[00:18:52]   [Fold 8/10] train=1461, val=185\n",
            "[00:18:57]     → Fold SMAPE: 1.062%\n",
            "[00:18:57]   [Fold 9/10] train=1646, val=185\n",
            "[00:19:02]     → Fold SMAPE: 1.092%\n",
            "[00:19:02]   [Fold 10/10] train=1831, val=185\n",
            "[00:19:07]     → Fold SMAPE: 1.357%\n",
            "[00:19:10]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:19:10]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:19:10]   [Fold 1/10] train=166, val=185\n",
            "[00:19:15]     → Fold SMAPE: 6.011%\n",
            "[00:19:15]   [Fold 2/10] train=351, val=185\n",
            "[00:19:20]     → Fold SMAPE: 3.928%\n",
            "[00:19:20]   [Fold 3/10] train=536, val=185\n",
            "[00:19:24]     → Fold SMAPE: 1.513%\n",
            "[00:19:24]   [Fold 4/10] train=721, val=185\n",
            "[00:19:29]     → Fold SMAPE: 3.235%\n",
            "[00:19:29]   [Fold 5/10] train=906, val=185\n",
            "[00:19:34]     → Fold SMAPE: 2.213%\n",
            "[00:19:34]   [Fold 6/10] train=1091, val=185\n",
            "[00:19:39]     → Fold SMAPE: 2.895%\n",
            "[00:19:39]   [Fold 7/10] train=1276, val=185\n",
            "[00:19:44]     → Fold SMAPE: 2.863%\n",
            "[00:19:44]   [Fold 8/10] train=1461, val=185\n",
            "[00:19:50]     → Fold SMAPE: 2.471%\n",
            "[00:19:50]   [Fold 9/10] train=1646, val=185\n",
            "[00:19:55]     → Fold SMAPE: 1.466%\n",
            "[00:19:55]   [Fold 10/10] train=1831, val=185\n",
            "[00:20:00]     → Fold SMAPE: 1.307%\n",
            "[00:20:04]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:20:04]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:20:04]   [Fold 1/10] train=166, val=185\n",
            "[00:20:09]     → Fold SMAPE: 6.196%\n",
            "[00:20:09]   [Fold 2/10] train=351, val=185\n",
            "[00:20:14]     → Fold SMAPE: 2.647%\n",
            "[00:20:14]   [Fold 3/10] train=536, val=185\n",
            "[00:20:18]     → Fold SMAPE: 1.798%\n",
            "[00:20:18]   [Fold 4/10] train=721, val=185\n",
            "[00:20:23]     → Fold SMAPE: 1.576%\n",
            "[00:20:23]   [Fold 5/10] train=906, val=185\n",
            "[00:20:28]     → Fold SMAPE: 2.442%\n",
            "[00:20:28]   [Fold 6/10] train=1091, val=185\n",
            "[00:20:34]     → Fold SMAPE: 2.807%\n",
            "[00:20:34]   [Fold 7/10] train=1276, val=185\n",
            "[00:20:40]     → Fold SMAPE: 2.535%\n",
            "[00:20:40]   [Fold 8/10] train=1461, val=185\n",
            "[00:20:46]     → Fold SMAPE: 2.191%\n",
            "[00:20:46]   [Fold 9/10] train=1646, val=185\n",
            "[00:20:52]     → Fold SMAPE: 1.470%\n",
            "[00:20:52]   [Fold 10/10] train=1831, val=185\n",
            "[00:20:58]     → Fold SMAPE: 0.776%\n",
            "[00:21:02]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:21:02]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:21:02]   [Fold 1/10] train=166, val=185\n",
            "[00:21:07]     → Fold SMAPE: 1.181%\n",
            "[00:21:07]   [Fold 2/10] train=351, val=185\n",
            "[00:21:11]     → Fold SMAPE: 4.722%\n",
            "[00:21:11]   [Fold 3/10] train=536, val=185\n",
            "[00:21:17]     → Fold SMAPE: 54.472%\n",
            "[00:21:17]   [Fold 4/10] train=721, val=185\n",
            "[00:21:23]     → Fold SMAPE: 6.176%\n",
            "[00:21:23]   [Fold 5/10] train=906, val=185\n",
            "[00:21:29]     → Fold SMAPE: 2.038%\n",
            "[00:21:29]   [Fold 6/10] train=1091, val=185\n",
            "[00:21:35]     → Fold SMAPE: 10.687%\n",
            "[00:21:35]   [Fold 7/10] train=1276, val=185\n",
            "[00:21:41]     → Fold SMAPE: 0.994%\n",
            "[00:21:41]   [Fold 8/10] train=1461, val=185\n",
            "[00:21:48]     → Fold SMAPE: 0.953%\n",
            "[00:21:48]   [Fold 9/10] train=1646, val=185\n",
            "[00:21:54]     → Fold SMAPE: 1.954%\n",
            "[00:21:54]   [Fold 10/10] train=1831, val=185\n",
            "[00:22:00]     → Fold SMAPE: 1.854%\n",
            "[00:22:04]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:22:04]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:22:04]   [Fold 1/10] train=166, val=185\n",
            "[00:22:09]     → Fold SMAPE: 3.842%\n",
            "[00:22:09]   [Fold 2/10] train=351, val=185\n",
            "[00:22:14]     → Fold SMAPE: 1.633%\n",
            "[00:22:14]   [Fold 3/10] train=536, val=185\n",
            "[00:22:19]     → Fold SMAPE: 3.331%\n",
            "[00:22:19]   [Fold 4/10] train=721, val=185\n",
            "[00:22:24]     → Fold SMAPE: 2.979%\n",
            "[00:22:24]   [Fold 5/10] train=906, val=185\n",
            "[00:22:29]     → Fold SMAPE: 1.195%\n",
            "[00:22:29]   [Fold 6/10] train=1091, val=185\n",
            "[00:22:34]     → Fold SMAPE: 2.646%\n",
            "[00:22:34]   [Fold 7/10] train=1276, val=185\n",
            "[00:22:39]     → Fold SMAPE: 1.961%\n",
            "[00:22:39]   [Fold 8/10] train=1461, val=185\n",
            "[00:22:44]     → Fold SMAPE: 1.743%\n",
            "[00:22:44]   [Fold 9/10] train=1646, val=185\n",
            "[00:22:49]     → Fold SMAPE: 0.829%\n",
            "[00:22:49]   [Fold 10/10] train=1831, val=185\n",
            "[00:22:54]     → Fold SMAPE: 1.164%\n",
            "[00:22:57]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:22:57]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:22:57]   [Fold 1/10] train=166, val=185\n",
            "[00:23:02]     → Fold SMAPE: 3.603%\n",
            "[00:23:02]   [Fold 2/10] train=351, val=185\n",
            "[00:23:07]     → Fold SMAPE: 2.636%\n",
            "[00:23:07]   [Fold 3/10] train=536, val=185\n",
            "[00:23:12]     → Fold SMAPE: 1.109%\n",
            "[00:23:12]   [Fold 4/10] train=721, val=185\n",
            "[00:23:18]     → Fold SMAPE: 2.029%\n",
            "[00:23:18]   [Fold 5/10] train=906, val=185\n",
            "[00:23:23]     → Fold SMAPE: 1.618%\n",
            "[00:23:23]   [Fold 6/10] train=1091, val=185\n",
            "[00:23:28]     → Fold SMAPE: 3.428%\n",
            "[00:23:28]   [Fold 7/10] train=1276, val=185\n",
            "[00:23:34]     → Fold SMAPE: 0.575%\n",
            "[00:23:34]   [Fold 8/10] train=1461, val=185\n",
            "[00:23:40]     → Fold SMAPE: 0.799%\n",
            "[00:23:40]   [Fold 9/10] train=1646, val=185\n",
            "[00:23:46]     → Fold SMAPE: 1.622%\n",
            "[00:23:46]   [Fold 10/10] train=1831, val=185\n",
            "[00:23:51]     → Fold SMAPE: 0.559%\n",
            "[00:23:55]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:23:55]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:23:55]   [Fold 1/10] train=166, val=185\n",
            "[00:24:00]     → Fold SMAPE: 4.521%\n",
            "[00:24:00]   [Fold 2/10] train=351, val=185\n",
            "[00:24:05]     → Fold SMAPE: 1.958%\n",
            "[00:24:05]   [Fold 3/10] train=536, val=185\n",
            "[00:24:10]     → Fold SMAPE: 4.775%\n",
            "[00:24:10]   [Fold 4/10] train=721, val=185\n",
            "[00:24:16]     → Fold SMAPE: 21.840%\n",
            "[00:24:16]   [Fold 5/10] train=906, val=185\n",
            "[00:24:23]     → Fold SMAPE: 2.822%\n",
            "[00:24:23]   [Fold 6/10] train=1091, val=185\n",
            "[00:24:30]     → Fold SMAPE: 3.399%\n",
            "[00:24:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:24:37]     → Fold SMAPE: 11.525%\n",
            "[00:24:37]   [Fold 8/10] train=1461, val=185\n",
            "[00:24:44]     → Fold SMAPE: 1.507%\n",
            "[00:24:44]   [Fold 9/10] train=1646, val=185\n",
            "[00:24:51]     → Fold SMAPE: 1.173%\n",
            "[00:24:51]   [Fold 10/10] train=1831, val=185\n",
            "[00:24:58]     → Fold SMAPE: 2.147%\n",
            "[00:25:03]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:25:03]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:25:03]   [Fold 1/10] train=166, val=185\n",
            "[00:25:08]     → Fold SMAPE: 5.377%\n",
            "[00:25:08]   [Fold 2/10] train=351, val=185\n",
            "[00:25:13]     → Fold SMAPE: 2.706%\n",
            "[00:25:13]   [Fold 3/10] train=536, val=185\n",
            "[00:25:18]     → Fold SMAPE: 2.238%\n",
            "[00:25:18]   [Fold 4/10] train=721, val=185\n",
            "[00:25:23]     → Fold SMAPE: 3.285%\n",
            "[00:25:23]   [Fold 5/10] train=906, val=185\n",
            "[00:25:29]     → Fold SMAPE: 1.376%\n",
            "[00:25:29]   [Fold 6/10] train=1091, val=185\n",
            "[00:25:34]     → Fold SMAPE: 1.941%\n",
            "[00:25:34]   [Fold 7/10] train=1276, val=185\n",
            "[00:25:39]     → Fold SMAPE: 4.438%\n",
            "[00:25:39]   [Fold 8/10] train=1461, val=185\n",
            "[00:25:44]     → Fold SMAPE: 3.255%\n",
            "[00:25:44]   [Fold 9/10] train=1646, val=185\n",
            "[00:25:50]     → Fold SMAPE: 1.085%\n",
            "[00:25:50]   [Fold 10/10] train=1831, val=185\n",
            "[00:25:56]     → Fold SMAPE: 1.765%\n",
            "[00:25:59]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:25:59]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:25:59]   [Fold 1/10] train=166, val=185\n",
            "[00:26:04]     → Fold SMAPE: 7.547%\n",
            "[00:26:04]   [Fold 2/10] train=351, val=185\n",
            "[00:26:09]     → Fold SMAPE: 2.429%\n",
            "[00:26:09]   [Fold 3/10] train=536, val=185\n",
            "[00:26:14]     → Fold SMAPE: 1.310%\n",
            "[00:26:14]   [Fold 4/10] train=721, val=185\n",
            "[00:26:20]     → Fold SMAPE: 1.066%\n",
            "[00:26:20]   [Fold 5/10] train=906, val=185\n",
            "[00:26:25]     → Fold SMAPE: 0.803%\n",
            "[00:26:25]   [Fold 6/10] train=1091, val=185\n",
            "[00:26:30]     → Fold SMAPE: 0.892%\n",
            "[00:26:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:26:36]     → Fold SMAPE: 0.694%\n",
            "[00:26:36]   [Fold 8/10] train=1461, val=185\n",
            "[00:26:41]     → Fold SMAPE: 0.623%\n",
            "[00:26:41]   [Fold 9/10] train=1646, val=185\n",
            "[00:26:47]     → Fold SMAPE: 0.561%\n",
            "[00:26:47]   [Fold 10/10] train=1831, val=185\n",
            "[00:26:52]     → Fold SMAPE: 1.292%\n",
            "[00:26:56]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:26:56]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:26:56]   [Fold 1/10] train=166, val=185\n",
            "[00:27:01]     → Fold SMAPE: 3.752%\n",
            "[00:27:01]   [Fold 2/10] train=351, val=185\n",
            "[00:27:06]     → Fold SMAPE: 2.819%\n",
            "[00:27:06]   [Fold 3/10] train=536, val=185\n",
            "[00:27:10]     → Fold SMAPE: 2.454%\n",
            "[00:27:10]   [Fold 4/10] train=721, val=185\n",
            "[00:27:15]     → Fold SMAPE: 1.423%\n",
            "[00:27:15]   [Fold 5/10] train=906, val=185\n",
            "[00:27:20]     → Fold SMAPE: 1.434%\n",
            "[00:27:20]   [Fold 6/10] train=1091, val=185\n",
            "[00:27:25]     → Fold SMAPE: 1.456%\n",
            "[00:27:25]   [Fold 7/10] train=1276, val=185\n",
            "[00:27:31]     → Fold SMAPE: 1.461%\n",
            "[00:27:31]   [Fold 8/10] train=1461, val=185\n",
            "[00:27:36]     → Fold SMAPE: 1.319%\n",
            "[00:27:36]   [Fold 9/10] train=1646, val=185\n",
            "[00:27:41]     → Fold SMAPE: 1.650%\n",
            "[00:27:41]   [Fold 10/10] train=1831, val=185\n",
            "[00:27:46]     → Fold SMAPE: 1.227%\n",
            "[00:27:50]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:27:50]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:27:50]   [Fold 1/10] train=166, val=185\n",
            "[00:27:54]     → Fold SMAPE: 4.911%\n",
            "[00:27:54]   [Fold 2/10] train=351, val=185\n",
            "[00:27:59]     → Fold SMAPE: 1.861%\n",
            "[00:27:59]   [Fold 3/10] train=536, val=185\n",
            "[00:28:04]     → Fold SMAPE: 1.281%\n",
            "[00:28:04]   [Fold 4/10] train=721, val=185\n",
            "[00:28:09]     → Fold SMAPE: 2.117%\n",
            "[00:28:09]   [Fold 5/10] train=906, val=185\n",
            "[00:28:14]     → Fold SMAPE: 1.775%\n",
            "[00:28:14]   [Fold 6/10] train=1091, val=185\n",
            "[00:28:19]     → Fold SMAPE: 1.971%\n",
            "[00:28:19]   [Fold 7/10] train=1276, val=185\n",
            "[00:28:24]     → Fold SMAPE: 3.030%\n",
            "[00:28:24]   [Fold 8/10] train=1461, val=185\n",
            "[00:28:29]     → Fold SMAPE: 1.986%\n",
            "[00:28:29]   [Fold 9/10] train=1646, val=185\n",
            "[00:28:34]     → Fold SMAPE: 1.821%\n",
            "[00:28:34]   [Fold 10/10] train=1831, val=185\n",
            "[00:28:40]     → Fold SMAPE: 0.807%\n",
            "[00:28:43]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:28:43]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:28:43]   [Fold 1/10] train=166, val=185\n",
            "[00:28:48]     → Fold SMAPE: 6.025%\n",
            "[00:28:48]   [Fold 2/10] train=351, val=185\n",
            "[00:28:53]     → Fold SMAPE: 3.009%\n",
            "[00:28:53]   [Fold 3/10] train=536, val=185\n",
            "[00:28:58]     → Fold SMAPE: 1.326%\n",
            "[00:28:58]   [Fold 4/10] train=721, val=185\n",
            "[00:29:03]     → Fold SMAPE: 1.841%\n",
            "[00:29:03]   [Fold 5/10] train=906, val=185\n",
            "[00:29:08]     → Fold SMAPE: 1.816%\n",
            "[00:29:08]   [Fold 6/10] train=1091, val=185\n",
            "[00:29:13]     → Fold SMAPE: 3.149%\n",
            "[00:29:13]   [Fold 7/10] train=1276, val=185\n",
            "[00:29:19]     → Fold SMAPE: 2.979%\n",
            "[00:29:19]   [Fold 8/10] train=1461, val=185\n",
            "[00:29:24]     → Fold SMAPE: 2.217%\n",
            "[00:29:24]   [Fold 9/10] train=1646, val=185\n",
            "[00:29:30]     → Fold SMAPE: 2.213%\n",
            "[00:29:30]   [Fold 10/10] train=1831, val=185\n",
            "[00:29:35]     → Fold SMAPE: 1.487%\n",
            "[00:29:39]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:29:39]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:29:39]   [Fold 1/10] train=166, val=185\n",
            "[00:29:44]     → Fold SMAPE: 4.591%\n",
            "[00:29:44]   [Fold 2/10] train=351, val=185\n",
            "[00:29:49]     → Fold SMAPE: 2.595%\n",
            "[00:29:49]   [Fold 3/10] train=536, val=185\n",
            "[00:29:54]     → Fold SMAPE: 1.490%\n",
            "[00:29:54]   [Fold 4/10] train=721, val=185\n",
            "[00:29:59]     → Fold SMAPE: 3.165%\n",
            "[00:29:59]   [Fold 5/10] train=906, val=185\n",
            "[00:30:04]     → Fold SMAPE: 1.915%\n",
            "[00:30:04]   [Fold 6/10] train=1091, val=185\n",
            "[00:30:09]     → Fold SMAPE: 1.296%\n",
            "[00:30:09]   [Fold 7/10] train=1276, val=185\n",
            "[00:30:15]     → Fold SMAPE: 0.905%\n",
            "[00:30:15]   [Fold 8/10] train=1461, val=185\n",
            "[00:30:20]     → Fold SMAPE: 0.815%\n",
            "[00:30:20]   [Fold 9/10] train=1646, val=185\n",
            "[00:30:25]     → Fold SMAPE: 0.724%\n",
            "[00:30:25]   [Fold 10/10] train=1831, val=185\n",
            "[00:30:31]     → Fold SMAPE: 0.932%\n",
            "[00:30:34]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:30:34]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:30:34]   [Fold 1/10] train=166, val=185\n",
            "[00:30:39]     → Fold SMAPE: 2.282%\n",
            "[00:30:39]   [Fold 2/10] train=351, val=185\n",
            "[00:30:44]     → Fold SMAPE: 1.228%\n",
            "[00:30:44]   [Fold 3/10] train=536, val=185\n",
            "[00:30:49]     → Fold SMAPE: 0.929%\n",
            "[00:30:49]   [Fold 4/10] train=721, val=185\n",
            "[00:30:53]     → Fold SMAPE: 0.545%\n",
            "[00:30:53]   [Fold 5/10] train=906, val=185\n",
            "[00:30:58]     → Fold SMAPE: 0.727%\n",
            "[00:30:58]   [Fold 6/10] train=1091, val=185\n",
            "[00:31:03]     → Fold SMAPE: 1.448%\n",
            "[00:31:03]   [Fold 7/10] train=1276, val=185\n",
            "[00:31:08]     → Fold SMAPE: 1.505%\n",
            "[00:31:08]   [Fold 8/10] train=1461, val=185\n",
            "[00:31:13]     → Fold SMAPE: 1.112%\n",
            "[00:31:13]   [Fold 9/10] train=1646, val=185\n",
            "[00:31:18]     → Fold SMAPE: 0.628%\n",
            "[00:31:18]   [Fold 10/10] train=1831, val=185\n",
            "[00:31:22]     → Fold SMAPE: 0.413%\n",
            "[00:31:25]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:31:25]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:31:25]   [Fold 1/10] train=166, val=185\n",
            "[00:31:30]     → Fold SMAPE: 1.511%\n",
            "[00:31:30]   [Fold 2/10] train=351, val=185\n",
            "[00:31:34]     → Fold SMAPE: 1.100%\n",
            "[00:31:34]   [Fold 3/10] train=536, val=185\n",
            "[00:31:39]     → Fold SMAPE: 0.862%\n",
            "[00:31:39]   [Fold 4/10] train=721, val=185\n",
            "[00:31:44]     → Fold SMAPE: 1.014%\n",
            "[00:31:44]   [Fold 5/10] train=906, val=185\n",
            "[00:31:49]     → Fold SMAPE: 0.597%\n",
            "[00:31:49]   [Fold 6/10] train=1091, val=185\n",
            "[00:31:54]     → Fold SMAPE: 0.855%\n",
            "[00:31:54]   [Fold 7/10] train=1276, val=185\n",
            "[00:31:59]     → Fold SMAPE: 2.523%\n",
            "[00:31:59]   [Fold 8/10] train=1461, val=185\n",
            "[00:32:04]     → Fold SMAPE: 1.682%\n",
            "[00:32:04]   [Fold 9/10] train=1646, val=185\n",
            "[00:32:09]     → Fold SMAPE: 2.748%\n",
            "[00:32:09]   [Fold 10/10] train=1831, val=185\n",
            "[00:32:16]     → Fold SMAPE: 3.884%\n",
            "[00:32:21]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:32:21]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:32:21]   [Fold 1/10] train=166, val=185\n",
            "[00:32:26]     → Fold SMAPE: 14.104%\n",
            "[00:32:26]   [Fold 2/10] train=351, val=185\n",
            "[00:32:31]     → Fold SMAPE: 21.208%\n",
            "[00:32:31]   [Fold 3/10] train=536, val=185\n",
            "[00:32:37]     → Fold SMAPE: 5.342%\n",
            "[00:32:37]   [Fold 4/10] train=721, val=185\n",
            "[00:32:43]     → Fold SMAPE: 6.163%\n",
            "[00:32:43]   [Fold 5/10] train=906, val=185\n",
            "[00:32:49]     → Fold SMAPE: 1.744%\n",
            "[00:32:49]   [Fold 6/10] train=1091, val=185\n",
            "[00:32:55]     → Fold SMAPE: 1.412%\n",
            "[00:32:55]   [Fold 7/10] train=1276, val=185\n",
            "[00:33:01]     → Fold SMAPE: 1.018%\n",
            "[00:33:01]   [Fold 8/10] train=1461, val=185\n",
            "[00:33:07]     → Fold SMAPE: 0.630%\n",
            "[00:33:07]   [Fold 9/10] train=1646, val=185\n",
            "[00:33:13]     → Fold SMAPE: 0.552%\n",
            "[00:33:13]   [Fold 10/10] train=1831, val=185\n",
            "[00:33:19]     → Fold SMAPE: 1.088%\n",
            "[00:33:23]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:33:23]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:33:23]   [Fold 1/10] train=166, val=185\n",
            "[00:33:28]     → Fold SMAPE: 4.392%\n",
            "[00:33:28]   [Fold 2/10] train=351, val=185\n",
            "[00:33:33]     → Fold SMAPE: 4.582%\n",
            "[00:33:33]   [Fold 3/10] train=536, val=185\n",
            "[00:33:38]     → Fold SMAPE: 1.969%\n",
            "[00:33:38]   [Fold 4/10] train=721, val=185\n",
            "[00:33:43]     → Fold SMAPE: 1.653%\n",
            "[00:33:43]   [Fold 5/10] train=906, val=185\n",
            "[00:33:48]     → Fold SMAPE: 1.693%\n",
            "[00:33:48]   [Fold 6/10] train=1091, val=185\n",
            "[00:33:53]     → Fold SMAPE: 1.043%\n",
            "[00:33:53]   [Fold 7/10] train=1276, val=185\n",
            "[00:33:58]     → Fold SMAPE: 2.669%\n",
            "[00:33:58]   [Fold 8/10] train=1461, val=185\n",
            "[00:34:04]     → Fold SMAPE: 5.439%\n",
            "[00:34:04]   [Fold 9/10] train=1646, val=185\n",
            "[00:34:09]     → Fold SMAPE: 2.566%\n",
            "[00:34:09]   [Fold 10/10] train=1831, val=185\n",
            "[00:34:15]     → Fold SMAPE: 2.163%\n",
            "[00:34:19]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:34:19]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:34:19]   [Fold 1/10] train=166, val=185\n",
            "[00:34:23]     → Fold SMAPE: 9.269%\n",
            "[00:34:23]   [Fold 2/10] train=351, val=185\n",
            "[00:34:28]     → Fold SMAPE: 0.715%\n",
            "[00:34:28]   [Fold 3/10] train=536, val=185\n",
            "[00:34:33]     → Fold SMAPE: 0.861%\n",
            "[00:34:33]   [Fold 4/10] train=721, val=185\n",
            "[00:34:38]     → Fold SMAPE: 3.265%\n",
            "[00:34:38]   [Fold 5/10] train=906, val=185\n",
            "[00:34:43]     → Fold SMAPE: 0.496%\n",
            "[00:34:43]   [Fold 6/10] train=1091, val=185\n",
            "[00:34:48]     → Fold SMAPE: 2.388%\n",
            "[00:34:48]   [Fold 7/10] train=1276, val=185\n",
            "[00:34:53]     → Fold SMAPE: 3.571%\n",
            "[00:34:53]   [Fold 8/10] train=1461, val=185\n",
            "[00:34:57]     → Fold SMAPE: 1.012%\n",
            "[00:34:57]   [Fold 9/10] train=1646, val=185\n",
            "[00:35:02]     → Fold SMAPE: 0.286%\n",
            "[00:35:02]   [Fold 10/10] train=1831, val=185\n",
            "[00:35:07]     → Fold SMAPE: 0.264%\n",
            "[00:35:11]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:35:11]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:35:11]   [Fold 1/10] train=166, val=185\n",
            "[00:35:15]     → Fold SMAPE: 2.671%\n",
            "[00:35:15]   [Fold 2/10] train=351, val=185\n",
            "[00:35:20]     → Fold SMAPE: 1.734%\n",
            "[00:35:20]   [Fold 3/10] train=536, val=185\n",
            "[00:35:25]     → Fold SMAPE: 0.625%\n",
            "[00:35:25]   [Fold 4/10] train=721, val=185\n",
            "[00:35:29]     → Fold SMAPE: 0.171%\n",
            "[00:35:29]   [Fold 5/10] train=906, val=185\n",
            "[00:35:34]     → Fold SMAPE: 0.802%\n",
            "[00:35:34]   [Fold 6/10] train=1091, val=185\n",
            "[00:35:39]     → Fold SMAPE: 0.585%\n",
            "[00:35:39]   [Fold 7/10] train=1276, val=185\n",
            "[00:35:44]     → Fold SMAPE: 1.189%\n",
            "[00:35:44]   [Fold 8/10] train=1461, val=185\n",
            "[00:35:49]     → Fold SMAPE: 1.068%\n",
            "[00:35:49]   [Fold 9/10] train=1646, val=185\n",
            "[00:35:54]     → Fold SMAPE: 0.556%\n",
            "[00:35:54]   [Fold 10/10] train=1831, val=185\n",
            "[00:35:58]     → Fold SMAPE: 0.390%\n",
            "[00:36:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:36:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:36:01]   [Fold 1/10] train=166, val=185\n",
            "[00:36:06]     → Fold SMAPE: 11.421%\n",
            "[00:36:06]   [Fold 2/10] train=351, val=185\n",
            "[00:36:11]     → Fold SMAPE: 1.474%\n",
            "[00:36:11]   [Fold 3/10] train=536, val=185\n",
            "[00:36:16]     → Fold SMAPE: 0.639%\n",
            "[00:36:16]   [Fold 4/10] train=721, val=185\n",
            "[00:36:21]     → Fold SMAPE: 0.788%\n",
            "[00:36:21]   [Fold 5/10] train=906, val=185\n",
            "[00:36:26]     → Fold SMAPE: 0.681%\n",
            "[00:36:26]   [Fold 6/10] train=1091, val=185\n",
            "[00:36:30]     → Fold SMAPE: 1.303%\n",
            "[00:36:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:36:36]     → Fold SMAPE: 1.472%\n",
            "[00:36:36]   [Fold 8/10] train=1461, val=185\n",
            "[00:36:40]     → Fold SMAPE: 1.129%\n",
            "[00:36:40]   [Fold 9/10] train=1646, val=185\n",
            "[00:36:45]     → Fold SMAPE: 0.580%\n",
            "[00:36:45]   [Fold 10/10] train=1831, val=185\n",
            "[00:36:50]     → Fold SMAPE: 0.456%\n",
            "[00:36:53]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:36:53]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:36:53]   [Fold 1/10] train=166, val=185\n",
            "[00:36:58]     → Fold SMAPE: 10.876%\n",
            "[00:36:58]   [Fold 2/10] train=351, val=185\n",
            "[00:37:03]     → Fold SMAPE: 2.687%\n",
            "[00:37:03]   [Fold 3/10] train=536, val=185\n",
            "[00:37:07]     → Fold SMAPE: 1.158%\n",
            "[00:37:07]   [Fold 4/10] train=721, val=185\n",
            "[00:37:12]     → Fold SMAPE: 3.501%\n",
            "[00:37:12]   [Fold 5/10] train=906, val=185\n",
            "[00:37:17]     → Fold SMAPE: 0.763%\n",
            "[00:37:17]   [Fold 6/10] train=1091, val=185\n",
            "[00:37:23]     → Fold SMAPE: 2.064%\n",
            "[00:37:23]   [Fold 7/10] train=1276, val=185\n",
            "[00:37:28]     → Fold SMAPE: 3.588%\n",
            "[00:37:28]   [Fold 8/10] train=1461, val=185\n",
            "[00:37:33]     → Fold SMAPE: 2.379%\n",
            "[00:37:33]   [Fold 9/10] train=1646, val=185\n",
            "[00:37:39]     → Fold SMAPE: 1.280%\n",
            "[00:37:39]   [Fold 10/10] train=1831, val=185\n",
            "[00:37:44]     → Fold SMAPE: 0.615%\n",
            "[00:37:48]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:37:48]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:37:48]   [Fold 1/10] train=166, val=185\n",
            "[00:37:52]     → Fold SMAPE: 7.247%\n",
            "[00:37:52]   [Fold 2/10] train=351, val=185\n",
            "[00:37:57]     → Fold SMAPE: 2.850%\n",
            "[00:37:57]   [Fold 3/10] train=536, val=185\n",
            "[00:38:02]     → Fold SMAPE: 3.082%\n",
            "[00:38:02]   [Fold 4/10] train=721, val=185\n",
            "[00:38:07]     → Fold SMAPE: 1.298%\n",
            "[00:38:07]   [Fold 5/10] train=906, val=185\n",
            "[00:38:13]     → Fold SMAPE: 1.848%\n",
            "[00:38:13]   [Fold 6/10] train=1091, val=185\n",
            "[00:38:18]     → Fold SMAPE: 1.709%\n",
            "[00:38:18]   [Fold 7/10] train=1276, val=185\n",
            "[00:38:23]     → Fold SMAPE: 3.109%\n",
            "[00:38:23]   [Fold 8/10] train=1461, val=185\n",
            "[00:38:29]     → Fold SMAPE: 2.577%\n",
            "[00:38:29]   [Fold 9/10] train=1646, val=185\n",
            "[00:38:34]     → Fold SMAPE: 0.997%\n",
            "[00:38:34]   [Fold 10/10] train=1831, val=185\n",
            "[00:38:40]     → Fold SMAPE: 1.082%\n",
            "[00:38:43]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:38:43]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:38:43]   [Fold 1/10] train=166, val=185\n",
            "[00:38:48]     → Fold SMAPE: 4.087%\n",
            "[00:38:48]   [Fold 2/10] train=351, val=185\n",
            "[00:38:53]     → Fold SMAPE: 2.756%\n",
            "[00:38:53]   [Fold 3/10] train=536, val=185\n",
            "[00:38:58]     → Fold SMAPE: 0.730%\n",
            "[00:38:58]   [Fold 4/10] train=721, val=185\n",
            "[00:39:03]     → Fold SMAPE: 0.668%\n",
            "[00:39:03]   [Fold 5/10] train=906, val=185\n",
            "[00:39:08]     → Fold SMAPE: 0.621%\n",
            "[00:39:08]   [Fold 6/10] train=1091, val=185\n",
            "[00:39:13]     → Fold SMAPE: 0.861%\n",
            "[00:39:13]   [Fold 7/10] train=1276, val=185\n",
            "[00:39:18]     → Fold SMAPE: 2.526%\n",
            "[00:39:18]   [Fold 8/10] train=1461, val=185\n",
            "[00:39:23]     → Fold SMAPE: 2.177%\n",
            "[00:39:23]   [Fold 9/10] train=1646, val=185\n",
            "[00:39:28]     → Fold SMAPE: 1.403%\n",
            "[00:39:28]   [Fold 10/10] train=1831, val=185\n",
            "[00:39:33]     → Fold SMAPE: 0.708%\n",
            "[00:39:37]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:39:37]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:39:37]   [Fold 1/10] train=166, val=185\n",
            "[00:39:42]     → Fold SMAPE: 5.954%\n",
            "[00:39:42]   [Fold 2/10] train=351, val=185\n",
            "[00:39:46]     → Fold SMAPE: 4.739%\n",
            "[00:39:46]   [Fold 3/10] train=536, val=185\n",
            "[00:39:52]     → Fold SMAPE: 5.357%\n",
            "[00:39:52]   [Fold 4/10] train=721, val=185\n",
            "[00:39:57]     → Fold SMAPE: 4.134%\n",
            "[00:39:57]   [Fold 5/10] train=906, val=185\n",
            "[00:40:02]     → Fold SMAPE: 4.163%\n",
            "[00:40:02]   [Fold 6/10] train=1091, val=185\n",
            "[00:40:07]     → Fold SMAPE: 2.432%\n",
            "[00:40:07]   [Fold 7/10] train=1276, val=185\n",
            "[00:40:13]     → Fold SMAPE: 5.049%\n",
            "[00:40:13]   [Fold 8/10] train=1461, val=185\n",
            "[00:40:18]     → Fold SMAPE: 2.247%\n",
            "[00:40:18]   [Fold 9/10] train=1646, val=185\n",
            "[00:40:23]     → Fold SMAPE: 1.020%\n",
            "[00:40:23]   [Fold 10/10] train=1831, val=185\n",
            "[00:40:29]     → Fold SMAPE: 1.264%\n",
            "[00:40:32]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:40:33]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:40:33]   [Fold 1/10] train=166, val=185\n",
            "[00:40:37]     → Fold SMAPE: 7.448%\n",
            "[00:40:37]   [Fold 2/10] train=351, val=185\n",
            "[00:40:42]     → Fold SMAPE: 1.610%\n",
            "[00:40:42]   [Fold 3/10] train=536, val=185\n",
            "[00:40:47]     → Fold SMAPE: 1.486%\n",
            "[00:40:47]   [Fold 4/10] train=721, val=185\n",
            "[00:40:52]     → Fold SMAPE: 2.426%\n",
            "[00:40:52]   [Fold 5/10] train=906, val=185\n",
            "[00:40:57]     → Fold SMAPE: 0.545%\n",
            "[00:40:57]   [Fold 6/10] train=1091, val=185\n",
            "[00:41:02]     → Fold SMAPE: 2.309%\n",
            "[00:41:02]   [Fold 7/10] train=1276, val=185\n",
            "[00:41:07]     → Fold SMAPE: 3.705%\n",
            "[00:41:07]   [Fold 8/10] train=1461, val=185\n",
            "[00:41:12]     → Fold SMAPE: 1.246%\n",
            "[00:41:12]   [Fold 9/10] train=1646, val=185\n",
            "[00:41:17]     → Fold SMAPE: 0.486%\n",
            "[00:41:17]   [Fold 10/10] train=1831, val=185\n",
            "[00:41:22]     → Fold SMAPE: 0.584%\n",
            "[00:41:25]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:41:26]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:41:26]   [Fold 1/10] train=166, val=185\n",
            "[00:41:30]     → Fold SMAPE: 8.040%\n",
            "[00:41:30]   [Fold 2/10] train=351, val=185\n",
            "[00:41:35]     → Fold SMAPE: 3.423%\n",
            "[00:41:35]   [Fold 3/10] train=536, val=185\n",
            "[00:41:40]     → Fold SMAPE: 2.458%\n",
            "[00:41:40]   [Fold 4/10] train=721, val=185\n",
            "[00:41:45]     → Fold SMAPE: 2.116%\n",
            "[00:41:45]   [Fold 5/10] train=906, val=185\n",
            "[00:41:50]     → Fold SMAPE: 1.181%\n",
            "[00:41:50]   [Fold 6/10] train=1091, val=185\n",
            "[00:41:55]     → Fold SMAPE: 1.200%\n",
            "[00:41:55]   [Fold 7/10] train=1276, val=185\n",
            "[00:42:01]     → Fold SMAPE: 2.404%\n",
            "[00:42:01]   [Fold 8/10] train=1461, val=185\n",
            "[00:42:06]     → Fold SMAPE: 2.109%\n",
            "[00:42:06]   [Fold 9/10] train=1646, val=185\n",
            "[00:42:11]     → Fold SMAPE: 1.875%\n",
            "[00:42:11]   [Fold 10/10] train=1831, val=185\n",
            "[00:42:16]     → Fold SMAPE: 2.306%\n",
            "[00:42:20]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:42:20]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:42:20]   [Fold 1/10] train=166, val=185\n",
            "[00:42:25]     → Fold SMAPE: 2.297%\n",
            "[00:42:25]   [Fold 2/10] train=351, val=185\n",
            "[00:42:30]     → Fold SMAPE: 1.958%\n",
            "[00:42:30]   [Fold 3/10] train=536, val=185\n",
            "[00:42:34]     → Fold SMAPE: 0.613%\n",
            "[00:42:34]   [Fold 4/10] train=721, val=185\n",
            "[00:42:39]     → Fold SMAPE: 1.286%\n",
            "[00:42:39]   [Fold 5/10] train=906, val=185\n",
            "[00:42:44]     → Fold SMAPE: 1.288%\n",
            "[00:42:44]   [Fold 6/10] train=1091, val=185\n",
            "[00:42:49]     → Fold SMAPE: 2.205%\n",
            "[00:42:49]   [Fold 7/10] train=1276, val=185\n",
            "[00:42:54]     → Fold SMAPE: 0.981%\n",
            "[00:42:54]   [Fold 8/10] train=1461, val=185\n",
            "[00:42:59]     → Fold SMAPE: 0.295%\n",
            "[00:42:59]   [Fold 9/10] train=1646, val=185\n",
            "[00:43:04]     → Fold SMAPE: 0.154%\n",
            "[00:43:04]   [Fold 10/10] train=1831, val=185\n",
            "[00:43:08]     → Fold SMAPE: 0.344%\n",
            "[00:43:11]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:43:11]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:43:11]   [Fold 1/10] train=166, val=185\n",
            "[00:43:16]     → Fold SMAPE: 6.433%\n",
            "[00:43:16]   [Fold 2/10] train=351, val=185\n",
            "[00:43:21]     → Fold SMAPE: 2.122%\n",
            "[00:43:21]   [Fold 3/10] train=536, val=185\n",
            "[00:43:26]     → Fold SMAPE: 1.908%\n",
            "[00:43:26]   [Fold 4/10] train=721, val=185\n",
            "[00:43:31]     → Fold SMAPE: 1.237%\n",
            "[00:43:31]   [Fold 5/10] train=906, val=185\n",
            "[00:43:36]     → Fold SMAPE: 0.963%\n",
            "[00:43:36]   [Fold 6/10] train=1091, val=185\n",
            "[00:43:42]     → Fold SMAPE: 0.841%\n",
            "[00:43:42]   [Fold 7/10] train=1276, val=185\n",
            "[00:43:47]     → Fold SMAPE: 0.950%\n",
            "[00:43:47]   [Fold 8/10] train=1461, val=185\n",
            "[00:43:52]     → Fold SMAPE: 0.865%\n",
            "[00:43:52]   [Fold 9/10] train=1646, val=185\n",
            "[00:43:58]     → Fold SMAPE: 0.978%\n",
            "[00:43:58]   [Fold 10/10] train=1831, val=185\n",
            "[00:44:03]     → Fold SMAPE: 0.777%\n",
            "[00:44:07]   [Done] OOF SMAPE=200.000%  scale=34070177367385068674220032.0000\n",
            "[00:44:07]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:44:07]   [Fold 1/10] train=166, val=185\n",
            "[00:44:12]     → Fold SMAPE: 5.845%\n",
            "[00:44:12]   [Fold 2/10] train=351, val=185\n",
            "[00:44:17]     → Fold SMAPE: 2.354%\n",
            "[00:44:17]   [Fold 3/10] train=536, val=185\n",
            "[00:44:22]     → Fold SMAPE: 1.820%\n",
            "[00:44:22]   [Fold 4/10] train=721, val=185\n",
            "[00:44:27]     → Fold SMAPE: 3.655%\n",
            "[00:44:27]   [Fold 5/10] train=906, val=185\n",
            "[00:44:31]     → Fold SMAPE: 1.144%\n",
            "[00:44:31]   [Fold 6/10] train=1091, val=185\n",
            "[00:44:37]     → Fold SMAPE: 2.031%\n",
            "[00:44:37]   [Fold 7/10] train=1276, val=185\n",
            "[00:44:42]     → Fold SMAPE: 2.134%\n",
            "[00:44:42]   [Fold 8/10] train=1461, val=185\n",
            "[00:44:47]     → Fold SMAPE: 2.840%\n",
            "[00:44:47]   [Fold 9/10] train=1646, val=185\n",
            "[00:44:52]     → Fold SMAPE: 1.173%\n",
            "[00:44:52]   [Fold 10/10] train=1831, val=185\n",
            "[00:44:57]     → Fold SMAPE: 1.363%\n",
            "[00:45:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:45:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:45:01]   [Fold 1/10] train=166, val=185\n",
            "[00:45:05]     → Fold SMAPE: 2.989%\n",
            "[00:45:05]   [Fold 2/10] train=351, val=185\n",
            "[00:45:11]     → Fold SMAPE: 19.289%\n",
            "[00:45:11]   [Fold 3/10] train=536, val=185\n",
            "[00:45:17]     → Fold SMAPE: 6.991%\n",
            "[00:45:17]   [Fold 4/10] train=721, val=185\n",
            "[00:45:23]     → Fold SMAPE: 6.567%\n",
            "[00:45:23]   [Fold 5/10] train=906, val=185\n",
            "[00:45:30]     → Fold SMAPE: 6.103%\n",
            "[00:45:30]   [Fold 6/10] train=1091, val=185\n",
            "[00:45:36]     → Fold SMAPE: 1.506%\n",
            "[00:45:36]   [Fold 7/10] train=1276, val=185\n",
            "[00:45:42]     → Fold SMAPE: 13.178%\n",
            "[00:45:42]   [Fold 8/10] train=1461, val=185\n",
            "[00:45:49]     → Fold SMAPE: 0.782%\n",
            "[00:45:49]   [Fold 9/10] train=1646, val=185\n",
            "[00:45:55]     → Fold SMAPE: 2.096%\n",
            "[00:45:55]   [Fold 10/10] train=1831, val=185\n",
            "[00:46:01]     → Fold SMAPE: 0.955%\n",
            "[00:46:05]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:46:05]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:46:05]   [Fold 1/10] train=166, val=185\n",
            "[00:46:10]     → Fold SMAPE: 4.976%\n",
            "[00:46:10]   [Fold 2/10] train=351, val=185\n",
            "[00:46:15]     → Fold SMAPE: 3.180%\n",
            "[00:46:15]   [Fold 3/10] train=536, val=185\n",
            "[00:46:20]     → Fold SMAPE: 4.054%\n",
            "[00:46:20]   [Fold 4/10] train=721, val=185\n",
            "[00:46:27]     → Fold SMAPE: 6.780%\n",
            "[00:46:27]   [Fold 5/10] train=906, val=185\n",
            "[00:46:33]     → Fold SMAPE: 3.803%\n",
            "[00:46:33]   [Fold 6/10] train=1091, val=185\n",
            "[00:46:40]     → Fold SMAPE: 2.830%\n",
            "[00:46:40]   [Fold 7/10] train=1276, val=185\n",
            "[00:46:47]     → Fold SMAPE: 10.667%\n",
            "[00:46:47]   [Fold 8/10] train=1461, val=185\n",
            "[00:46:54]     → Fold SMAPE: 2.239%\n",
            "[00:46:54]   [Fold 9/10] train=1646, val=185\n",
            "[00:47:01]     → Fold SMAPE: 1.529%\n",
            "[00:47:01]   [Fold 10/10] train=1831, val=185\n",
            "[00:47:08]     → Fold SMAPE: 2.176%\n",
            "[00:47:13]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:47:13]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:47:13]   [Fold 1/10] train=166, val=185\n",
            "[00:47:18]     → Fold SMAPE: 4.981%\n",
            "[00:47:18]   [Fold 2/10] train=351, val=185\n",
            "[00:47:23]     → Fold SMAPE: 1.777%\n",
            "[00:47:23]   [Fold 3/10] train=536, val=185\n",
            "[00:47:28]     → Fold SMAPE: 1.772%\n",
            "[00:47:28]   [Fold 4/10] train=721, val=185\n",
            "[00:47:32]     → Fold SMAPE: 4.919%\n",
            "[00:47:32]   [Fold 5/10] train=906, val=185\n",
            "[00:47:37]     → Fold SMAPE: 1.884%\n",
            "[00:47:37]   [Fold 6/10] train=1091, val=185\n",
            "[00:47:42]     → Fold SMAPE: 1.571%\n",
            "[00:47:42]   [Fold 7/10] train=1276, val=185\n",
            "[00:47:47]     → Fold SMAPE: 0.996%\n",
            "[00:47:47]   [Fold 8/10] train=1461, val=185\n",
            "[00:47:52]     → Fold SMAPE: 1.265%\n",
            "[00:47:52]   [Fold 9/10] train=1646, val=185\n",
            "[00:47:57]     → Fold SMAPE: 1.058%\n",
            "[00:47:57]   [Fold 10/10] train=1831, val=185\n",
            "[00:48:03]     → Fold SMAPE: 0.900%\n",
            "[00:48:06]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:48:06]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:48:06]   [Fold 1/10] train=166, val=185\n",
            "[00:48:11]     → Fold SMAPE: 7.324%\n",
            "[00:48:11]   [Fold 2/10] train=351, val=185\n",
            "[00:48:17]     → Fold SMAPE: 5.740%\n",
            "[00:48:17]   [Fold 3/10] train=536, val=185\n",
            "[00:48:23]     → Fold SMAPE: 2.825%\n",
            "[00:48:23]   [Fold 4/10] train=721, val=185\n",
            "[00:48:29]     → Fold SMAPE: 5.428%\n",
            "[00:48:29]   [Fold 5/10] train=906, val=185\n",
            "[00:48:35]     → Fold SMAPE: 2.580%\n",
            "[00:48:35]   [Fold 6/10] train=1091, val=185\n",
            "[00:48:41]     → Fold SMAPE: 4.406%\n",
            "[00:48:41]   [Fold 7/10] train=1276, val=185\n",
            "[00:48:48]     → Fold SMAPE: 5.453%\n",
            "[00:48:48]   [Fold 8/10] train=1461, val=185\n",
            "[00:48:54]     → Fold SMAPE: 3.541%\n",
            "[00:48:54]   [Fold 9/10] train=1646, val=185\n",
            "[00:49:00]     → Fold SMAPE: 3.677%\n",
            "[00:49:00]   [Fold 10/10] train=1831, val=185\n",
            "[00:49:07]     → Fold SMAPE: 1.194%\n",
            "[00:49:12]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:49:12]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:49:12]   [Fold 1/10] train=166, val=185\n",
            "[00:49:17]     → Fold SMAPE: 5.187%\n",
            "[00:49:17]   [Fold 2/10] train=351, val=185\n",
            "[00:49:21]     → Fold SMAPE: 3.700%\n",
            "[00:49:21]   [Fold 3/10] train=536, val=185\n",
            "[00:49:26]     → Fold SMAPE: 1.984%\n",
            "[00:49:26]   [Fold 4/10] train=721, val=185\n",
            "[00:49:31]     → Fold SMAPE: 4.209%\n",
            "[00:49:31]   [Fold 5/10] train=906, val=185\n",
            "[00:49:36]     → Fold SMAPE: 1.068%\n",
            "[00:49:36]   [Fold 6/10] train=1091, val=185\n",
            "[00:49:41]     → Fold SMAPE: 3.202%\n",
            "[00:49:41]   [Fold 7/10] train=1276, val=185\n",
            "[00:49:46]     → Fold SMAPE: 3.701%\n",
            "[00:49:46]   [Fold 8/10] train=1461, val=185\n",
            "[00:49:52]     → Fold SMAPE: 2.933%\n",
            "[00:49:52]   [Fold 9/10] train=1646, val=185\n",
            "[00:49:57]     → Fold SMAPE: 1.449%\n",
            "[00:49:57]   [Fold 10/10] train=1831, val=185\n",
            "[00:50:02]     → Fold SMAPE: 1.223%\n",
            "[00:50:06]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:50:06]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:50:06]   [Fold 1/10] train=166, val=185\n",
            "[00:50:10]     → Fold SMAPE: 5.292%\n",
            "[00:50:10]   [Fold 2/10] train=351, val=185\n",
            "[00:50:15]     → Fold SMAPE: 1.374%\n",
            "[00:50:15]   [Fold 3/10] train=536, val=185\n",
            "[00:50:20]     → Fold SMAPE: 1.266%\n",
            "[00:50:20]   [Fold 4/10] train=721, val=185\n",
            "[00:50:25]     → Fold SMAPE: 3.060%\n",
            "[00:50:25]   [Fold 5/10] train=906, val=185\n",
            "[00:50:31]     → Fold SMAPE: 1.677%\n",
            "[00:50:31]   [Fold 6/10] train=1091, val=185\n",
            "[00:50:36]     → Fold SMAPE: 2.909%\n",
            "[00:50:36]   [Fold 7/10] train=1276, val=185\n",
            "[00:50:42]     → Fold SMAPE: 21.325%\n",
            "[00:50:42]   [Fold 8/10] train=1461, val=185\n",
            "[00:50:49]     → Fold SMAPE: 3.067%\n",
            "[00:50:49]   [Fold 9/10] train=1646, val=185\n",
            "[00:50:56]     → Fold SMAPE: 3.356%\n",
            "[00:50:56]   [Fold 10/10] train=1831, val=185\n",
            "[00:51:03]     → Fold SMAPE: 3.131%\n",
            "[00:51:08]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:51:08]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:51:08]   [Fold 1/10] train=166, val=185\n",
            "[00:51:13]     → Fold SMAPE: 5.976%\n",
            "[00:51:13]   [Fold 2/10] train=351, val=185\n",
            "[00:51:18]     → Fold SMAPE: 7.733%\n",
            "[00:51:18]   [Fold 3/10] train=536, val=185\n",
            "[00:51:23]     → Fold SMAPE: 1.931%\n",
            "[00:51:23]   [Fold 4/10] train=721, val=185\n",
            "[00:51:28]     → Fold SMAPE: 2.723%\n",
            "[00:51:28]   [Fold 5/10] train=906, val=185\n",
            "[00:51:33]     → Fold SMAPE: 2.270%\n",
            "[00:51:33]   [Fold 6/10] train=1091, val=185\n",
            "[00:51:39]     → Fold SMAPE: 2.672%\n",
            "[00:51:39]   [Fold 7/10] train=1276, val=185\n",
            "[00:51:44]     → Fold SMAPE: 4.983%\n",
            "[00:51:44]   [Fold 8/10] train=1461, val=185\n",
            "[00:51:50]     → Fold SMAPE: 2.630%\n",
            "[00:51:50]   [Fold 9/10] train=1646, val=185\n",
            "[00:51:56]     → Fold SMAPE: 1.546%\n",
            "[00:51:56]   [Fold 10/10] train=1831, val=185\n",
            "[00:52:01]     → Fold SMAPE: 1.347%\n",
            "[00:52:05]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:52:05]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:52:05]   [Fold 1/10] train=166, val=185\n",
            "[00:52:10]     → Fold SMAPE: 7.662%\n",
            "[00:52:10]   [Fold 2/10] train=351, val=185\n",
            "[00:52:15]     → Fold SMAPE: 4.835%\n",
            "[00:52:15]   [Fold 3/10] train=536, val=185\n",
            "[00:52:20]     → Fold SMAPE: 2.223%\n",
            "[00:52:20]   [Fold 4/10] train=721, val=185\n",
            "[00:52:25]     → Fold SMAPE: 3.129%\n",
            "[00:52:25]   [Fold 5/10] train=906, val=185\n",
            "[00:52:30]     → Fold SMAPE: 1.387%\n",
            "[00:52:30]   [Fold 6/10] train=1091, val=185\n",
            "[00:52:36]     → Fold SMAPE: 1.814%\n",
            "[00:52:36]   [Fold 7/10] train=1276, val=185\n",
            "[00:52:41]     → Fold SMAPE: 1.913%\n",
            "[00:52:41]   [Fold 8/10] train=1461, val=185\n",
            "[00:52:46]     → Fold SMAPE: 2.909%\n",
            "[00:52:46]   [Fold 9/10] train=1646, val=185\n",
            "[00:52:52]     → Fold SMAPE: 1.191%\n",
            "[00:52:52]   [Fold 10/10] train=1831, val=185\n",
            "[00:52:58]     → Fold SMAPE: 0.943%\n",
            "[00:53:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:53:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:53:01]   [Fold 1/10] train=166, val=185\n",
            "[00:53:06]     → Fold SMAPE: 4.734%\n",
            "[00:53:06]   [Fold 2/10] train=351, val=185\n",
            "[00:53:11]     → Fold SMAPE: 1.339%\n",
            "[00:53:11]   [Fold 3/10] train=536, val=185\n",
            "[00:53:16]     → Fold SMAPE: 1.510%\n",
            "[00:53:16]   [Fold 4/10] train=721, val=185\n",
            "[00:53:21]     → Fold SMAPE: 2.759%\n",
            "[00:53:21]   [Fold 5/10] train=906, val=185\n",
            "[00:53:26]     → Fold SMAPE: 0.763%\n",
            "[00:53:26]   [Fold 6/10] train=1091, val=185\n",
            "[00:53:31]     → Fold SMAPE: 1.183%\n",
            "[00:53:31]   [Fold 7/10] train=1276, val=185\n",
            "[00:53:36]     → Fold SMAPE: 2.322%\n",
            "[00:53:36]   [Fold 8/10] train=1461, val=185\n",
            "[00:53:42]     → Fold SMAPE: 1.906%\n",
            "[00:53:42]   [Fold 9/10] train=1646, val=185\n",
            "[00:53:47]     → Fold SMAPE: 0.987%\n",
            "[00:53:47]   [Fold 10/10] train=1831, val=185\n",
            "[00:53:52]     → Fold SMAPE: 1.038%\n",
            "[00:53:56]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:53:56]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:53:56]   [Fold 1/10] train=166, val=185\n",
            "[00:54:00]     → Fold SMAPE: 3.828%\n",
            "[00:54:00]   [Fold 2/10] train=351, val=185\n",
            "[00:54:05]     → Fold SMAPE: 5.279%\n",
            "[00:54:05]   [Fold 3/10] train=536, val=185\n",
            "[00:54:11]     → Fold SMAPE: 5.180%\n",
            "[00:54:11]   [Fold 4/10] train=721, val=185\n",
            "[00:54:17]     → Fold SMAPE: 9.608%\n",
            "[00:54:17]   [Fold 5/10] train=906, val=185\n",
            "[00:54:23]     → Fold SMAPE: 6.432%\n",
            "[00:54:23]   [Fold 6/10] train=1091, val=185\n",
            "[00:54:30]     → Fold SMAPE: 7.188%\n",
            "[00:54:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:54:36]     → Fold SMAPE: 9.732%\n",
            "[00:54:36]   [Fold 8/10] train=1461, val=185\n",
            "[00:54:42]     → Fold SMAPE: 2.787%\n",
            "[00:54:42]   [Fold 9/10] train=1646, val=185\n",
            "[00:54:48]     → Fold SMAPE: 0.931%\n",
            "[00:54:48]   [Fold 10/10] train=1831, val=185\n",
            "[00:54:55]     → Fold SMAPE: 0.620%\n",
            "[00:54:59]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:54:59]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:54:59]   [Fold 1/10] train=166, val=185\n",
            "[00:55:04]     → Fold SMAPE: 3.163%\n",
            "[00:55:04]   [Fold 2/10] train=351, val=185\n",
            "[00:55:09]     → Fold SMAPE: 2.079%\n",
            "[00:55:09]   [Fold 3/10] train=536, val=185\n",
            "[00:55:14]     → Fold SMAPE: 1.260%\n",
            "[00:55:14]   [Fold 4/10] train=721, val=185\n",
            "[00:55:19]     → Fold SMAPE: 2.233%\n",
            "[00:55:19]   [Fold 5/10] train=906, val=185\n",
            "[00:55:25]     → Fold SMAPE: 2.530%\n",
            "[00:55:25]   [Fold 6/10] train=1091, val=185\n",
            "[00:55:30]     → Fold SMAPE: 5.173%\n",
            "[00:55:30]   [Fold 7/10] train=1276, val=185\n",
            "[00:55:35]     → Fold SMAPE: 4.604%\n",
            "[00:55:35]   [Fold 8/10] train=1461, val=185\n",
            "[00:55:41]     → Fold SMAPE: 2.177%\n",
            "[00:55:41]   [Fold 9/10] train=1646, val=185\n",
            "[00:55:47]     → Fold SMAPE: 1.208%\n",
            "[00:55:47]   [Fold 10/10] train=1831, val=185\n",
            "[00:55:52]     → Fold SMAPE: 1.014%\n",
            "[00:55:56]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:55:56]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:55:56]   [Fold 1/10] train=166, val=185\n",
            "[00:56:01]     → Fold SMAPE: 9.567%\n",
            "[00:56:01]   [Fold 2/10] train=351, val=185\n",
            "[00:56:06]     → Fold SMAPE: 5.091%\n",
            "[00:56:06]   [Fold 3/10] train=536, val=185\n",
            "[00:56:11]     → Fold SMAPE: 1.243%\n",
            "[00:56:11]   [Fold 4/10] train=721, val=185\n",
            "[00:56:16]     → Fold SMAPE: 1.753%\n",
            "[00:56:16]   [Fold 5/10] train=906, val=185\n",
            "[00:56:21]     → Fold SMAPE: 2.099%\n",
            "[00:56:21]   [Fold 6/10] train=1091, val=185\n",
            "[00:56:26]     → Fold SMAPE: 4.586%\n",
            "[00:56:26]   [Fold 7/10] train=1276, val=185\n",
            "[00:56:33]     → Fold SMAPE: 12.643%\n",
            "[00:56:33]   [Fold 8/10] train=1461, val=185\n",
            "[00:56:39]     → Fold SMAPE: 5.201%\n",
            "[00:56:39]   [Fold 9/10] train=1646, val=185\n",
            "[00:56:46]     → Fold SMAPE: 1.802%\n",
            "[00:56:46]   [Fold 10/10] train=1831, val=185\n",
            "[00:56:53]     → Fold SMAPE: 1.489%\n",
            "[00:56:58]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:56:59]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:56:59]   [Fold 1/10] train=166, val=185\n",
            "[00:57:03]     → Fold SMAPE: 5.415%\n",
            "[00:57:03]   [Fold 2/10] train=351, val=185\n",
            "[00:57:08]     → Fold SMAPE: 3.939%\n",
            "[00:57:08]   [Fold 3/10] train=536, val=185\n",
            "[00:57:13]     → Fold SMAPE: 2.016%\n",
            "[00:57:13]   [Fold 4/10] train=721, val=185\n",
            "[00:57:18]     → Fold SMAPE: 3.193%\n",
            "[00:57:18]   [Fold 5/10] train=906, val=185\n",
            "[00:57:23]     → Fold SMAPE: 0.922%\n",
            "[00:57:23]   [Fold 6/10] train=1091, val=185\n",
            "[00:57:28]     → Fold SMAPE: 2.524%\n",
            "[00:57:28]   [Fold 7/10] train=1276, val=185\n",
            "[00:57:34]     → Fold SMAPE: 4.923%\n",
            "[00:57:34]   [Fold 8/10] train=1461, val=185\n",
            "[00:57:39]     → Fold SMAPE: 2.415%\n",
            "[00:57:39]   [Fold 9/10] train=1646, val=185\n",
            "[00:57:45]     → Fold SMAPE: 0.745%\n",
            "[00:57:45]   [Fold 10/10] train=1831, val=185\n",
            "[00:57:50]     → Fold SMAPE: 2.664%\n",
            "[00:57:54]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:57:55]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:57:55]   [Fold 1/10] train=166, val=185\n",
            "[00:57:59]     → Fold SMAPE: 4.478%\n",
            "[00:57:59]   [Fold 2/10] train=351, val=185\n",
            "[00:58:04]     → Fold SMAPE: 3.444%\n",
            "[00:58:04]   [Fold 3/10] train=536, val=185\n",
            "[00:58:09]     → Fold SMAPE: 2.351%\n",
            "[00:58:09]   [Fold 4/10] train=721, val=185\n",
            "[00:58:14]     → Fold SMAPE: 10.568%\n",
            "[00:58:14]   [Fold 5/10] train=906, val=185\n",
            "[00:58:20]     → Fold SMAPE: 1.953%\n",
            "[00:58:20]   [Fold 6/10] train=1091, val=185\n",
            "[00:58:26]     → Fold SMAPE: 13.583%\n",
            "[00:58:26]   [Fold 7/10] train=1276, val=185\n",
            "[00:58:31]     → Fold SMAPE: 5.200%\n",
            "[00:58:31]   [Fold 8/10] train=1461, val=185\n",
            "[00:58:37]     → Fold SMAPE: 2.150%\n",
            "[00:58:37]   [Fold 9/10] train=1646, val=185\n",
            "[00:58:43]     → Fold SMAPE: 1.101%\n",
            "[00:58:43]   [Fold 10/10] train=1831, val=185\n",
            "[00:58:49]     → Fold SMAPE: 1.390%\n",
            "[00:58:53]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:58:53]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:58:53]   [Fold 1/10] train=166, val=185\n",
            "[00:58:57]     → Fold SMAPE: 2.919%\n",
            "[00:58:57]   [Fold 2/10] train=351, val=185\n",
            "[00:59:02]     → Fold SMAPE: 3.543%\n",
            "[00:59:02]   [Fold 3/10] train=536, val=185\n",
            "[00:59:07]     → Fold SMAPE: 2.237%\n",
            "[00:59:07]   [Fold 4/10] train=721, val=185\n",
            "[00:59:12]     → Fold SMAPE: 2.380%\n",
            "[00:59:12]   [Fold 5/10] train=906, val=185\n",
            "[00:59:17]     → Fold SMAPE: 0.488%\n",
            "[00:59:17]   [Fold 6/10] train=1091, val=185\n",
            "[00:59:22]     → Fold SMAPE: 0.611%\n",
            "[00:59:22]   [Fold 7/10] train=1276, val=185\n",
            "[00:59:26]     → Fold SMAPE: 2.595%\n",
            "[00:59:26]   [Fold 8/10] train=1461, val=185\n",
            "[00:59:32]     → Fold SMAPE: 2.006%\n",
            "[00:59:32]   [Fold 9/10] train=1646, val=185\n",
            "[00:59:36]     → Fold SMAPE: 5.575%\n",
            "[00:59:36]   [Fold 10/10] train=1831, val=185\n",
            "[00:59:41]     → Fold SMAPE: 0.698%\n",
            "[00:59:45]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[00:59:45]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[00:59:45]   [Fold 1/10] train=166, val=185\n",
            "[00:59:49]     → Fold SMAPE: 6.427%\n",
            "[00:59:49]   [Fold 2/10] train=351, val=185\n",
            "[00:59:54]     → Fold SMAPE: 2.091%\n",
            "[00:59:54]   [Fold 3/10] train=536, val=185\n",
            "[00:59:59]     → Fold SMAPE: 1.120%\n",
            "[00:59:59]   [Fold 4/10] train=721, val=185\n",
            "[01:00:04]     → Fold SMAPE: 1.793%\n",
            "[01:00:04]   [Fold 5/10] train=906, val=185\n",
            "[01:00:09]     → Fold SMAPE: 0.785%\n",
            "[01:00:09]   [Fold 6/10] train=1091, val=185\n",
            "[01:00:14]     → Fold SMAPE: 1.502%\n",
            "[01:00:14]   [Fold 7/10] train=1276, val=185\n",
            "[01:00:20]     → Fold SMAPE: 3.011%\n",
            "[01:00:20]   [Fold 8/10] train=1461, val=185\n",
            "[01:00:25]     → Fold SMAPE: 1.838%\n",
            "[01:00:25]   [Fold 9/10] train=1646, val=185\n",
            "[01:00:30]     → Fold SMAPE: 0.866%\n",
            "[01:00:30]   [Fold 10/10] train=1831, val=185\n",
            "[01:00:36]     → Fold SMAPE: 1.169%\n",
            "[01:00:39]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:00:39]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:00:39]   [Fold 1/10] train=166, val=185\n",
            "[01:00:44]     → Fold SMAPE: 4.721%\n",
            "[01:00:44]   [Fold 2/10] train=351, val=185\n",
            "[01:00:49]     → Fold SMAPE: 3.142%\n",
            "[01:00:49]   [Fold 3/10] train=536, val=185\n",
            "[01:00:54]     → Fold SMAPE: 2.904%\n",
            "[01:00:54]   [Fold 4/10] train=721, val=185\n",
            "[01:00:59]     → Fold SMAPE: 4.607%\n",
            "[01:00:59]   [Fold 5/10] train=906, val=185\n",
            "[01:01:04]     → Fold SMAPE: 2.218%\n",
            "[01:01:04]   [Fold 6/10] train=1091, val=185\n",
            "[01:01:09]     → Fold SMAPE: 3.820%\n",
            "[01:01:09]   [Fold 7/10] train=1276, val=185\n",
            "[01:01:14]     → Fold SMAPE: 0.900%\n",
            "[01:01:14]   [Fold 8/10] train=1461, val=185\n",
            "[01:01:19]     → Fold SMAPE: 0.999%\n",
            "[01:01:19]   [Fold 9/10] train=1646, val=185\n",
            "[01:01:24]     → Fold SMAPE: 0.547%\n",
            "[01:01:24]   [Fold 10/10] train=1831, val=185\n",
            "[01:01:29]     → Fold SMAPE: 0.801%\n",
            "[01:01:32]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:01:32]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:01:32]   [Fold 1/10] train=166, val=185\n",
            "[01:01:37]     → Fold SMAPE: 5.357%\n",
            "[01:01:37]   [Fold 2/10] train=351, val=185\n",
            "[01:01:41]     → Fold SMAPE: 1.611%\n",
            "[01:01:41]   [Fold 3/10] train=536, val=185\n",
            "[01:01:47]     → Fold SMAPE: 1.647%\n",
            "[01:01:47]   [Fold 4/10] train=721, val=185\n",
            "[01:01:51]     → Fold SMAPE: 1.847%\n",
            "[01:01:51]   [Fold 5/10] train=906, val=185\n",
            "[01:01:57]     → Fold SMAPE: 0.907%\n",
            "[01:01:57]   [Fold 6/10] train=1091, val=185\n",
            "[01:02:02]     → Fold SMAPE: 1.631%\n",
            "[01:02:02]   [Fold 7/10] train=1276, val=185\n",
            "[01:02:07]     → Fold SMAPE: 4.072%\n",
            "[01:02:07]   [Fold 8/10] train=1461, val=185\n",
            "[01:02:12]     → Fold SMAPE: 2.296%\n",
            "[01:02:12]   [Fold 9/10] train=1646, val=185\n",
            "[01:02:17]     → Fold SMAPE: 0.891%\n",
            "[01:02:17]   [Fold 10/10] train=1831, val=185\n",
            "[01:02:23]     → Fold SMAPE: 0.743%\n",
            "[01:02:26]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:02:26]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:02:26]   [Fold 1/10] train=166, val=185\n",
            "[01:02:31]     → Fold SMAPE: 4.880%\n",
            "[01:02:31]   [Fold 2/10] train=351, val=185\n",
            "[01:02:36]     → Fold SMAPE: 2.604%\n",
            "[01:02:36]   [Fold 3/10] train=536, val=185\n",
            "[01:02:41]     → Fold SMAPE: 2.074%\n",
            "[01:02:41]   [Fold 4/10] train=721, val=185\n",
            "[01:02:46]     → Fold SMAPE: 1.636%\n",
            "[01:02:46]   [Fold 5/10] train=906, val=185\n",
            "[01:02:51]     → Fold SMAPE: 1.244%\n",
            "[01:02:51]   [Fold 6/10] train=1091, val=185\n",
            "[01:02:57]     → Fold SMAPE: 1.624%\n",
            "[01:02:57]   [Fold 7/10] train=1276, val=185\n",
            "[01:03:02]     → Fold SMAPE: 1.168%\n",
            "[01:03:02]   [Fold 8/10] train=1461, val=185\n",
            "[01:03:07]     → Fold SMAPE: 1.031%\n",
            "[01:03:07]   [Fold 9/10] train=1646, val=185\n",
            "[01:03:13]     → Fold SMAPE: 0.989%\n",
            "[01:03:13]   [Fold 10/10] train=1831, val=185\n",
            "[01:03:18]     → Fold SMAPE: 0.961%\n",
            "[01:03:22]   [Done] OOF SMAPE=199.995%  scale=25599819269669203968.0000\n",
            "[01:03:22]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:03:22]   [Fold 1/10] train=166, val=185\n",
            "[01:03:27]     → Fold SMAPE: 7.248%\n",
            "[01:03:27]   [Fold 2/10] train=351, val=185\n",
            "[01:03:32]     → Fold SMAPE: 3.413%\n",
            "[01:03:32]   [Fold 3/10] train=536, val=185\n",
            "[01:03:36]     → Fold SMAPE: 1.794%\n",
            "[01:03:36]   [Fold 4/10] train=721, val=185\n",
            "[01:03:41]     → Fold SMAPE: 2.455%\n",
            "[01:03:41]   [Fold 5/10] train=906, val=185\n",
            "[01:03:46]     → Fold SMAPE: 1.742%\n",
            "[01:03:46]   [Fold 6/10] train=1091, val=185\n",
            "[01:03:52]     → Fold SMAPE: 2.047%\n",
            "[01:03:52]   [Fold 7/10] train=1276, val=185\n",
            "[01:03:57]     → Fold SMAPE: 2.775%\n",
            "[01:03:57]   [Fold 8/10] train=1461, val=185\n",
            "[01:04:02]     → Fold SMAPE: 2.023%\n",
            "[01:04:02]   [Fold 9/10] train=1646, val=185\n",
            "[01:04:08]     → Fold SMAPE: 1.449%\n",
            "[01:04:08]   [Fold 10/10] train=1831, val=185\n",
            "[01:04:13]     → Fold SMAPE: 1.851%\n",
            "[01:04:17]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:04:17]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:04:17]   [Fold 1/10] train=166, val=185\n",
            "[01:04:21]     → Fold SMAPE: 7.499%\n",
            "[01:04:21]   [Fold 2/10] train=351, val=185\n",
            "[01:04:26]     → Fold SMAPE: 1.884%\n",
            "[01:04:26]   [Fold 3/10] train=536, val=185\n",
            "[01:04:31]     → Fold SMAPE: 3.876%\n",
            "[01:04:31]   [Fold 4/10] train=721, val=185\n",
            "[01:04:36]     → Fold SMAPE: 1.242%\n",
            "[01:04:36]   [Fold 5/10] train=906, val=185\n",
            "[01:04:41]     → Fold SMAPE: 1.116%\n",
            "[01:04:41]   [Fold 6/10] train=1091, val=185\n",
            "[01:04:47]     → Fold SMAPE: 1.134%\n",
            "[01:04:47]   [Fold 7/10] train=1276, val=185\n",
            "[01:04:52]     → Fold SMAPE: 1.917%\n",
            "[01:04:52]   [Fold 8/10] train=1461, val=185\n",
            "[01:04:57]     → Fold SMAPE: 2.070%\n",
            "[01:04:57]   [Fold 9/10] train=1646, val=185\n",
            "[01:05:03]     → Fold SMAPE: 1.425%\n",
            "[01:05:03]   [Fold 10/10] train=1831, val=185\n",
            "[01:05:08]     → Fold SMAPE: 1.517%\n",
            "[01:05:12]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:05:12]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:05:12]   [Fold 1/10] train=166, val=185\n",
            "[01:05:16]     → Fold SMAPE: 9.967%\n",
            "[01:05:16]   [Fold 2/10] train=351, val=185\n",
            "[01:05:21]     → Fold SMAPE: 3.534%\n",
            "[01:05:21]   [Fold 3/10] train=536, val=185\n",
            "[01:05:26]     → Fold SMAPE: 2.445%\n",
            "[01:05:26]   [Fold 4/10] train=721, val=185\n",
            "[01:05:31]     → Fold SMAPE: 2.095%\n",
            "[01:05:31]   [Fold 5/10] train=906, val=185\n",
            "[01:05:37]     → Fold SMAPE: 1.596%\n",
            "[01:05:37]   [Fold 6/10] train=1091, val=185\n",
            "[01:05:42]     → Fold SMAPE: 1.208%\n",
            "[01:05:42]   [Fold 7/10] train=1276, val=185\n",
            "[01:05:47]     → Fold SMAPE: 2.725%\n",
            "[01:05:47]   [Fold 8/10] train=1461, val=185\n",
            "[01:05:53]     → Fold SMAPE: 2.984%\n",
            "[01:05:53]   [Fold 9/10] train=1646, val=185\n",
            "[01:05:58]     → Fold SMAPE: 1.058%\n",
            "[01:05:58]   [Fold 10/10] train=1831, val=185\n",
            "[01:06:04]     → Fold SMAPE: 2.878%\n",
            "[01:06:09]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:06:09]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:06:09]   [Fold 1/10] train=166, val=185\n",
            "[01:06:13]     → Fold SMAPE: 6.423%\n",
            "[01:06:13]   [Fold 2/10] train=351, val=185\n",
            "[01:06:18]     → Fold SMAPE: 1.792%\n",
            "[01:06:18]   [Fold 3/10] train=536, val=185\n",
            "[01:06:23]     → Fold SMAPE: 1.550%\n",
            "[01:06:23]   [Fold 4/10] train=721, val=185\n",
            "[01:06:28]     → Fold SMAPE: 4.299%\n",
            "[01:06:28]   [Fold 5/10] train=906, val=185\n",
            "[01:06:33]     → Fold SMAPE: 0.708%\n",
            "[01:06:33]   [Fold 6/10] train=1091, val=185\n",
            "[01:06:38]     → Fold SMAPE: 2.284%\n",
            "[01:06:38]   [Fold 7/10] train=1276, val=185\n",
            "[01:06:43]     → Fold SMAPE: 1.615%\n",
            "[01:06:43]   [Fold 8/10] train=1461, val=185\n",
            "[01:06:48]     → Fold SMAPE: 1.454%\n",
            "[01:06:48]   [Fold 9/10] train=1646, val=185\n",
            "[01:06:53]     → Fold SMAPE: 0.891%\n",
            "[01:06:53]   [Fold 10/10] train=1831, val=185\n",
            "[01:06:58]     → Fold SMAPE: 0.711%\n",
            "[01:07:01]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:07:01]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:07:01]   [Fold 1/10] train=166, val=185\n",
            "[01:07:06]     → Fold SMAPE: 5.271%\n",
            "[01:07:06]   [Fold 2/10] train=351, val=185\n",
            "[01:07:11]     → Fold SMAPE: 1.373%\n",
            "[01:07:11]   [Fold 3/10] train=536, val=185\n",
            "[01:07:16]     → Fold SMAPE: 2.689%\n",
            "[01:07:16]   [Fold 4/10] train=721, val=185\n",
            "[01:07:21]     → Fold SMAPE: 5.089%\n",
            "[01:07:21]   [Fold 5/10] train=906, val=185\n",
            "[01:07:26]     → Fold SMAPE: 1.395%\n",
            "[01:07:26]   [Fold 6/10] train=1091, val=185\n",
            "[01:07:32]     → Fold SMAPE: 1.171%\n",
            "[01:07:32]   [Fold 7/10] train=1276, val=185\n",
            "[01:07:37]     → Fold SMAPE: 0.982%\n",
            "[01:07:37]   [Fold 8/10] train=1461, val=185\n",
            "[01:07:42]     → Fold SMAPE: 0.809%\n",
            "[01:07:42]   [Fold 9/10] train=1646, val=185\n",
            "[01:07:47]     → Fold SMAPE: 1.141%\n",
            "[01:07:47]   [Fold 10/10] train=1831, val=185\n",
            "[01:07:53]     → Fold SMAPE: 0.912%\n",
            "[01:07:56]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:07:56]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:07:56]   [Fold 1/10] train=166, val=185\n",
            "[01:08:01]     → Fold SMAPE: 6.680%\n",
            "[01:08:01]   [Fold 2/10] train=351, val=185\n",
            "[01:08:06]     → Fold SMAPE: 2.821%\n",
            "[01:08:06]   [Fold 3/10] train=536, val=185\n",
            "[01:08:11]     → Fold SMAPE: 2.084%\n",
            "[01:08:11]   [Fold 4/10] train=721, val=185\n",
            "[01:08:16]     → Fold SMAPE: 5.756%\n",
            "[01:08:16]   [Fold 5/10] train=906, val=185\n",
            "[01:08:21]     → Fold SMAPE: 0.878%\n",
            "[01:08:21]   [Fold 6/10] train=1091, val=185\n",
            "[01:08:26]     → Fold SMAPE: 2.930%\n",
            "[01:08:26]   [Fold 7/10] train=1276, val=185\n",
            "[01:08:31]     → Fold SMAPE: 3.197%\n",
            "[01:08:31]   [Fold 8/10] train=1461, val=185\n",
            "[01:08:36]     → Fold SMAPE: 3.408%\n",
            "[01:08:36]   [Fold 9/10] train=1646, val=185\n",
            "[01:08:41]     → Fold SMAPE: 0.810%\n",
            "[01:08:41]   [Fold 10/10] train=1831, val=185\n",
            "[01:08:47]     → Fold SMAPE: 0.789%\n",
            "[01:08:50]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:08:50]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:08:50]   [Fold 1/10] train=166, val=185\n",
            "[01:08:55]     → Fold SMAPE: 7.546%\n",
            "[01:08:55]   [Fold 2/10] train=351, val=185\n",
            "[01:09:00]     → Fold SMAPE: 3.389%\n",
            "[01:09:00]   [Fold 3/10] train=536, val=185\n",
            "[01:09:04]     → Fold SMAPE: 1.390%\n",
            "[01:09:04]   [Fold 4/10] train=721, val=185\n",
            "[01:09:09]     → Fold SMAPE: 1.233%\n",
            "[01:09:09]   [Fold 5/10] train=906, val=185\n",
            "[01:09:14]     → Fold SMAPE: 2.673%\n",
            "[01:09:14]   [Fold 6/10] train=1091, val=185\n",
            "[01:09:19]     → Fold SMAPE: 5.978%\n",
            "[01:09:19]   [Fold 7/10] train=1276, val=185\n",
            "[01:09:27]     → Fold SMAPE: 11.295%\n",
            "[01:09:27]   [Fold 8/10] train=1461, val=185\n",
            "[01:09:33]     → Fold SMAPE: 8.337%\n",
            "[01:09:33]   [Fold 9/10] train=1646, val=185\n",
            "[01:09:41]     → Fold SMAPE: 2.187%\n",
            "[01:09:41]   [Fold 10/10] train=1831, val=185\n",
            "[01:09:48]     → Fold SMAPE: 1.630%\n",
            "[01:09:53]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:09:53]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:09:53]   [Fold 1/10] train=166, val=185\n",
            "[01:09:58]     → Fold SMAPE: 4.424%\n",
            "[01:09:58]   [Fold 2/10] train=351, val=185\n",
            "[01:10:03]     → Fold SMAPE: 4.060%\n",
            "[01:10:03]   [Fold 3/10] train=536, val=185\n",
            "[01:10:07]     → Fold SMAPE: 1.995%\n",
            "[01:10:07]   [Fold 4/10] train=721, val=185\n",
            "[01:10:12]     → Fold SMAPE: 4.207%\n",
            "[01:10:12]   [Fold 5/10] train=906, val=185\n",
            "[01:10:17]     → Fold SMAPE: 2.249%\n",
            "[01:10:17]   [Fold 6/10] train=1091, val=185\n",
            "[01:10:22]     → Fold SMAPE: 4.098%\n",
            "[01:10:22]   [Fold 7/10] train=1276, val=185\n",
            "[01:10:28]     → Fold SMAPE: 3.638%\n",
            "[01:10:28]   [Fold 8/10] train=1461, val=185\n",
            "[01:10:33]     → Fold SMAPE: 3.034%\n",
            "[01:10:33]   [Fold 9/10] train=1646, val=185\n",
            "[01:10:38]     → Fold SMAPE: 1.447%\n",
            "[01:10:38]   [Fold 10/10] train=1831, val=185\n",
            "[01:10:43]     → Fold SMAPE: 1.332%\n",
            "[01:10:47]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:10:47]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:10:47]   [Fold 1/10] train=166, val=185\n",
            "[01:10:52]     → Fold SMAPE: 3.410%\n",
            "[01:10:52]   [Fold 2/10] train=351, val=185\n",
            "[01:10:56]     → Fold SMAPE: 1.489%\n",
            "[01:10:56]   [Fold 3/10] train=536, val=185\n",
            "[01:11:02]     → Fold SMAPE: 0.866%\n",
            "[01:11:02]   [Fold 4/10] train=721, val=185\n",
            "[01:11:06]     → Fold SMAPE: 1.158%\n",
            "[01:11:06]   [Fold 5/10] train=906, val=185\n",
            "[01:11:12]     → Fold SMAPE: 1.906%\n",
            "[01:11:12]   [Fold 6/10] train=1091, val=185\n",
            "[01:11:17]     → Fold SMAPE: 2.039%\n",
            "[01:11:17]   [Fold 7/10] train=1276, val=185\n",
            "[01:11:22]     → Fold SMAPE: 13.516%\n",
            "[01:11:22]   [Fold 8/10] train=1461, val=185\n",
            "[01:11:28]     → Fold SMAPE: 6.743%\n",
            "[01:11:28]   [Fold 9/10] train=1646, val=185\n",
            "[01:11:34]     → Fold SMAPE: 6.400%\n",
            "[01:11:34]   [Fold 10/10] train=1831, val=185\n",
            "[01:11:40]     → Fold SMAPE: 4.169%\n",
            "[01:11:45]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:11:45]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:11:45]   [Fold 1/10] train=166, val=185\n",
            "[01:11:49]     → Fold SMAPE: 7.246%\n",
            "[01:11:49]   [Fold 2/10] train=351, val=185\n",
            "[01:11:54]     → Fold SMAPE: 4.443%\n",
            "[01:11:54]   [Fold 3/10] train=536, val=185\n",
            "[01:12:00]     → Fold SMAPE: 1.672%\n",
            "[01:12:00]   [Fold 4/10] train=721, val=185\n",
            "[01:12:05]     → Fold SMAPE: 5.304%\n",
            "[01:12:05]   [Fold 5/10] train=906, val=185\n",
            "[01:12:10]     → Fold SMAPE: 2.599%\n",
            "[01:12:10]   [Fold 6/10] train=1091, val=185\n",
            "[01:12:16]     → Fold SMAPE: 2.459%\n",
            "[01:12:16]   [Fold 7/10] train=1276, val=185\n",
            "[01:12:21]     → Fold SMAPE: 2.297%\n",
            "[01:12:21]   [Fold 8/10] train=1461, val=185\n",
            "[01:12:27]     → Fold SMAPE: 4.885%\n",
            "[01:12:27]   [Fold 9/10] train=1646, val=185\n",
            "[01:12:32]     → Fold SMAPE: 0.936%\n",
            "[01:12:32]   [Fold 10/10] train=1831, val=185\n",
            "[01:12:38]     → Fold SMAPE: 1.279%\n",
            "[01:12:42]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:12:42]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:12:42]   [Fold 1/10] train=166, val=185\n",
            "[01:12:47]     → Fold SMAPE: 4.391%\n",
            "[01:12:47]   [Fold 2/10] train=351, val=185\n",
            "[01:12:52]     → Fold SMAPE: 4.217%\n",
            "[01:12:52]   [Fold 3/10] train=536, val=185\n",
            "[01:12:57]     → Fold SMAPE: 1.702%\n",
            "[01:12:57]   [Fold 4/10] train=721, val=185\n",
            "[01:13:02]     → Fold SMAPE: 2.202%\n",
            "[01:13:02]   [Fold 5/10] train=906, val=185\n",
            "[01:13:07]     → Fold SMAPE: 2.415%\n",
            "[01:13:07]   [Fold 6/10] train=1091, val=185\n",
            "[01:13:12]     → Fold SMAPE: 2.520%\n",
            "[01:13:12]   [Fold 7/10] train=1276, val=185\n",
            "[01:13:18]     → Fold SMAPE: 2.682%\n",
            "[01:13:18]   [Fold 8/10] train=1461, val=185\n",
            "[01:13:23]     → Fold SMAPE: 2.825%\n",
            "[01:13:23]   [Fold 9/10] train=1646, val=185\n",
            "[01:13:29]     → Fold SMAPE: 1.498%\n",
            "[01:13:29]   [Fold 10/10] train=1831, val=185\n",
            "[01:13:34]     → Fold SMAPE: 1.330%\n",
            "[01:13:38]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:13:38]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:13:38]   [Fold 1/10] train=166, val=185\n",
            "[01:13:43]     → Fold SMAPE: 5.731%\n",
            "[01:13:43]   [Fold 2/10] train=351, val=185\n",
            "[01:13:48]     → Fold SMAPE: 2.397%\n",
            "[01:13:48]   [Fold 3/10] train=536, val=185\n",
            "[01:13:53]     → Fold SMAPE: 2.164%\n",
            "[01:13:53]   [Fold 4/10] train=721, val=185\n",
            "[01:13:58]     → Fold SMAPE: 3.108%\n",
            "[01:13:58]   [Fold 5/10] train=906, val=185\n",
            "[01:14:03]     → Fold SMAPE: 1.701%\n",
            "[01:14:03]   [Fold 6/10] train=1091, val=185\n",
            "[01:14:08]     → Fold SMAPE: 3.909%\n",
            "[01:14:08]   [Fold 7/10] train=1276, val=185\n",
            "[01:14:15]     → Fold SMAPE: 15.063%\n",
            "[01:14:15]   [Fold 8/10] train=1461, val=185\n",
            "[01:14:22]     → Fold SMAPE: 2.999%\n",
            "[01:14:22]   [Fold 9/10] train=1646, val=185\n",
            "[01:14:28]     → Fold SMAPE: 1.758%\n",
            "[01:14:28]   [Fold 10/10] train=1831, val=185\n",
            "[01:14:35]     → Fold SMAPE: 2.264%\n",
            "[01:14:40]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:14:40]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:14:40]   [Fold 1/10] train=166, val=185\n",
            "[01:14:45]     → Fold SMAPE: 5.268%\n",
            "[01:14:45]   [Fold 2/10] train=351, val=185\n",
            "[01:14:52]     → Fold SMAPE: 15.034%\n",
            "[01:14:52]   [Fold 3/10] train=536, val=185\n",
            "[01:14:58]     → Fold SMAPE: 7.791%\n",
            "[01:14:58]   [Fold 4/10] train=721, val=185\n",
            "[01:15:05]     → Fold SMAPE: 7.345%\n",
            "[01:15:05]   [Fold 5/10] train=906, val=185\n",
            "[01:15:11]     → Fold SMAPE: 4.827%\n",
            "[01:15:11]   [Fold 6/10] train=1091, val=185\n",
            "[01:15:18]     → Fold SMAPE: 2.386%\n",
            "[01:15:18]   [Fold 7/10] train=1276, val=185\n",
            "[01:15:25]     → Fold SMAPE: 3.263%\n",
            "[01:15:25]   [Fold 8/10] train=1461, val=185\n",
            "[01:15:31]     → Fold SMAPE: 5.554%\n",
            "[01:15:31]   [Fold 9/10] train=1646, val=185\n",
            "[01:15:38]     → Fold SMAPE: 2.337%\n",
            "[01:15:38]   [Fold 10/10] train=1831, val=185\n",
            "[01:15:45]     → Fold SMAPE: 1.420%\n",
            "[01:15:50]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:15:50]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:15:50]   [Fold 1/10] train=166, val=185\n",
            "[01:15:55]     → Fold SMAPE: 4.390%\n",
            "[01:15:55]   [Fold 2/10] train=351, val=185\n",
            "[01:16:00]     → Fold SMAPE: 3.248%\n",
            "[01:16:00]   [Fold 3/10] train=536, val=185\n",
            "[01:16:05]     → Fold SMAPE: 2.948%\n",
            "[01:16:05]   [Fold 4/10] train=721, val=185\n",
            "[01:16:10]     → Fold SMAPE: 4.318%\n",
            "[01:16:10]   [Fold 5/10] train=906, val=185\n",
            "[01:16:15]     → Fold SMAPE: 2.315%\n",
            "[01:16:15]   [Fold 6/10] train=1091, val=185\n",
            "[01:16:20]     → Fold SMAPE: 1.417%\n",
            "[01:16:20]   [Fold 7/10] train=1276, val=185\n",
            "[01:16:25]     → Fold SMAPE: 1.148%\n",
            "[01:16:25]   [Fold 8/10] train=1461, val=185\n",
            "[01:16:30]     → Fold SMAPE: 2.064%\n",
            "[01:16:30]   [Fold 9/10] train=1646, val=185\n",
            "[01:16:35]     → Fold SMAPE: 0.718%\n",
            "[01:16:35]   [Fold 10/10] train=1831, val=185\n",
            "[01:16:41]     → Fold SMAPE: 0.766%\n",
            "[01:16:44]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[01:16:44]   [Info] samples=2040, features=35, test_horizon=168\n",
            "[01:16:44]   [Fold 1/10] train=166, val=185\n",
            "[01:16:49]     → Fold SMAPE: 3.914%\n",
            "[01:16:49]   [Fold 2/10] train=351, val=185\n",
            "[01:16:54]     → Fold SMAPE: 2.376%\n",
            "[01:16:54]   [Fold 3/10] train=536, val=185\n",
            "[01:16:59]     → Fold SMAPE: 3.127%\n",
            "[01:16:59]   [Fold 4/10] train=721, val=185\n",
            "[01:17:04]     → Fold SMAPE: 1.666%\n",
            "[01:17:04]   [Fold 5/10] train=906, val=185\n",
            "[01:17:09]     → Fold SMAPE: 2.178%\n",
            "[01:17:09]   [Fold 6/10] train=1091, val=185\n",
            "[01:17:14]     → Fold SMAPE: 3.090%\n",
            "[01:17:14]   [Fold 7/10] train=1276, val=185\n",
            "[01:17:20]     → Fold SMAPE: 5.349%\n",
            "[01:17:20]   [Fold 8/10] train=1461, val=185\n",
            "[01:17:26]     → Fold SMAPE: 1.633%\n",
            "[01:17:26]   [Fold 9/10] train=1646, val=185\n",
            "[01:17:31]     → Fold SMAPE: 2.567%\n",
            "[01:17:31]   [Fold 10/10] train=1831, val=185\n",
            "[01:17:38]     → Fold SMAPE: 1.196%\n",
            "[01:17:42]   [Done] OOF SMAPE=nan%  scale=inf\n",
            "[Global] OOF SMAPE: nan%\n",
            "Saved submission → /content/drive/MyDrive/KUBIG/25_summer_contest/submission_gpu_xgb_smape_tuned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# --- 저장 경로 자동 생성 ---\n",
        "SAVE_DIR = '/content/drive/MyDrive/KUBIG/25_summer_contest'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "SUBMISSION_OUT = os.path.join(SAVE_DIR, f\"submission_gpu_xgb_{timestamp}.csv\")\n",
        "\n",
        "# 최종 저장\n",
        "sub_final.to_csv(SUBMISSION_OUT, index=False)\n",
        "print(f\"Saved submission → {SUBMISSION_OUT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "-X13FhrBh3Ui",
        "outputId": "7c888c23-faf1-459a-9116-314be14f0984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sub_final' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1089363718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 최종 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msub_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMISSION_OUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved submission → {SUBMISSION_OUT}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sub_final' is not defined"
          ]
        }
      ]
    }
  ]
}