{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84827120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
      "--- train.csv ë°ì´í„° ìƒ˜í”Œ ---\n",
      "            ID   age gender  tenure  frequent  payment_interval  \\\n",
      "0  TRAIN_00000  54.0      F    47.0      22.0               8.0   \n",
      "1  TRAIN_00001  30.0      M    16.0      15.0               5.0   \n",
      "2  TRAIN_00002  29.0      M     8.0      30.0              21.0   \n",
      "3  TRAIN_00003  38.0      F    38.0      23.0              10.0   \n",
      "4  TRAIN_00004  25.0      F    52.0       3.0              17.0   \n",
      "\n",
      "  subscription_type  contract_length  after_interaction  support_needs  \n",
      "0            member               90               25.0              0  \n",
      "1               vip              360               23.0              0  \n",
      "2              plus               30               21.0              0  \n",
      "3               vip               90                6.0              0  \n",
      "4            member               30                1.0              2  \n",
      "âœ… íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
      "--- test.csv ë°ì´í„° ìƒ˜í”Œ ---\n",
      "           ID   age gender  tenure  frequent  payment_interval  \\\n",
      "0  TEST_00000  18.0      M    40.0       6.0              15.0   \n",
      "1  TEST_00001  40.0      M    41.0      23.0               0.0   \n",
      "2  TEST_00002  59.0      F    30.0       1.0              21.0   \n",
      "3  TEST_00003  38.0      M     2.0      10.0               0.0   \n",
      "4  TEST_00004  30.0      M    28.0      21.0              20.0   \n",
      "\n",
      "  subscription_type  contract_length  after_interaction  \n",
      "0            member               30               18.0  \n",
      "1            member               90               16.0  \n",
      "2            member              360               25.0  \n",
      "3            member               30               18.0  \n",
      "4            member              360               28.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\eunse\\OneDrive\\ë°”íƒ• í™”ë©´\\KUBIG 2025\\í”„ë¡œì íŠ¸\\open\\train.csv\"\n",
    "\n",
    "try:\n",
    "    # ì§€ì •ëœ ê²½ë¡œì˜ train.csv íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    train_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ìƒìœ„ 5ê°œ í–‰ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    print(\"âœ… íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"--- train.csv ë°ì´í„° ìƒ˜í”Œ ---\")\n",
    "    print(train_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: í•´ë‹¹ ê²½ë¡œì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\eunse\\OneDrive\\ë°”íƒ• í™”ë©´\\KUBIG 2025\\í”„ë¡œì íŠ¸\\open\\test.csv\"\n",
    "\n",
    "try:\n",
    "    # ì§€ì •ëœ ê²½ë¡œì˜ test.csv íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    test_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ìƒìœ„ 5ê°œ í–‰ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    print(\"âœ… íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"--- test.csv ë°ì´í„° ìƒ˜í”Œ ---\")\n",
    "    print(test_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: í•´ë‹¹ ê²½ë¡œì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d467e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score# 0) ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì • (optional)\n",
    "# -----------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# ================== âœ… 1. ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ ì •ì˜ ==================\n",
    "def feature_engineering_advanced(df, is_train=True, agg_maps=None):\n",
    "    \"\"\"ê·¸ë£¹ë³„ í†µê³„ í”¼ì²˜ë¥¼ ìƒì„±í•˜ëŠ” ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # ê·¸ë£¹í™”í•  ë²”ì£¼í˜• ì»¬ëŸ¼ê³¼ í†µê³„ë¥¼ ë‚¼ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì •ì˜\n",
    "    group_cols = [\"gender\", \"subscription_type\", \"contract_length\"]\n",
    "    agg_cols = [\"age\", \"tenure\", \"frequent\", \"after_interaction\"]\n",
    "    \n",
    "    if is_train:\n",
    "        # í•™ìŠµ ë°ì´í„°ì¼ ê²½ìš°, ê·¸ë£¹ë³„ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•˜ê³  ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "        agg_maps = {}\n",
    "        for group_col in group_cols:\n",
    "            agg_map = df_copy.groupby(group_col)[agg_cols].agg(['mean', 'std']).reset_index()\n",
    "            # ë©€í‹°ë ˆë²¨ ì»¬ëŸ¼ì„ ë‹¨ì¼ ë ˆë²¨ë¡œ ë§Œë“¦ (ì˜ˆ: ('age', 'mean') -> 'age_mean')\n",
    "            agg_map.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in agg_map.columns]\n",
    "            agg_maps[group_col] = agg_map\n",
    "    \n",
    "    # ì €ì¥ëœ í†µê³„ ë§µì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ì¶”ê°€\n",
    "    for group_col, agg_map in agg_maps.items():\n",
    "        df_copy = pd.merge(df_copy, agg_map, on=group_col, how=\"left\")\n",
    "        for col in agg_cols:\n",
    "            # ê·¸ë£¹ í‰ê·  ëŒ€ë¹„ ê°œì¸ ìˆ˜ì¹˜ì˜ ì°¨ì´/ì •ê·œí™” í”¼ì²˜ ìƒì„±\n",
    "            df_copy[f'{col}_diff_from_{group_col}_mean'] = df_copy[col] - df_copy[f'{col}_mean']\n",
    "            df_copy[f'{col}_norm_by_{group_col}'] = (df_copy[col] - df_copy[f'{col}_mean']) / (df_copy[f'{col}_std'] + epsilon)\n",
    "            # ì›ë³¸ í†µê³„ í”¼ì²˜ëŠ” ì‚­ì œ\n",
    "            df_copy = df_copy.drop(columns=[f'{col}_mean', f'{col}_std'])\n",
    "            \n",
    "    if is_train:\n",
    "        return df_copy, agg_maps\n",
    "    else:\n",
    "        # ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ëŠ” ê·¸ë£¹ìœ¼ë¡œ ì¸í•´ NaNì´ ìƒê¸¸ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€\n",
    "        df_copy = df_copy.fillna(0)\n",
    "        return df_copy\n",
    "# =========================================================================\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TARGET = \"support_needs\"\n",
    "# (NUM_COLSëŠ” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í›„ ë‹¤ì‹œ ì •ì˜ë  ì˜ˆì •)\n",
    "CAT_COLS = [\"gender\",\"subscription_type\"]\n",
    "\n",
    "\n",
    "# 2) ë°ì´í„° ìœ ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ê°€ì¥ ë¨¼ì € ë¶„ë¦¬\n",
    "# -----------------\n",
    "tr_df, va_df = train_df.pipe(lambda d: train_test_split(d, test_size=0.2, stratify=d[TARGET], random_state=42))\n",
    "\n",
    "\n",
    "# ================== âœ… 2. ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš© ==================\n",
    "# í•™ìŠµ ë°ì´í„°ë¡œ ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰ ë° í†µê³„ ë§µ ì €ì¥\n",
    "tr_df_featured, agg_maps = feature_engineering_advanced(tr_df, is_train=True)\n",
    "\n",
    "# ì €ì¥ëœ í†µê³„ ë§µì„ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ë°ì´í„° ë³€í™˜\n",
    "va_df_featured = feature_engineering_advanced(va_df, is_train=False, agg_maps=agg_maps)\n",
    "\n",
    "# ì „ì²˜ë¦¬ ëŒ€ìƒì´ ë  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ëª©ë¡ì„ ìƒˆë¡œ ìƒì„±ëœ í”¼ì²˜ë¥¼ í¬í•¨í•˜ë„ë¡ ì—…ë°ì´íŠ¸\n",
    "original_num_cols = [\"age\",\"tenure\",\"frequent\",\"payment_interval\",\"contract_length\",\"after_interaction\"]\n",
    "new_num_cols = [col for col in tr_df_featured.columns if 'diff' in col or 'norm' in col]\n",
    "NUM_COLS = original_num_cols + new_num_cols\n",
    "\n",
    "# ì´í›„ ì½”ë“œì—ì„œ ì‚¬ìš©í•  ë°ì´í„°í”„ë ˆì„ì„ í”¼ì²˜ê°€ ì¶”ê°€ëœ ë²„ì „ìœ¼ë¡œ êµì²´\n",
    "tr_df = tr_df_featured\n",
    "va_df = va_df_featured\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# 3) ì „ì²˜ë¦¬ê¸° í´ë˜ìŠ¤ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cat_cols_fit = None\n",
    "\n",
    "    def fit_transform_train(self, df):\n",
    "        Xnum = self.scaler.fit_transform(df[NUM_COLS])\n",
    "        Xcat = pd.get_dummies(df[CAT_COLS], drop_first=False)\n",
    "        self.cat_cols_fit = Xcat.columns.tolist()\n",
    "        X = np.hstack([Xnum, Xcat.values])\n",
    "        y = df[TARGET].values\n",
    "        return X, y\n",
    "\n",
    "    def transform_val_or_test(self, df, has_target=True):\n",
    "        Xnum = self.scaler.transform(df[NUM_COLS])\n",
    "        Xcat = pd.get_dummies(df[CAT_COLS], drop_first=False)\n",
    "        Xcat = Xcat.reindex(columns=self.cat_cols_fit, fill_value=0)\n",
    "        X = np.hstack([Xnum, Xcat.values])\n",
    "        y = df[TARGET].values if has_target else None\n",
    "        return X, y\n",
    "\n",
    "prep = Preprocessor()\n",
    "X_tr, y_tr = prep.fit_transform_train(tr_df)\n",
    "X_va, y_va = prep.transform_val_or_test(va_df, has_target=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14955778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "Epoch 001 | TrainLoss 0.5072 | TrainAcc 0.4181 | ValLoss 0.4656 | ValAcc 0.4576 | MacroF1 0.4404 | LR 2.93e-04\n",
      "Epoch 002 | TrainLoss 0.4785 | TrainAcc 0.4385 | ValLoss 0.4668 | ValAcc 0.4606 | MacroF1 0.4404 | LR 2.71e-04\n",
      "Epoch 003 | TrainLoss 0.4718 | TrainAcc 0.4424 | ValLoss 0.4631 | ValAcc 0.4718 | MacroF1 0.4457 | LR 2.38e-04\n",
      "Epoch 004 | TrainLoss 0.4662 | TrainAcc 0.4567 | ValLoss 0.4636 | ValAcc 0.4614 | MacroF1 0.4503 | LR 1.97e-04\n",
      "Epoch 005 | TrainLoss 0.4622 | TrainAcc 0.4606 | ValLoss 0.4579 | ValAcc 0.4689 | MacroF1 0.4591 | LR 1.50e-04\n",
      "Epoch 006 | TrainLoss 0.4574 | TrainAcc 0.4665 | ValLoss 0.4581 | ValAcc 0.4770 | MacroF1 0.4605 | LR 1.04e-04\n",
      "Epoch 007 | TrainLoss 0.4530 | TrainAcc 0.4688 | ValLoss 0.4553 | ValAcc 0.4749 | MacroF1 0.4612 | LR 6.26e-05\n",
      "Epoch 008 | TrainLoss 0.4522 | TrainAcc 0.4725 | ValLoss 0.4554 | ValAcc 0.4783 | MacroF1 0.4613 | LR 2.96e-05\n",
      "Epoch 009 | TrainLoss 0.4510 | TrainAcc 0.4738 | ValLoss 0.4555 | ValAcc 0.4765 | MacroF1 0.4612 | LR 8.32e-06\n",
      "Epoch 010 | TrainLoss 0.4520 | TrainAcc 0.4746 | ValLoss 0.4559 | ValAcc 0.4765 | MacroF1 0.4618 | LR 3.00e-04\n",
      "Epoch 011 | TrainLoss 0.4557 | TrainAcc 0.4636 | ValLoss 0.4566 | ValAcc 0.4989 | MacroF1 0.4538 | LR 2.98e-04\n",
      "Epoch 012 | TrainLoss 0.4519 | TrainAcc 0.4717 | ValLoss 0.4526 | ValAcc 0.4911 | MacroF1 0.4607 | LR 2.93e-04\n",
      "Epoch 013 | TrainLoss 0.4483 | TrainAcc 0.4747 | ValLoss 0.4523 | ValAcc 0.4827 | MacroF1 0.4642 | LR 2.84e-04\n",
      "Epoch 014 | TrainLoss 0.4441 | TrainAcc 0.4856 | ValLoss 0.4548 | ValAcc 0.4697 | MacroF1 0.4603 | LR 2.71e-04\n",
      "Epoch 015 | TrainLoss 0.4431 | TrainAcc 0.4883 | ValLoss 0.4506 | ValAcc 0.4851 | MacroF1 0.4549 | LR 2.56e-04\n",
      "Epoch 016 | TrainLoss 0.4398 | TrainAcc 0.4898 | ValLoss 0.4529 | ValAcc 0.4747 | MacroF1 0.4619 | LR 2.38e-04\n",
      "Epoch 017 | TrainLoss 0.4371 | TrainAcc 0.4976 | ValLoss 0.4534 | ValAcc 0.4749 | MacroF1 0.4622 | LR 2.18e-04\n",
      "Epoch 018 | TrainLoss 0.4368 | TrainAcc 0.4964 | ValLoss 0.4496 | ValAcc 0.4789 | MacroF1 0.4647 | LR 1.97e-04\n",
      "Epoch 019 | TrainLoss 0.4367 | TrainAcc 0.4966 | ValLoss 0.4495 | ValAcc 0.4738 | MacroF1 0.4624 | LR 1.74e-04\n",
      "Epoch 020 | TrainLoss 0.4351 | TrainAcc 0.4991 | ValLoss 0.4492 | ValAcc 0.4710 | MacroF1 0.4621 | LR 1.50e-04\n",
      "Epoch 021 | TrainLoss 0.4340 | TrainAcc 0.5015 | ValLoss 0.4492 | ValAcc 0.4809 | MacroF1 0.4695 | LR 1.27e-04\n",
      "Epoch 022 | TrainLoss 0.4320 | TrainAcc 0.5027 | ValLoss 0.4495 | ValAcc 0.4856 | MacroF1 0.4667 | LR 1.04e-04\n",
      "Epoch 023 | TrainLoss 0.4321 | TrainAcc 0.5063 | ValLoss 0.4499 | ValAcc 0.4759 | MacroF1 0.4643 | LR 8.26e-05\n",
      "Epoch 024 | TrainLoss 0.4308 | TrainAcc 0.5038 | ValLoss 0.4497 | ValAcc 0.4841 | MacroF1 0.4694 | LR 6.26e-05\n",
      "Epoch 025 | TrainLoss 0.4312 | TrainAcc 0.5051 | ValLoss 0.4503 | ValAcc 0.4752 | MacroF1 0.4642 | LR 4.48e-05\n",
      "Epoch 026 | TrainLoss 0.4305 | TrainAcc 0.5079 | ValLoss 0.4503 | ValAcc 0.4807 | MacroF1 0.4674 | LR 2.96e-05\n",
      "Epoch 027 | TrainLoss 0.4308 | TrainAcc 0.5027 | ValLoss 0.4498 | ValAcc 0.4802 | MacroF1 0.4672 | LR 1.73e-05\n",
      "Epoch 028 | TrainLoss 0.4302 | TrainAcc 0.5097 | ValLoss 0.4498 | ValAcc 0.4780 | MacroF1 0.4656 | LR 8.32e-06\n",
      "Epoch 029 | TrainLoss 0.4296 | TrainAcc 0.5056 | ValLoss 0.4502 | ValAcc 0.4773 | MacroF1 0.4652 | LR 2.84e-06\n",
      "Epoch 030 | TrainLoss 0.4301 | TrainAcc 0.5080 | ValLoss 0.4500 | ValAcc 0.4763 | MacroF1 0.4644 | LR 3.00e-04\n",
      "Epoch 031 | TrainLoss 0.4340 | TrainAcc 0.5011 | ValLoss 0.4490 | ValAcc 0.4689 | MacroF1 0.4627 | LR 3.00e-04\n",
      "Epoch 032 | TrainLoss 0.4339 | TrainAcc 0.4952 | ValLoss 0.4485 | ValAcc 0.4716 | MacroF1 0.4604 | LR 2.98e-04\n",
      "Epoch 033 | TrainLoss 0.4348 | TrainAcc 0.5009 | ValLoss 0.4481 | ValAcc 0.4827 | MacroF1 0.4684 | LR 2.96e-04\n",
      "Epoch 034 | TrainLoss 0.4329 | TrainAcc 0.5045 | ValLoss 0.4499 | ValAcc 0.4801 | MacroF1 0.4667 | LR 2.93e-04\n",
      "Epoch 035 | TrainLoss 0.4318 | TrainAcc 0.5041 | ValLoss 0.4458 | ValAcc 0.4896 | MacroF1 0.4705 | LR 2.89e-04\n",
      "Epoch 036 | TrainLoss 0.4304 | TrainAcc 0.5070 | ValLoss 0.4472 | ValAcc 0.4843 | MacroF1 0.4641 | LR 2.84e-04\n",
      "Epoch 037 | TrainLoss 0.4304 | TrainAcc 0.5100 | ValLoss 0.4503 | ValAcc 0.4752 | MacroF1 0.4646 | LR 2.78e-04\n",
      "Epoch 038 | TrainLoss 0.4290 | TrainAcc 0.5051 | ValLoss 0.4482 | ValAcc 0.4883 | MacroF1 0.4635 | LR 2.71e-04\n",
      "Epoch 039 | TrainLoss 0.4292 | TrainAcc 0.5049 | ValLoss 0.4471 | ValAcc 0.4781 | MacroF1 0.4651 | LR 2.64e-04\n",
      "Epoch 040 | TrainLoss 0.4288 | TrainAcc 0.5048 | ValLoss 0.4478 | ValAcc 0.4750 | MacroF1 0.4627 | LR 2.56e-04\n",
      "Epoch 041 | TrainLoss 0.4284 | TrainAcc 0.5097 | ValLoss 0.4479 | ValAcc 0.4785 | MacroF1 0.4664 | LR 2.48e-04\n",
      "Epoch 042 | TrainLoss 0.4270 | TrainAcc 0.5122 | ValLoss 0.4487 | ValAcc 0.4702 | MacroF1 0.4613 | LR 2.38e-04\n",
      "Epoch 043 | TrainLoss 0.4267 | TrainAcc 0.5092 | ValLoss 0.4503 | ValAcc 0.4716 | MacroF1 0.4610 | LR 2.29e-04\n",
      "Epoch 044 | TrainLoss 0.4271 | TrainAcc 0.5054 | ValLoss 0.4490 | ValAcc 0.4812 | MacroF1 0.4680 | LR 2.18e-04\n",
      "Epoch 045 | TrainLoss 0.4263 | TrainAcc 0.5120 | ValLoss 0.4488 | ValAcc 0.4726 | MacroF1 0.4636 | LR 2.08e-04\n",
      "Epoch 046 | TrainLoss 0.4252 | TrainAcc 0.5094 | ValLoss 0.4496 | ValAcc 0.4713 | MacroF1 0.4640 | LR 1.97e-04\n",
      "Epoch 047 | TrainLoss 0.4252 | TrainAcc 0.5138 | ValLoss 0.4494 | ValAcc 0.4755 | MacroF1 0.4642 | LR 1.85e-04\n",
      "Epoch 048 | TrainLoss 0.4253 | TrainAcc 0.5081 | ValLoss 0.4508 | ValAcc 0.4843 | MacroF1 0.4677 | LR 1.74e-04\n",
      "Epoch 049 | TrainLoss 0.4243 | TrainAcc 0.5127 | ValLoss 0.4473 | ValAcc 0.4705 | MacroF1 0.4613 | LR 1.62e-04\n",
      "Epoch 050 | TrainLoss 0.4245 | TrainAcc 0.5114 | ValLoss 0.4484 | ValAcc 0.4875 | MacroF1 0.4676 | LR 1.50e-04\n",
      "> 50 Epochì—ì„œ ì¡°ê¸° ì¢…ë£Œ. Best ValLoss: 0.4458\n"
     ]
    }
   ],
   "source": [
    "# 4) PyTorch Dataset / DataLoader (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "class CSNDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None: return self.X[i]\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(CSNDataset(X_tr, y_tr), batch_size=512, shuffle=True)\n",
    "val_loader   = DataLoader(CSNDataset(X_va, y_va), batch_size=1024, shuffle=False)\n",
    "\n",
    "# -----------------\n",
    "# 5) ëª¨ë¸ ì •ì˜ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "input_dim = X_tr.shape[1]\n",
    "num_classes = len(np.unique(y_tr))\n",
    "\n",
    "class WideMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden=512, layers=4, pdrop=0.2):\n",
    "        super().__init__()\n",
    "        dims = [input_dim] + [hidden]*layers\n",
    "        blocks = []\n",
    "        for i in range(len(dims)-1):\n",
    "            blocks += [\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(pdrop),\n",
    "            ]\n",
    "        self.backbone = nn.Sequential(*blocks)\n",
    "        self.head = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "model = WideMLP(input_dim, num_classes, hidden=512, layers=4, pdrop=0.2).to(DEVICE)\n",
    "\n",
    "# -----------------\n",
    "# 6) ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "classes, counts = np.unique(y_tr, return_counts=True)\n",
    "alpha = (counts.sum() / (len(classes) * counts)).astype(np.float32)\n",
    "alpha = torch.tensor(alpha, device=DEVICE)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma; self.alpha = alpha; self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        log_prob = torch.log_softmax(logits, dim=1)\n",
    "        prob = torch.softmax(logits, dim=1)\n",
    "        pt = prob[torch.arange(logits.size(0)), target]\n",
    "        log_pt = log_prob[torch.arange(logits.size(0)), target]\n",
    "        focal = (1 - pt).pow(self.gamma)\n",
    "        if self.alpha is not None:\n",
    "            a = self.alpha[target]\n",
    "            loss = -a * focal * log_pt\n",
    "        else:\n",
    "            loss = -focal * log_pt\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0, alpha=alpha, reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# -----------------\n",
    "# 7) ì¡°ê¸° ì¢…ë£Œ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, mode=\"min\", min_delta=0.0):\n",
    "        self.patience = patience; self.mode = mode; self.min_delta = min_delta\n",
    "        self.best = None; self.count = 0; self.stop = False\n",
    "    def __call__(self, current):\n",
    "        if self.best is None: self.best = current; return\n",
    "        improved = (current < self.best - self.min_delta) if self.mode==\"min\" else (current > self.best + self.min_delta)\n",
    "        if improved: self.best = current; self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience: self.stop = True\n",
    "\n",
    "early_stop = EarlyStopping(patience=15, mode=\"min\", min_delta=0.0)\n",
    "\n",
    "# -----------------\n",
    "# 8) í•™ìŠµ ë£¨í”„ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "epochs = 150\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "print(\"ğŸ”¥ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item() * len(xb)\n",
    "        tr_correct += (out.argmax(1) == yb).sum().item()\n",
    "        tr_total += len(xb)\n",
    "        scheduler.step(epoch - 1 + (tr_total / len(train_loader.dataset)))\n",
    "\n",
    "    tr_loss /= tr_total; tr_acc = tr_correct / tr_total\n",
    "\n",
    "    model.eval()\n",
    "    va_loss, va_total, va_correct = 0.0, 0, 0\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            va_loss += loss.item() * len(xb)\n",
    "            pred = out.argmax(1)\n",
    "            va_correct += (pred == yb).sum().item()\n",
    "            va_total += len(xb)\n",
    "            all_preds.append(pred.cpu().numpy()); all_true.append(yb.cpu().numpy())\n",
    "    va_loss /= va_total; va_acc = va_correct / va_total\n",
    "    y_true = np.concatenate(all_true); y_pred = np.concatenate(all_preds)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | TrainLoss {tr_loss:.4f} | TrainAcc {tr_acc:.4f} | \"\n",
    "          f\"ValLoss {va_loss:.4f} | ValAcc {va_acc:.4f} | MacroF1 {macro_f1:.4f} | LR {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if va_loss < best_val_loss:\n",
    "        best_val_loss = va_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    early_stop(va_loss)\n",
    "    if early_stop.stop:\n",
    "        print(f\"> {epoch} Epochì—ì„œ ì¡°ê¸° ì¢…ë£Œ. Best ValLoss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b01b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•©ë‹ˆë‹¤...\n",
      "[FullTrain] Epoch 01 | Loss 0.5072 | Acc 0.4104\n",
      "[FullTrain] Epoch 02 | Loss 0.4822 | Acc 0.4318\n",
      "[FullTrain] Epoch 03 | Loss 0.4732 | Acc 0.4385\n",
      "[FullTrain] Epoch 04 | Loss 0.4649 | Acc 0.4540\n",
      "[FullTrain] Epoch 05 | Loss 0.4590 | Acc 0.4590\n",
      "[FullTrain] Epoch 06 | Loss 0.4570 | Acc 0.4652\n",
      "[FullTrain] Epoch 07 | Loss 0.4522 | Acc 0.4711\n",
      "[FullTrain] Epoch 08 | Loss 0.4511 | Acc 0.4708\n",
      "[FullTrain] Epoch 09 | Loss 0.4494 | Acc 0.4751\n",
      "[FullTrain] Epoch 10 | Loss 0.4502 | Acc 0.4728\n",
      "[FullTrain] Epoch 11 | Loss 0.4536 | Acc 0.4692\n",
      "[FullTrain] Epoch 12 | Loss 0.4489 | Acc 0.4794\n",
      "[FullTrain] Epoch 13 | Loss 0.4468 | Acc 0.4835\n",
      "[FullTrain] Epoch 14 | Loss 0.4427 | Acc 0.4850\n",
      "[FullTrain] Epoch 15 | Loss 0.4414 | Acc 0.4870\n",
      "[FullTrain] Epoch 16 | Loss 0.4401 | Acc 0.4938\n",
      "[FullTrain] Epoch 17 | Loss 0.4377 | Acc 0.4950\n",
      "[FullTrain] Epoch 18 | Loss 0.4376 | Acc 0.4977\n",
      "[FullTrain] Epoch 19 | Loss 0.4362 | Acc 0.4971\n",
      "[FullTrain] Epoch 20 | Loss 0.4355 | Acc 0.4971\n",
      "[FullTrain] Epoch 21 | Loss 0.4334 | Acc 0.5025\n",
      "[FullTrain] Epoch 22 | Loss 0.4345 | Acc 0.4978\n",
      "[FullTrain] Epoch 23 | Loss 0.4324 | Acc 0.5031\n",
      "[FullTrain] Epoch 24 | Loss 0.4324 | Acc 0.5016\n",
      "[FullTrain] Epoch 25 | Loss 0.4319 | Acc 0.5027\n",
      "[FullTrain] Epoch 26 | Loss 0.4304 | Acc 0.5035\n",
      "[FullTrain] Epoch 27 | Loss 0.4311 | Acc 0.5055\n",
      "[FullTrain] Epoch 28 | Loss 0.4323 | Acc 0.5013\n",
      "[FullTrain] Epoch 29 | Loss 0.4317 | Acc 0.5038\n",
      "[FullTrain] Epoch 30 | Loss 0.4312 | Acc 0.5041\n"
     ]
    }
   ],
   "source": [
    "# 9) ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "retrain_for_submit = True\n",
    "if retrain_for_submit:\n",
    "    print(\"\\nğŸ”¥ ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•©ë‹ˆë‹¤...\")\n",
    "    # âœ… ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ì ìš©\n",
    "    full_train_featured, full_agg_maps = feature_engineering_advanced(train_df, is_train=True)\n",
    "    \n",
    "    # âœ… NUM_COLSë¥¼ ë‹¤ì‹œ í•œë²ˆ ì—…ë°ì´íŠ¸ (Full Train ê¸°ì¤€)\n",
    "    full_new_num_cols = [col for col in full_train_featured.columns if 'diff' in col or 'norm' in col]\n",
    "    NUM_COLS = original_num_cols + full_new_num_cols\n",
    "\n",
    "    full_prep = Preprocessor()\n",
    "    X_full, y_full = full_prep.fit_transform_train(full_train_featured)\n",
    "    full_loader = DataLoader(CSNDataset(X_full, y_full), batch_size=512, shuffle=True)\n",
    "    \n",
    "    model = WideMLP(X_full.shape[1], num_classes, hidden=512, layers=4, pdrop=0.2).to(DEVICE)\n",
    "\n",
    "    _, counts_full = np.unique(y_full, return_counts=True)\n",
    "    alpha_full = (counts_full.sum() / (len(classes) * counts_full)).astype(np.float32)\n",
    "    alpha_full = torch.tensor(alpha_full, device=DEVICE)\n",
    "\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=alpha_full, reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    model.train()\n",
    "    warm_epochs = 30\n",
    "    for e in range(1, warm_epochs+1):\n",
    "        tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
    "        for xb, yb in full_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item() * len(xb)\n",
    "            tr_correct += (out.argmax(1) == yb).sum().item()\n",
    "            tr_total += len(xb)\n",
    "            scheduler.step(e - 1 + (tr_total / len(full_loader.dataset)))\n",
    "        print(f\"[FullTrain] Epoch {e:02d} | Loss {tr_loss/tr_total:.4f} | Acc {tr_correct/tr_total:.4f}\")\n",
    "\n",
    "    prep = full_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651713a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ test.csv ë°ì´í„°ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...\n",
      "âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission_20250818_165750.csv\n"
     ]
    }
   ],
   "source": [
    "# 10) test.csv ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ì €ì¥ (ìˆ˜ì • ì—†ìŒ)\n",
    "# -----------------\n",
    "print(\"\\nğŸ”¥ test.csv ë°ì´í„°ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "# âœ… ì‹¬í™” í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©\n",
    "test_df_featured = feature_engineering_advanced(test_df, is_train=False, agg_maps=full_agg_maps)\n",
    "\n",
    "id_col = \"ID\"\n",
    "X_te, _ = prep.transform_val_or_test(test_df_featured, has_target=False)\n",
    "\n",
    "test_loader = DataLoader(CSNDataset(X_te, None), batch_size=1024, shuffle=False)\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        pred = out.argmax(1).cpu().numpy()\n",
    "        preds.append(pred)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"ID\": test_df[id_col],\n",
    "    \"support_needs\": preds.astype(int)\n",
    "})\n",
    "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"submission_{stamp}.csv\"\n",
    "sub.to_csv(out_path, index=False)\n",
    "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
