{"cells":[{"cell_type":"markdown","metadata":{"id":"i1sviZfjra6c"},"source":["참고 (딥러닝을 이용한 자연어 처리)\n",": https://wikidocs.net/94600\n","\n","위 링크에는 keras framework로 신경망을 구현한 반면, 이번 과제에서는 **pytorch** framework로 구현해보도록 합니다.\n","\n","참고 : https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/\n","https://wonhwa.tistory.com/35"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJdUylezMQ19"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3Piwi4SkyZs"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"{device}\" \" is available.\")"]},{"cell_type":"markdown","metadata":{"id":"7VWSo5CAjxu8"},"source":["### 데이터 불러오기 (네이버 쇼핑 리뷰 감성 분석 데이터)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IXiir7_oFO6"},"outputs":[],"source":["!pip install konlpy\n","!pip install mecab-python\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"312yGF-nn-7Z","executionInfo":{"status":"ok","timestamp":1753534205512,"user_tz":-540,"elapsed":7510,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1XyR9D-oSsJ"},"outputs":[],"source":["# 데이터 로드하기\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n","total_data = pd.read_table('ratings_total.txt', names = ['ratings', 'reviews'])\n","\n","# 데이터 개수 확인\n","print('리뷰 개수 : ', len(total_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_uJez_mol6E"},"outputs":[],"source":["total_data[:5] # 상위 5개 데이터만 샘플로 확인"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1R4QJEaxorat","executionInfo":{"status":"ok","timestamp":1753534258215,"user_tz":-540,"elapsed":90,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# 감성 분석을 위한 라벨링\n","total_data['label'] = np.select([total_data.ratings > 3], [1], default=0) # 4,5점은 긍정 1 / 1,2 점은 부정 0 으로 라벨링\n","total_data.drop_duplicates(subset=['reviews'], inplace=True) # 중복 제거\n","\n","# 훈련 데이터와 테스트 데이터 split\n","train_data, test_data = train_test_split(total_data, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lI6XJRUqTIp"},"outputs":[],"source":["train_data['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"vMftA9UKqiBl"},"source":["### 데이터 정제 및 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5JDUsBpqj45"},"outputs":[],"source":["# 한글과 공백을 제외하고 모두 제거 (train)\n","# [^ㄱ-ㅎㅏ-ㅣ가-힣 ]: 정규 표현식으로, 한글(모음과 자음)과 띄어쓰기를 제외한 문자들을 매칭\n","train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","train_data['reviews'].replace('', np.nan, inplace=True)\n","\n","# test data에도 동일하게 적용\n","test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n","test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거"]},{"cell_type":"markdown","metadata":{"id":"cn6IM4UQq8ad"},"source":["### 토큰화"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"4u0NrFSaq9vd","executionInfo":{"status":"ok","timestamp":1753534437838,"user_tz":-540,"elapsed":60128,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# Mecab 모델로 형태소 분석 및 토큰화\n","mecab = Mecab()\n","\n","# 불용어 설정\n","# stopword.txt 파일이 저장된 경로를 정확히 입력해주세요\n","with open('/content/stopword.txt') as f:\n","    list_file = f.readlines()\n","\n","stopwords_list = []\n","for stopword in list_file:\n","  stopwords = re.sub('[\\n]', '', stopword)\n","  stopwords_list.append(stopwords)\n","\n","# train data 토큰화\n","train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n","train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords_list])\n","\n","# test data 토큰화\n","test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n","test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords_list])"]},{"cell_type":"markdown","metadata":{"id":"_CD_HiUHroCe"},"source":["### 정수 인코딩"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"4H9Bu8yxr03N","executionInfo":{"status":"ok","timestamp":1753534437854,"user_tz":-540,"elapsed":2,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# train과 test를 위한 X,Y data 분류\n","\n","X_train = train_data['tokenized'].values\n","y_train = train_data['label'].values\n","X_test= test_data['tokenized'].values\n","y_test = test_data['label'].values"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"aiQQp7KCrpR6","executionInfo":{"status":"ok","timestamp":1753534439706,"user_tz":-540,"elapsed":1853,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# 단어 집합 생성 및 정수 인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"nUz8VtqgsFjB","executionInfo":{"status":"ok","timestamp":1753534442875,"user_tz":-540,"elapsed":3167,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# vocab_size 설정\n","vocab_size = len(tokenizer.word_index)\n","\n","# 텍스트 시퀀스 -> 정수 시퀀스\n","tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"markdown","metadata":{"id":"OYZGDvYhsbF2"},"source":["### 패딩\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WULaQy5-scVq"},"outputs":[],"source":["print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmlo_6gVO3az"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","num_tokens = [len(review) for review in X_train]\n","\n","plt.title('all text length')\n","plt.hist(num_tokens, bins=30)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44WPTfENOWAa"},"outputs":[],"source":["select_length = 40\n","\n","def below_threshold_len(max_len, nested_list):\n","    cnt = 0\n","    for s in nested_list:\n","        if(len(s) <= max_len):\n","            cnt = cnt + 1\n","\n","    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n","\n","below_threshold_len(select_length, X_train)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"LiyYv--EsjAO","executionInfo":{"status":"ok","timestamp":1753534444570,"user_tz":-540,"elapsed":831,"user":{"displayName":"기광민","userId":"01358374244313288364"}}},"outputs":[],"source":["# 최대 길이를 40으로 잡고 패딩\n","max_len = 40\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)"]},{"cell_type":"markdown","metadata":{"id":"xV8fVR4KxSb_"},"source":["### 퀴즈\n","\n","문제: 패딩을 하는 이유는 무엇일까요? (어떤 경우에 사용하는지 생각해보세요)\n","\n","답변:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZPf38qXttio"},"outputs":[],"source":["# model 변경 함수\n","def get_model(model, model_params):\n","    models = {\n","        \"rnn\": RNNModel,\n","        \"lstm\": LSTMModel,\n","        \"gru\": GRUModel,\n","    }\n","    return models.get(model.lower())(**model_params)"]},{"cell_type":"markdown","metadata":{"id":"JXExaNzZLUG1"},"source":["### DataLoader 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyVTAop2L9Kv"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class LabeledNumpyArrayDataset(Dataset):\n","    def __init__(self, numpy_data, numpy_labels, transform=None):\n","        self.data = numpy_data\n","        self.labels = numpy_labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        sample = self.data[index]\n","        label = self.labels[index]\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROX3dxWBLWA2"},"outputs":[],"source":["batch_size = 64\n","\n","# 레이블링된 데이터셋 객체 생성\n","train_dataset = LabeledNumpyArrayDataset(X_train, y_train)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_dataset = LabeledNumpyArrayDataset(X_test, y_test)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"2TMAHztyso_m"},"source":["### 모델 학습 (Vanilla RNN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaX9qNpYljQi"},"outputs":[],"source":["class RNNModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n","        super(RNNModel, self).__init__()\n","        self.device = device\n","        self.hidden_dim = hidden_dim\n","        self.layer_dim = layer_dim\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.rnn = nn.RNN(\n","            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","        # Fully connected layer\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","\n","        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n","        h0 = h0.to(self.device)\n","        out, h0 = self.rnn(embedded, h0.detach())\n","\n","        # 현재 out의 차원은 (batch_size, seq_length, hidden_size)입니다.\n","        # 이를 fully connected layer에 fit하게 차원을 변경(batch_size, hidden_size)해주어야 합니다.\n","        out = out[:, -1, :] # 64 x 64 size\n","\n","        \"\"\"\n","        문제 1: 이제 out을 우리가 원하는 ouput_dim 차원으로 변환해주어야 합니다.\n","        빈칸에 들어갈 인스턴스 변수를 채워넣어주세요.\n","        \"\"\"\n","        out = self.\"빈칸\"(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLCP04LFvZlD"},"outputs":[],"source":["num_epoch = 15\n","batch_size = 64\n","embedding_dim = 64\n","hidden_dim = 64\n","layer_dim = 1\n","output_dim = 4\n","\n","model = RNNModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n","\n","\n","\"\"\"\n","문제 2: RNN에서 loss를 계산하는 기준은 무엇일까요? 빈칸에 알맞은 답을 적어주세요\n","\"\"\"\n","criterion = nn.\"빈칸\"()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFQCXBDf17hW","outputId":"71aec980-e1fa-4922-a916-317c698452f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.3417228162288666. Accuracy: 86.61898040771484\n","Iteration: 1000. Loss: 0.5003020763397217. Accuracy: 86.22630310058594\n","Iteration: 1500. Loss: 0.3734841048717499. Accuracy: 87.24925994873047\n","Iteration: 2000. Loss: 0.3508836030960083. Accuracy: 87.20924377441406\n","Iteration: 2500. Loss: 0.3431645929813385. Accuracy: 87.65194702148438\n","Iteration: 3000. Loss: 0.2798011302947998. Accuracy: 87.61192321777344\n","Iteration: 3500. Loss: 0.5112345814704895. Accuracy: 87.80951690673828\n","Iteration: 4000. Loss: 0.2871612012386322. Accuracy: 88.45980834960938\n","Iteration: 4500. Loss: 0.24245236814022064. Accuracy: 89.33270263671875\n","Iteration: 5000. Loss: 0.32162782549858093. Accuracy: 89.32019805908203\n","Iteration: 5500. Loss: 0.23347806930541992. Accuracy: 89.57280731201172\n","Iteration: 6000. Loss: 0.321621298789978. Accuracy: 88.90250396728516\n","Iteration: 6500. Loss: 0.3201277256011963. Accuracy: 89.46276092529297\n","Iteration: 7000. Loss: 0.166553795337677. Accuracy: 89.33270263671875\n","Iteration: 7500. Loss: 0.18673327565193176. Accuracy: 89.13761138916016\n","Iteration: 8000. Loss: 0.2524878978729248. Accuracy: 89.05257415771484\n","Iteration: 8500. Loss: 0.31181445717811584. Accuracy: 87.93707275390625\n","Iteration: 9000. Loss: 0.19383327662944794. Accuracy: 89.18013000488281\n","Iteration: 9500. Loss: 0.4007154107093811. Accuracy: 89.15512084960938\n","Iteration: 10000. Loss: 0.2052377164363861. Accuracy: 87.95957946777344\n","Iteration: 10500. Loss: 0.3899165391921997. Accuracy: 88.57485961914062\n","Iteration: 11000. Loss: 0.3506643772125244. Accuracy: 85.8886489868164\n","Iteration: 11500. Loss: 0.22120116651058197. Accuracy: 88.38977813720703\n","Iteration: 12000. Loss: 0.24845638871192932. Accuracy: 88.65989685058594\n","Iteration: 12500. Loss: 0.2154388576745987. Accuracy: 87.50187683105469\n","Iteration: 13000. Loss: 0.37935057282447815. Accuracy: 88.38227081298828\n","Iteration: 13500. Loss: 0.15180271863937378. Accuracy: 88.3272476196289\n","Iteration: 14000. Loss: 0.24958094954490662. Accuracy: 88.60237121582031\n","Iteration: 14500. Loss: 0.28850269317626953. Accuracy: 88.42478942871094\n","Iteration: 15000. Loss: 0.48623767495155334. Accuracy: 88.71241760253906\n","Iteration: 15500. Loss: 0.23146137595176697. Accuracy: 88.742431640625\n","Iteration: 16000. Loss: 0.23904380202293396. Accuracy: 88.9050064086914\n","Iteration: 16500. Loss: 0.1867428719997406. Accuracy: 88.85498809814453\n","Iteration: 17000. Loss: 0.31442201137542725. Accuracy: 89.26516723632812\n","Iteration: 17500. Loss: 0.28077080845832825. Accuracy: 88.35726165771484\n","Iteration: 18000. Loss: 0.23309564590454102. Accuracy: 88.4673080444336\n","Iteration: 18500. Loss: 0.22572723031044006. Accuracy: 89.18513488769531\n","Iteration: 19000. Loss: 0.3733830451965332. Accuracy: 88.75244140625\n","Iteration: 19500. Loss: 0.2490193098783493. Accuracy: 87.99209594726562\n","Iteration: 20000. Loss: 0.14003154635429382. Accuracy: 89.0275650024414\n","Iteration: 20500. Loss: 0.28678208589553833. Accuracy: 88.87248992919922\n","Iteration: 21000. Loss: 0.27531537413597107. Accuracy: 88.88249969482422\n","Iteration: 21500. Loss: 0.1741550713777542. Accuracy: 87.34930419921875\n","Iteration: 22000. Loss: 0.26726987957954407. Accuracy: 88.77494812011719\n","Iteration: 22500. Loss: 0.22894622385501862. Accuracy: 88.62738037109375\n","Iteration: 23000. Loss: 0.22791649401187897. Accuracy: 88.987548828125\n","Iteration: 23500. Loss: 0.12518063187599182. Accuracy: 88.60237121582031\n","Iteration: 24000. Loss: 0.30351245403289795. Accuracy: 88.37226867675781\n","Iteration: 24500. Loss: 0.26316219568252563. Accuracy: 88.83998107910156\n","Iteration: 25000. Loss: 0.2405957281589508. Accuracy: 88.77244567871094\n","Iteration: 25500. Loss: 0.32814881205558777. Accuracy: 88.69491577148438\n","Iteration: 26000. Loss: 0.2593920826911926. Accuracy: 87.49687194824219\n","Iteration: 26500. Loss: 0.1897873431444168. Accuracy: 88.48982238769531\n","Iteration: 27000. Loss: 0.21581268310546875. Accuracy: 88.21720123291016\n","Iteration: 27500. Loss: 0.24175186455249786. Accuracy: 88.40478515625\n","Iteration: 28000. Loss: 0.279264360666275. Accuracy: 88.68490600585938\n","Iteration: 28500. Loss: 0.33845892548561096. Accuracy: 88.5948715209961\n","Iteration: 29000. Loss: 0.32855623960494995. Accuracy: 87.61192321777344\n","Iteration: 29500. Loss: 0.2315824031829834. Accuracy: 88.23970794677734\n","Iteration: 30000. Loss: 0.32836127281188965. Accuracy: 88.07963562011719\n","Iteration: 30500. Loss: 0.41383105516433716. Accuracy: 88.33475494384766\n","Iteration: 31000. Loss: 0.2045784741640091. Accuracy: 88.169677734375\n","Iteration: 31500. Loss: 0.20078010857105255. Accuracy: 88.1746826171875\n","Iteration: 32000. Loss: 0.2616656720638275. Accuracy: 88.59736633300781\n","Iteration: 32500. Loss: 0.20425169169902802. Accuracy: 88.46981048583984\n","Iteration: 33000. Loss: 0.20733612775802612. Accuracy: 88.2472152709961\n","Iteration: 33500. Loss: 0.20033226907253265. Accuracy: 88.54484558105469\n","Iteration: 34000. Loss: 0.19680951535701752. Accuracy: 88.864990234375\n","Iteration: 34500. Loss: 0.29152047634124756. Accuracy: 88.2872314453125\n","Iteration: 35000. Loss: 0.21900369226932526. Accuracy: 88.13966369628906\n","Iteration: 35500. Loss: 0.42489108443260193. Accuracy: 87.70947265625\n","Iteration: 36000. Loss: 0.19023454189300537. Accuracy: 87.96458435058594\n","Iteration: 36500. Loss: 0.23114174604415894. Accuracy: 88.08464050292969\n","Iteration: 37000. Loss: 0.49269333481788635. Accuracy: 88.05963134765625\n"]}],"source":["iter = 0\n","for epoch in range(num_epoch):\n","    for i, (text, labels) in enumerate(train_dataloader):\n","        model.train()\n","\n","        text = text.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits = model(text).to(device)\n","\n","\n","\n","        \"\"\"\n","        문제2: loss를 구하기 위해서 위에서 정의한 변수 중 무엇을 사용하면 될까요? 빈칸을 채워넣어주세요.\n","        \"\"\"\n","        loss = \"빈칸\"(logits, labels)\n","\n","        loss.backward()\n","\n","        \"\"\"\n","        문제3: 역전파를 거친 후 매개변수(가중치)를 업데이트하기 위해서 필요한 메서드는 무엇이었나요?\n","        빈칸을 채워넣어주세요.\n","        \"\"\"\n","        \"빈칸\"()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            model.eval()\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for test_text, test_labels in test_dataloader:\n","                test_text = test_text.to(device)\n","                test_labels = test_labels.to(device)\n","                # Forward pass only to get logits/output\n","                outputs = model(test_text)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += test_labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == test_labels).sum()\n","\n","            accuracy = 100 * correct / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"7C3Q3Cd3lkYw"},"source":["### 모델학습 (LSTM, Long short term memory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j23sTsP2Sv_"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","\n","    def __init__(self,vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n","        super(LSTMModel, self).__init__()\n","\n","        self.device = device\n","\n","        # Defining the number of layers and the nodes in each layer\n","        self.hidden_dim = hidden_dim\n","        self.layer_dim = layer_dim\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(\n","            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","\n","        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n","        h0 = h0.to(self.device)\n","\n","        # 초기에 cell state를 영행렬로 초기화\n","        c0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n","        c0 = c0.to(self.device)\n","\n","        out, (hn, cn) = self.lstm(embedded, (h0.detach(), c0.detach()))\n","\n","        # 현재 out의 차원은 (batch_size, seq_length, hidden_size)입니다.\n","        # 이를 fully connected layer에 fit하게 차원을 변경(batch_size, hidden_size)해주어야 합니다.\n","        out = out[:, -1, :]\n","\n","        # fc layer를 통해 (batch_size, output_dim)로 차원을 변경해줍니다.\n","        out = self.fc(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-PZquMSZ6Fp"},"outputs":[],"source":["num_epoch = 15\n","batch_size = 64\n","embedding_dim = 64\n","hidden_dim = 64\n","layer_dim = 1\n","output_dim = 4\n","\n","model = LSTMModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n","\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvcL3dQVYDx5","outputId":"f4d83ee0-7828-4622-92b8-d2e9aba95d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 1000. Loss: 0.31025418639183044. Accuracy: 90.41068267822266\n","Iteration: 2000. Loss: 0.17411217093467712. Accuracy: 91.39862823486328\n","Iteration: 3000. Loss: 0.20097750425338745. Accuracy: 91.2535629272461\n","Iteration: 4000. Loss: 0.24761079251766205. Accuracy: 91.52618408203125\n","Iteration: 5000. Loss: 0.10168638825416565. Accuracy: 91.44615173339844\n","Iteration: 6000. Loss: 0.13005031645298004. Accuracy: 91.34110260009766\n","Iteration: 7000. Loss: 0.16045863926410675. Accuracy: 91.55370330810547\n","Iteration: 8000. Loss: 0.22772757709026337. Accuracy: 91.35610961914062\n","Iteration: 9000. Loss: 0.052641093730926514. Accuracy: 91.3761215209961\n","Iteration: 10000. Loss: 0.10561151057481766. Accuracy: 91.31108856201172\n","Iteration: 11000. Loss: 0.15938007831573486. Accuracy: 91.16352081298828\n","Iteration: 12000. Loss: 0.13293315470218658. Accuracy: 91.04846954345703\n","Iteration: 13000. Loss: 0.05415041744709015. Accuracy: 91.13351440429688\n","Iteration: 14000. Loss: 0.031943608075380325. Accuracy: 90.76334381103516\n","Iteration: 15000. Loss: 0.07826254516839981. Accuracy: 90.95343017578125\n","Iteration: 16000. Loss: 0.09376862645149231. Accuracy: 90.4857177734375\n","Iteration: 17000. Loss: 0.08778738975524902. Accuracy: 90.61577606201172\n","Iteration: 18000. Loss: 0.12674373388290405. Accuracy: 90.5982666015625\n","Iteration: 19000. Loss: 0.09493274986743927. Accuracy: 90.49072265625\n","Iteration: 20000. Loss: 0.1554013043642044. Accuracy: 90.4131851196289\n","Iteration: 21000. Loss: 0.034450430423021317. Accuracy: 90.24811553955078\n","Iteration: 22000. Loss: 0.051366619765758514. Accuracy: 90.26811981201172\n","Iteration: 23000. Loss: 0.1693437397480011. Accuracy: 90.17057800292969\n","Iteration: 24000. Loss: 0.05095380172133446. Accuracy: 90.03551483154297\n","Iteration: 25000. Loss: 0.1073891744017601. Accuracy: 90.00050354003906\n","Iteration: 26000. Loss: 0.025584083050489426. Accuracy: 90.11805725097656\n","Iteration: 27000. Loss: 0.09318941086530685. Accuracy: 89.97798919677734\n","Iteration: 28000. Loss: 0.06543240696191788. Accuracy: 89.88794708251953\n","Iteration: 29000. Loss: 0.07437781989574432. Accuracy: 89.69286346435547\n","Iteration: 30000. Loss: 0.06576196849346161. Accuracy: 90.0255126953125\n","Iteration: 31000. Loss: 0.01783139631152153. Accuracy: 89.92546844482422\n","Iteration: 32000. Loss: 0.08521056920289993. Accuracy: 90.02801513671875\n","Iteration: 33000. Loss: 0.052069153636693954. Accuracy: 89.70787048339844\n","Iteration: 34000. Loss: 0.04325362667441368. Accuracy: 89.70536804199219\n","Iteration: 35000. Loss: 0.02997981756925583. Accuracy: 90.0255126953125\n","Iteration: 36000. Loss: 0.06973631680011749. Accuracy: 89.77539825439453\n","Iteration: 37000. Loss: 0.08148634433746338. Accuracy: 89.76789855957031\n"]}],"source":["iter = 0\n","for epoch in range(num_epoch):\n","    for i, (text, labels) in enumerate(train_dataloader):\n","        model.train()\n","\n","        text = text.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits = model(text).to(device)\n","\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 1000 == 0:\n","            model.eval()\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for test_text, test_labels in test_dataloader:\n","                test_text = test_text.to(device)\n","                test_labels = test_labels.to(device)\n","                # Forward pass only to get logits/output\n","                outputs = model(test_text)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += test_labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == test_labels).sum()\n","\n","            accuracy = 100 * correct / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"CEmWGtFslpiz"},"source":["### 모델 학습 (Gated Recurrent Unit (GRU))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpaGyyY2ltWF"},"outputs":[],"source":["class GRUModel(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, dropout_prob, device):\n","        super(GRUModel, self).__init__()\n","\n","        # Defining the number of layers and the nodes in each layer\n","        self.device = device\n","        self.hidden_dim = hidden_dim\n","        self.layer_dim = layer_dim\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # GRU layers\n","        self.gru = nn.GRU(\n","            embedding_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, text):\n","\n","        embedded = self.embedding(text)\n","\n","        # Initializing hidden state for first input with zeros\n","        h0 = torch.zeros(self.layer_dim, text.size(0), self.hidden_dim).requires_grad_()\n","        h0 = h0.to(self.device)\n","\n","        # Forward propagation by passing in the input and hidden state into the model\n","        out, _ = self.gru(embedded, h0.detach())\n","\n","        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n","        # so that it can fit into the fully connected layer\n","        out = out[:, -1, :]\n","\n","        # Convert the final state to our desired output shape (batch_size, output_dim)\n","        out = self.fc(out)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q59GPyZSaZjX"},"outputs":[],"source":["num_epoch = 15\n","batch_size = 64\n","embedding_dim = 64\n","hidden_dim = 64\n","layer_dim = 1\n","output_dim = 4\n","\n","model = GRUModel(vocab_size, embedding_dim, hidden_dim, layer_dim, output_dim, 0.5, device)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr= 0.005)\n","\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLd0utoBacFW","outputId":"0e761450-b311-45ce-a4e6-4292860a78c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 1000. Loss: 0.22900210320949554. Accuracy: 90.2606201171875\n","Iteration: 2000. Loss: 0.2125210016965866. Accuracy: 91.1810302734375\n","Iteration: 3000. Loss: 0.244240403175354. Accuracy: 91.38612365722656\n","Iteration: 4000. Loss: 0.2796165645122528. Accuracy: 91.42613983154297\n","Iteration: 5000. Loss: 0.2500617802143097. Accuracy: 91.49617767333984\n","Iteration: 6000. Loss: 0.15174134075641632. Accuracy: 91.12600708007812\n","Iteration: 7000. Loss: 0.12852153182029724. Accuracy: 91.4936752319336\n","Iteration: 8000. Loss: 0.07080493867397308. Accuracy: 91.17852783203125\n","Iteration: 9000. Loss: 0.17887607216835022. Accuracy: 91.11100006103516\n","Iteration: 10000. Loss: 0.04742032662034035. Accuracy: 91.26606750488281\n","Iteration: 11000. Loss: 0.22352956235408783. Accuracy: 90.95843505859375\n","Iteration: 12000. Loss: 0.05240829661488533. Accuracy: 90.93842315673828\n","Iteration: 13000. Loss: 0.3067023456096649. Accuracy: 90.63328552246094\n","Iteration: 14000. Loss: 0.05012770742177963. Accuracy: 90.79085540771484\n","Iteration: 15000. Loss: 0.11716630309820175. Accuracy: 90.75584411621094\n","Iteration: 16000. Loss: 0.22163096070289612. Accuracy: 90.13306427001953\n","Iteration: 17000. Loss: 0.07878800481557846. Accuracy: 90.69581604003906\n","Iteration: 18000. Loss: 0.08611034601926804. Accuracy: 90.50322723388672\n","Iteration: 19000. Loss: 0.20948976278305054. Accuracy: 90.33565521240234\n","Iteration: 20000. Loss: 0.09742748737335205. Accuracy: 90.46321105957031\n","Iteration: 21000. Loss: 0.09649358689785004. Accuracy: 90.10054779052734\n","Iteration: 22000. Loss: 0.17636123299598694. Accuracy: 90.50823211669922\n","Iteration: 23000. Loss: 0.09731245040893555. Accuracy: 90.36066436767578\n","Iteration: 24000. Loss: 0.13266141712665558. Accuracy: 90.38317108154297\n","Iteration: 25000. Loss: 0.03281518444418907. Accuracy: 90.27812957763672\n","Iteration: 26000. Loss: 0.1921587437391281. Accuracy: 89.89795684814453\n","Iteration: 27000. Loss: 0.0732559859752655. Accuracy: 90.16057586669922\n","Iteration: 28000. Loss: 0.1796078383922577. Accuracy: 90.21309661865234\n","Iteration: 29000. Loss: 0.14677980542182922. Accuracy: 90.07303619384766\n","Iteration: 30000. Loss: 0.14862655103206635. Accuracy: 90.24811553955078\n","Iteration: 31000. Loss: 0.061387475579977036. Accuracy: 90.19058990478516\n","Iteration: 32000. Loss: 0.11199799925088882. Accuracy: 90.38317108154297\n","Iteration: 33000. Loss: 0.08256645500659943. Accuracy: 89.9129638671875\n","Iteration: 34000. Loss: 0.11032489687204361. Accuracy: 90.07803344726562\n","Iteration: 35000. Loss: 0.10241980850696564. Accuracy: 90.07553100585938\n","Iteration: 36000. Loss: 0.1917630285024643. Accuracy: 89.93797302246094\n","Iteration: 37000. Loss: 0.12547031044960022. Accuracy: 90.04302215576172\n"]}],"source":["iter = 0\n","for epoch in range(num_epoch):\n","    for i, (text, labels) in enumerate(train_dataloader):\n","        model.train()\n","\n","        text = text.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        logits = model(text).to(device)\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 1000 == 0:\n","            model.eval()\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for test_text, test_labels in test_dataloader:\n","                test_text = test_text.to(device)\n","                test_labels = test_labels.to(device)\n","                # Forward pass only to get logits/output\n","                outputs = model(test_text)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += test_labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == test_labels).sum()\n","\n","            accuracy = 100 * correct / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"mjuXGLI7eA2X"},"source":["## 모델 성능 비교"]},{"cell_type":"markdown","metadata":{"id":"cH2yfHfheC-l"},"source":["옵티마이저, 파라미터 등을 바꿔가며 모델의 성능을 향상시켜보세요.\n","\n","이후 세 가지 모델의 성능 차이를 비교하고, 자유롭게 해석해보세요!"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}