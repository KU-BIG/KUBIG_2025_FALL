{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkUGTqixRWo"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "GPU 사용 가능 여부 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oYsV4H8fCpZ-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_N2UDLevYWn"
      },
      "source": [
        "현재 Hugging Face 라이브러리는 BERT 작업을 위한 가장 널리 사용되는 인터페이스입니다. 사전 훈련된 다양한 transformer 모델을 지원하는 것 외에도 라이브러리에는 특정 작업에 적합한 이러한 모델의 사전 구축된 수정 사항도 포함되어 있습니다. 예를 들어, 본 과제에서는 `BertForSequenceClassification`을 사용합니다.\n",
        "\n",
        "라이브러리에는 토큰 분류, 질문 답변, 다음 문장 예측 등을 위한 작업별 클래스도 포함되어 있습니다. 이러한 사전 구축된 클래스를 사용하면 목적에 맞게 BERT를 수정하는 프로세스가 단순화됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# 2. Loading CoLA Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZKxKc04Btk"
      },
      "source": [
        "단일 문장 분류에는 [CoLA(The Corpus of Linguistic Acceptability)](https://nyu-mll.github.io/CoLA/) 데이터세트를 사용하겠습니다. 문법적으로 정확하거나 틀린 것으로 표시된 문장 데이터셋입니다. 2018년 5월에 처음 공개되었으며 BERT와 같은 모델이 평가되는 \"GLUE 벤치마크\"에 포함된 테스트 중 하나입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "## 2.1. Download & Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X"
      },
      "source": [
        "`wget` 패키지로 데이터를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m6AnuFv0QXQ"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pMtmPMkBzrvs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Yv-tNv20dnH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/          \n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/      \n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "## 2.2. Parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_UkeC7SG2krJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5264</th>\n",
              "      <td>b_82</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The men would not have been all working.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>r-67</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Mary and kissed an old friend who comes from M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>cj99</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The more you eat, the more you want.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>sgww85</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I consider that a rude remark and in very bad ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>cj99</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Bill can well imagine the more he eats, the fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7984</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>That photograph of Jane of Lucy's</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4535</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Have you anything to share with the group?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3222</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A fragrant stew bubbled over the fire.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8217</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I wondered whether Medea had fled.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6432</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Any man didn't eat dinner.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "5264            b_82      1         NaN   \n",
              "1721            r-67      0           *   \n",
              "241             cj99      1         NaN   \n",
              "6997          sgww85      1         NaN   \n",
              "138             cj99      0           *   \n",
              "7984            ad03      1         NaN   \n",
              "4535            ks08      1         NaN   \n",
              "3222            l-93      1         NaN   \n",
              "8217            ad03      1         NaN   \n",
              "6432            d_98      1         NaN   \n",
              "\n",
              "                                               sentence  \n",
              "5264           The men would not have been all working.  \n",
              "1721  Mary and kissed an old friend who comes from M...  \n",
              "241                The more you eat, the more you want.  \n",
              "6997  I consider that a rude remark and in very bad ...  \n",
              "138   Bill can well imagine the more he eats, the fa...  \n",
              "7984                  That photograph of Jane of Lucy's  \n",
              "4535         Have you anything to share with the group?  \n",
              "3222             A fragrant stew bubbled over the fire.  \n",
              "8217                 I wondered whether Medea had fled.  \n",
              "6432                         Any man didn't eat dinner.  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWzpPi92UAH"
      },
      "source": [
        "`sentence` 와 `label`\b만 남기겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "blqIvQaQncdJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7558</th>\n",
              "      <td>Our rabbit and the neighbor's cat like them.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>Sees he I often Mary?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4688</th>\n",
              "      <td>Pavarotti relied on Loren and Bond Hepburn.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4719</th>\n",
              "      <td>New York was slept in.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4757</th>\n",
              "      <td>Who did Kim work for and Sandy rely on Mary?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          sentence  label\n",
              "7558  Our rabbit and the neighbor's cat like them.      0\n",
              "392                          Sees he I often Mary?      0\n",
              "4688   Pavarotti relied on Loren and Bond Hepburn.      0\n",
              "4719                        New York was slept in.      0\n",
              "4757  Who did Kim work for and Sandy rely on Mary?      0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "outputs": [],
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "이 섹션에서는 데이터 세트를 BERT가 학습할 수 있는 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5"
      },
      "source": [
        "## 3.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2"
      },
      "source": [
        "텍스트를 BERT에 공급하려면 텍스트를 토큰으로 분할한 다음 이러한 토큰을 토크나이저 어휘의 인덱스에 매핑해야 합니다.\n",
        "\n",
        "토큰화는 BERT에 포함된 토크나이저에 의해 수행되어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z474sSC6oe7A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc58706c8eba410f990422972abc0fc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\junseok\\miniconda3\\envs\\py311-aws\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\junseok\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7f0e99de0294ba789c664577ed6da88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afb80da4dbad412faf5bb88976ed9d0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ec2846452794c36a1636b4e71fba2d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dLIbudgfh6F0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeNIc4auFUdF"
      },
      "source": [
        "실제로 모든 문장을 변환할 때 `tokenize`와 `convert_tokens_to_ids`를 별도로 호출하는 대신 `tokenize.encode` 함수를 사용하여 두 단계를 모두 처리합니다.\n",
        "\n",
        "하지만 그렇게 하기 전에 BERT의 형식 요구 사항 중 일부에 대해 살펴보겠습니다다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww"
      },
      "source": [
        "## 3.2. Required Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDcqNlvVhL5W"
      },
      "source": [
        "위의 코드에는 여기서 살펴볼 몇 가지 필수 형식 지정 단계가 생략되었습니다.\n",
        "\n",
        "\n",
        "우리는 다음을 수행해야 합니다.\n",
        "1. 각 문장의 시작과 끝 부분에 특수 토큰을 추가.\n",
        "2. 모든 문장을 하나의 일정한 길이로 채우고 자릅니다.\n",
        "3. \"attention mask\"를 사용하여 실제 토큰과 패딩 토큰을 명시적으로 구별합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mceWWOjZnw"
      },
      "source": [
        "### Special Tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykk0P9JiKtVe"
      },
      "source": [
        "**`[SEP]`**\n",
        "\n",
        "모든 문장 끝에 특수 `[SEP]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 BERT에 두 개의 별도 문장이 제공됨을 알립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C9objaKu8f"
      },
      "source": [
        "**`[CLS]`**\n",
        "\n",
        "분류 작업을 위해서는 모든 문장의 시작 부분에 특수 `[CLS]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 특별한 의미를 갖습니다. BERT는 12개의 Transformer 레이어로 구성됩니다. 각 transformer는 토큰 임베딩 목록을 가져와 출력에 동일한 수의 임베딩을 생성합니다.\n",
        "\n",
        "![Illustration of CLS token purpose](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n",
        "\n",
        "최종(12번째) transformer의 출력에서 *classifier는 *첫 번째 임베딩([CLS] 토큰에 해당)만 사용합니다*.\n",
        "\n",
        "또한 BERT는 분류를 위해 이 [CLS] 토큰만 사용하도록 훈련되었기 때문에 모델이 분류 단계에 필요한 모든 것을 단일 768 값 임베딩 벡터로 인코딩하도록 되었습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51v0kFxeteu"
      },
      "source": [
        "### Sentence Length & Attention Mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNuwqZVK3T6"
      },
      "source": [
        "BERT에는 두 가지 제약 조건이 있습니다.\n",
        "1. 모든 문장은 고정된 단일 길이로 채워지거나 잘려야 합니다.\n",
        "2. 최대 문장 길이는 512 토큰입니다.\n",
        "\n",
        "패딩은 BERT 어휘의 인덱스 0에 있는 특수 `[PAD]` 토큰을 사용하여 수행됩니다. 아래 그림은 8개 토큰의 \"MAX_LEN\"에 대한 패딩을 보여줍니다.\n",
        "\n",
        "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
        "\n",
        "\"attention mask\"는 단순히 패딩되는 토큰과 패딩되지 않는 토큰을 나타내는 1과 0의 배열입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ"
      },
      "source": [
        "## 3.2. Sentences to IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV"
      },
      "source": [
        "`tokenizer.encode` 함수는 여러 단계를 결합합니다:\n",
        "1. 문장을 토큰으로 나눕니다.\n",
        "2. 특수 `[CLS]` 및 `[SEP]` 토큰을 추가합니다.\n",
        "3. 토큰을 해당 ID에 매핑합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2bBdb3pt8LuQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwCKszh6ych"
      },
      "source": [
        "## 3.3. Padding & Truncating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0"
      },
      "source": [
        "시퀀스를 모두 채우고 잘라서 길이가 모두 'MAX_LEN'이 되도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqiWTDrn_nGB"
      },
      "source": [
        "First, what's the maximum sentence length in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JhUZO9vc_l6T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ],
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cp9BPRd1tMIo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "source": [
        "## 3.4. Attention Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "어텐션 마스크는 어떤 토큰이 실제 단어인지, 어떤 토큰이 패딩인지를 명확하게 보여줍니다.\n",
        "\n",
        "BERT 어휘는 ID 0을 사용하지 않으므로 토큰 ID가 0이면 패딩이고 그렇지 않으면 실제 토큰입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "## 3.5. Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06"
      },
      "source": [
        "train/test를 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR"
      },
      "source": [
        "## 3.6. Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4"
      },
      "source": [
        "우리 모델은 numpy.ndarrays 대신 PyTorch 텐서를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN"
      },
      "source": [
        "또한 토치 DataLoader 클래스를 사용하여 데이터세트에 대한 반복자를 생성합니다. 이는 for 루프와 달리 반복자를 사용하면 전체 데이터세트를 메모리에 로드할 필요가 없기 때문에 훈련 중에 메모리를 절약하는 데 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# 4. Train Our Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "## 4.1. BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sjzRT1V0zwm"
      },
      "source": [
        "이 작업을 위해 먼저 사전 훈련된 BERT 모델을 수정하여 분류를 위한 출력을 제공한 다음 전체 모델이 엔드투엔드에 적합할 때까지 데이터 세트에서 모델을 계속 훈련하려고 합니다.\n",
        "\n",
        "현재 미세 조정을 위해 Huggingface에서 제공되는 클래스 목록은 다음과 같습니다.\n",
        "* BertModel\n",
        "* BertForPreTraining\n",
        "* BertForMaskedLM\n",
        "* BertForNextSentence예측\n",
        "* **BertForSequenceClassification** -> 우리가 사용할 것입니다.\n",
        "* BertForTokenClassification\n",
        "* BertForQuestionAnswering\n",
        "\n",
        "이에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)에서 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXYitPoE-cjH"
      },
      "source": [
        "우리는 [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)을 사용할 것입니다. 이것은 문장 분류기로 사용할 분류를 위해 상단에 단일 선형 레이어가 추가된 일반 BERT 모델입니다. 입력 데이터를 제공하면 사전 훈련된 전체 BERT 모델과 훈련되지 않은 추가 분류 계층이 특정 작업에 대해 훈련됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQW9E-bBCRt"
      },
      "source": [
        "`from_pretrained` 에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)에 있습니다.\n",
        "추가적인 parameter는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)에 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gFsCTp_mporB"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e60ed3a585b34ba79fbf0d745d359d55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch import optim\n",
        "from torch.optim import AdamW # Corrected import\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8PIiVlDYCtSq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "이제 모델을 로드했으므로 저장된 모델 내에서 훈련 하이퍼파라미터를 가져와야 합니다.\n",
        "\n",
        "미세 조정을 위해 저자는 다음 값 중에서 선택할 것을 권장합니다.\n",
        "- batch size: 16, 32(DataLoader를 생성할 때 32를 선택했습니다).\n",
        "- learning rate(Adam): 5e-5, 3e-5, 2e-5(여기에서는 2e-5를 사용하겠습니다).\n",
        "- epochs: 2, 3, 4(여기에서는 4를 사용합니다).\n",
        "\n",
        "엡실론 매개변수 `eps = 1e-8`은 \"구현 시 0으로 나누는 것을 방지하기 위한 매우 작은 숫자\"입니다([여기](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "'run_glue.py' [여기](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)에서 AdamW 최적화 프로그램 생성을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 4.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "다음은 훈련 루프입니다. 많은 일이 진행되고 있지만 기본적으로 루프의 각 패스에는 트라이닝 단계와 검증 단계가 있습니다. 각 패스에서 우리는 다음을 수행해야 합니다.\n",
        "\n",
        "훈련 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 이전 단계에서 계산된 그래디언트를 지웁니다.\n",
        "     - pytorch에서는 명시적으로 지우지 않는 한 기본적으로 그래디언트가 누적됩니다(RNN과 같은 작업에 유용함).\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 역방향 전달(역전파)\n",
        "- 네트워크에 Optimizer.step()을 사용하여 매개변수를 업데이트하도록 지시합니다.\n",
        "- 진행상황 모니터링을 위한 변수 추적\n",
        "\n",
        "평가 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 검증 데이터의 손실을 계산하고 진행 상황을 모니터링하기 위한 변수를 추적합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6J-FYdx6nFE_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:04.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:14.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:07.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:03.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:06.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:10.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:13.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "68xreA9JAmG5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "test 용 데이터세트를 로드하고 [Matthew의 상관 계수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)를 사용하여 예측을 평가합니다.\n",
        "\n",
        "이 지표를 사용하면 +1이 최고 점수이고 -1이 최저 점수입니다. 이런 식으로 우리는 이 특정 작업에 대한 최신 모델에 비해 우리가 얼마나 잘 수행하는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mAN0LZBOOPVh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hWcy0X1hirdx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cRaZQ4XC7kLs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xytAr_C48wnu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[np.float64(0.049286405809014416),\n",
              " np.float64(-0.1044465935734187),\n",
              " np.float64(0.4040950971038548),\n",
              " np.float64(0.30508307783296046),\n",
              " np.float64(0.5222329678670935),\n",
              " np.float64(0.7410010097502685),\n",
              " np.float64(0.5269860393922079),\n",
              " 0.0,\n",
              " np.float64(1.0),\n",
              " np.float64(0.7530836820370708),\n",
              " np.float64(0.8459051693633014),\n",
              " np.float64(0.647150228929434),\n",
              " np.float64(0.936441710371274),\n",
              " np.float64(0.7141684885491869),\n",
              " np.float64(0.4622501635210242),\n",
              " np.float64(0.5266354311522131),\n",
              " 0.0]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matthews_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oCYZa1lQ8Jn8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCC: 0.561\n"
          ]
        }
      ],
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py311-aws",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
