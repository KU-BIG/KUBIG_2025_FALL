{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un0kSCKKPz_r"
      },
      "source": [
        "# ViT assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hYQBhItP4Em"
      },
      "source": [
        "colab의 경우, 런타임 유형을 GPU로 바꿔주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCaKr-jb-RuF"
      },
      "source": [
        "# 0. Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VGdid66o92_a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from einops import repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0FY_DkY-BOc"
      },
      "source": [
        "# 1. Project input to patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVBEFoV7-a1u",
        "outputId": "c4490455-98f6-4191-daaa-51baec0f81fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Patch embeddings shape: torch.Size([8, 196, 768])\n",
            "Number of patches: 196\n"
          ]
        }
      ],
      "source": [
        "class PatchProjection(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size//patch_size)**2 # 이미지 크기와 패치 크기에 따른 총 패치 수 계산\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # TODO: 패치 임베딩을 위한 핵심 레이어 (Conv2d)와 차원 재배치(Rearrange)를 완성하세요\n",
        "            # 힌트: Conv2d 커널 크기와 스트라이드는 patch_size와 같아야 합니다.\n",
        "            #       Rearrange는 (b, emb_size, h, w) -> (b, h*w, emb_size) 형태로 변환합니다.\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=self.patch_size, stride=self.patch_size), # b x N x N x e\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: 입력 이미지를 patch embedding으로 변환하는 부분 완성\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)\n",
        "    patch_proj = PatchProjection()\n",
        "    out = patch_proj(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Patch embeddings shape: {out.shape}')\n",
        "    print(f'Number of patches: {patch_proj.num_patches}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bs = 8, 196 = 14x14 총 패치 개수, 768 = emb_size"
      ],
      "metadata": {
        "id": "XfbsjYNC0yyB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXVMCsl-rP9"
      },
      "source": [
        "# 2. Patches embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5-Ulu9Z-n_W",
        "outputId": "c2bc2125-2be6-4ca0-bc1b-61004b47fa65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Expected: (8, 197, 768)\n"
          ]
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size//patch_size)**2 # 이미지 크기와 패치 크기에 따른 총 패치 수 계산\n",
        "\n",
        "        # Patch projection\n",
        "        self.projection = nn.Sequential(\n",
        "            # TODO: 패치 임베딩을 위한 핵심 레이어 (Conv2d)와 차원 재배치(Rearrange)를 완성하세요\n",
        "            # 힌트: Conv2d 커널 크기와 스트라이드는 patch_size와 같아야 합니다.\n",
        "            #       Rearrange는 (b, emb_size, h, w) -> (b, h*w, emb_size) 형태로 변환합니다.\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "        )\n",
        "\n",
        "        # CLS token and positional encoding\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn(self.num_patches + 1, emb_size))\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B = x.shape[0]\n",
        "        # print(B)\n",
        "        # print(x.shape)\n",
        "        # Project to patches\n",
        "        # TODO: self.projection을 활용해 patch embedding을 수행하세요.\n",
        "        x = self.projection(x)\n",
        "        # print(x.shape)\n",
        "        # Add CLS token\n",
        "        # TODO: batch 크기에 맞게 cls_token을 확장하고 입력에 연결하세요.\n",
        "\n",
        "        x = torch.cat([self.cls_token.expand(B, -1, -1), x], dim=1)\n",
        "        # Add positional encoding\n",
        "        # TODO: positional encoding을 더하세요.\n",
        "        x = x + self.positions\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)\n",
        "    patch_emb = PatchEmbedding()\n",
        "    out = patch_emb(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Expected: (8, 197, 768)') # 196 patches + 1 CLS token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhR6ukKj-zBJ"
      },
      "source": [
        "# 3. Multi Head Attention (MHA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJTJmUN-yZW",
        "outputId": "19f9a7fd-fa4b-48ee-9f96-a908b06b461c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 2,360,064\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        assert emb_size % num_heads == 0\n",
        "\n",
        "        # Q, K, V projections\n",
        "        # TODO: Q, K, V를 한 번에 만드는 선형레이어와\n",
        "        #       출력 투영 선형레이어를 선언하세요.\n",
        "        #       bias는 qkv에선 False로, proj에선 True(기본)로 둡니다.\n",
        "        self.qkv = nn.Linear(emb_size, 3*emb_size, bias=False)\n",
        "        self.proj = nn.Linear(emb_size, emb_size, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        # TODO: qkv 선형레이어로 Q,K,V 생성 후 (B, N, 3, num_heads, head_dim)으로 reshape 하고,\n",
        "        #       (3, B, num_heads, N, head_dim)으로 permute하여 q,k,v로 분리하세요.\n",
        "        x = self.qkv(x) # B, N, 3 * C\n",
        "        x = x.reshape(B, N, 3, self.num_heads, self.head_dim) # B, N, 3, num_heads, head_dim\n",
        "        q, k, v = x.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "\n",
        "        # Attention computation\n",
        "        # TODO: scaled dot-product attention 계산 후 softmax, dropout 적용\n",
        "        score = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attention = nn.functional.softmax(score, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Apply attention to values\n",
        "        # TODO: attention 결과에 v를 곱하고,\n",
        "        #       (B, N, C) 형태로 reshape 후 proj와 dropout 적용\n",
        "        x = torch.matmul(attention, v)\n",
        "        x = x.reshape(B, N, self.num_heads*self.head_dim)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)  # (batch, patches+cls, emb_size)\n",
        "    mha = MultiHeadAttention()\n",
        "    out = mha(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in mha.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91glGnw--8wB"
      },
      "source": [
        "# 4. Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-OZsyS_-60F",
        "outputId": "484e1914-6364-45cb-db70-12b50bbd69bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 7,085,568\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_size=768, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden_size = int(emb_size * mlp_ratio)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            # TODO: nn.Sequential을 활용해\n",
        "            #       emb_size -> hidden_size -> emb_size 순서의 MLP를 만드세요.\n",
        "            #       중간에 GELU와 Dropout 포함\n",
        "            nn.Linear(emb_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, emb_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # TODO: qkv, proj linear 레이어 선언 및 dropout 선언\n",
        "        # qkv는 bias=False, proj는 기본\n",
        "        self.qkv = nn.Linear(emb_size, 3*emb_size, bias=False)\n",
        "        self.proj = nn.Linear(emb_size, emb_size, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # TODO: qkv 생성, reshape, permute 후 q,k,v 분리\n",
        "        x = self.qkv(x) # B, N, 3 * C\n",
        "        x = x.reshape(B, N, 3, self.num_heads, self.head_dim) # B, N, 3, num_heads, head_dim\n",
        "        q, k, v = x.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # TODO: scaled dot-product attention 계산, softmax, dropout 적용\n",
        "        score = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attention = nn.functional.softmax(score, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # TODO: attention 결과와 v 곱하고, 다시 proj와 dropout 적용\n",
        "        x = torch.matmul(attention, v)\n",
        "        x = x.reshape(B, N, self.num_heads*self.head_dim)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: LayerNorm 2개, MultiHeadAttention, MLP 선언\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
        "        self.mlp = MLP(emb_size, mlp_ratio, dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Pre-norm + residual connection for attention\n",
        "        x = x + self.attention(self.norm1(x))\n",
        "\n",
        "        # Pre-norm + residual connection for MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)\n",
        "    block = TransformerEncoderBlock()\n",
        "    out = block(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in block.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GQ4-g8m_Cae"
      },
      "source": [
        "# 5. Complete ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RWUlqVx_AY1",
        "outputId": "7e7e8898-bfe0-4e24-9dd3-255d8f8c433f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 3, 224, 224])\n",
            "Output shape: torch.Size([2, 1000])\n",
            "Total parameters: 86,540,008\n",
            "\n",
            "=== ViT Configurations ===\n",
            "ViT-Tiny: 5,710,504 parameters\n",
            "ViT-Small: 22,036,840 parameters\n",
            "ViT-Base: 86,540,008 parameters\n",
            "ViT-Large: 304,252,904 parameters\n"
          ]
        }
      ],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        emb_size=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1,\n",
        "        drop_path=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding (from Stage 2)\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        # Transformer encoder blocks (from Stage 4)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "        nn.init.constant_(self.head.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: patch embedding 적용\n",
        "        x = self.patch_embed(x)\n",
        "        # TODO: Transformer encoder blocks 순차 적용\n",
        "        for block in self.blocks:\n",
        "          x = block(x)\n",
        "\n",
        "        # Classification head (use CLS token)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]  # Extract CLS token\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    # ViT-Base configuration\n",
        "    model = VisionTransformer(\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        emb_size=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    x = torch.randn(2, 3, 224, 224)\n",
        "    out = model(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    # Different ViT configurations\n",
        "    print('\\n=== ViT Configurations ===')\n",
        "    configs = {\n",
        "        'ViT-Tiny': {'emb_size': 192, 'depth': 12, 'num_heads': 3},\n",
        "        'ViT-Small': {'emb_size': 384, 'depth': 12, 'num_heads': 6},\n",
        "        'ViT-Base': {'emb_size': 768, 'depth': 12, 'num_heads': 12},\n",
        "        'ViT-Large': {'emb_size': 1024, 'depth': 24, 'num_heads': 16},\n",
        "    }\n",
        "\n",
        "    for name, config in configs.items():\n",
        "        model = VisionTransformer(**config, num_classes=1000)\n",
        "        params = sum(p.numel() for p in model.parameters())\n",
        "        print(f'{name}: {params:,} parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGSRZJkAEuIx"
      },
      "source": [
        "# 6. ViT for CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vAg_ETks8yj"
      },
      "source": [
        "위의 코드를 완성했다면, 아래 코드를 실행하여 전체 모델을 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQerYjbvEt1T",
        "outputId": "cbc2ecee-4188-4985-ea0a-419192bd3d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 32, 32])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Parameters: 4,766,474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/60\n",
            "Batch 0: Loss 2.3263, Acc 16.41%\n",
            "Batch 100: Loss 2.3167, Acc 13.87%\n",
            "Train Loss: 2.3237, Train Acc: 13.71%\n",
            "Test Loss: 2.3238, Test Acc: 14.06%\n",
            "LR: 0.000500, Epoch time: 41.01s\n",
            "\n",
            "Epoch 2/60\n",
            "Batch 0: Loss 2.3406, Acc 8.98%\n",
            "Batch 100: Loss 1.8893, Acc 28.50%\n",
            "Train Loss: 1.9312, Train Acc: 32.82%\n",
            "Test Loss: 1.8170, Test Acc: 38.86%\n",
            "LR: 0.000500, Epoch time: 42.09s\n",
            "\n",
            "Epoch 3/60\n",
            "Batch 0: Loss 1.7813, Acc 41.41%\n",
            "Batch 100: Loss 1.6672, Acc 41.06%\n",
            "Train Loss: 1.7341, Train Acc: 42.29%\n",
            "Test Loss: 1.6684, Test Acc: 44.99%\n",
            "LR: 0.000499, Epoch time: 43.50s\n",
            "\n",
            "Epoch 4/60\n",
            "Batch 0: Loss 1.6971, Acc 44.92%\n",
            "Batch 100: Loss 1.6097, Acc 46.11%\n",
            "Train Loss: 1.6387, Train Acc: 46.86%\n",
            "Test Loss: 1.6238, Test Acc: 48.14%\n",
            "LR: 0.000497, Epoch time: 43.01s\n",
            "\n",
            "Epoch 5/60\n",
            "Batch 0: Loss 1.6417, Acc 46.48%\n",
            "Batch 100: Loss 1.5482, Acc 49.15%\n",
            "Train Loss: 1.5831, Train Acc: 49.78%\n",
            "Test Loss: 1.5526, Test Acc: 51.65%\n",
            "LR: 0.000494, Epoch time: 42.92s\n",
            "\n",
            "Epoch 6/60\n",
            "Batch 0: Loss 1.5742, Acc 51.95%\n",
            "Batch 100: Loss 1.4565, Acc 50.95%\n",
            "Train Loss: 1.5453, Train Acc: 51.47%\n",
            "Test Loss: 1.5054, Test Acc: 52.95%\n",
            "LR: 0.000491, Epoch time: 42.84s\n",
            "\n",
            "Epoch 7/60\n",
            "Batch 0: Loss 1.5470, Acc 51.95%\n",
            "Batch 100: Loss 1.5139, Acc 52.90%\n",
            "Train Loss: 1.5166, Train Acc: 53.15%\n",
            "Test Loss: 1.4797, Test Acc: 54.46%\n",
            "LR: 0.000487, Epoch time: 43.24s\n",
            "\n",
            "Epoch 8/60\n",
            "Batch 0: Loss 1.4817, Acc 51.95%\n",
            "Batch 100: Loss 1.4569, Acc 55.27%\n",
            "Train Loss: 1.4731, Train Acc: 55.03%\n",
            "Test Loss: 1.4842, Test Acc: 54.92%\n",
            "LR: 0.000483, Epoch time: 42.86s\n",
            "\n",
            "Epoch 9/60\n",
            "Batch 0: Loss 1.4005, Acc 59.38%\n",
            "Batch 100: Loss 1.4324, Acc 56.28%\n",
            "Train Loss: 1.4470, Train Acc: 56.27%\n",
            "Test Loss: 1.5028, Test Acc: 53.23%\n",
            "LR: 0.000478, Epoch time: 43.24s\n",
            "\n",
            "Epoch 10/60\n",
            "Batch 0: Loss 1.4165, Acc 57.42%\n",
            "Batch 100: Loss 1.5536, Acc 57.78%\n",
            "Train Loss: 1.4168, Train Acc: 57.84%\n",
            "Test Loss: 1.3958, Test Acc: 59.24%\n",
            "LR: 0.000472, Epoch time: 42.89s\n",
            "\n",
            "Epoch 11/60\n",
            "Batch 0: Loss 1.3424, Acc 60.94%\n",
            "Batch 100: Loss 1.3820, Acc 59.46%\n",
            "Train Loss: 1.3826, Train Acc: 59.62%\n",
            "Test Loss: 1.4132, Test Acc: 58.53%\n",
            "LR: 0.000465, Epoch time: 42.85s\n",
            "\n",
            "Epoch 12/60\n",
            "Batch 0: Loss 1.2814, Acc 66.80%\n",
            "Batch 100: Loss 1.4335, Acc 60.54%\n",
            "Train Loss: 1.3601, Train Acc: 60.65%\n",
            "Test Loss: 1.3892, Test Acc: 59.49%\n",
            "LR: 0.000458, Epoch time: 43.07s\n",
            "\n",
            "Epoch 13/60\n",
            "Batch 0: Loss 1.3441, Acc 62.50%\n",
            "Batch 100: Loss 1.2037, Acc 61.49%\n",
            "Train Loss: 1.3340, Train Acc: 61.76%\n",
            "Test Loss: 1.3507, Test Acc: 60.66%\n",
            "LR: 0.000451, Epoch time: 42.96s\n",
            "\n",
            "Epoch 14/60\n",
            "Batch 0: Loss 1.2490, Acc 61.33%\n",
            "Batch 100: Loss 1.3249, Acc 62.75%\n",
            "Train Loss: 1.3041, Train Acc: 63.25%\n",
            "Test Loss: 1.3200, Test Acc: 62.87%\n",
            "LR: 0.000442, Epoch time: 42.84s\n",
            "\n",
            "Epoch 15/60\n",
            "Batch 0: Loss 1.2386, Acc 65.62%\n",
            "Batch 100: Loss 1.3884, Acc 64.68%\n",
            "Train Loss: 1.2782, Train Acc: 64.66%\n",
            "Test Loss: 1.2902, Test Acc: 63.32%\n",
            "LR: 0.000434, Epoch time: 43.39s\n",
            "\n",
            "Epoch 16/60\n",
            "Batch 0: Loss 1.2506, Acc 60.55%\n",
            "Batch 100: Loss 1.2642, Acc 65.70%\n",
            "Train Loss: 1.2518, Train Acc: 65.92%\n",
            "Test Loss: 1.2489, Test Acc: 65.84%\n",
            "LR: 0.000424, Epoch time: 42.93s\n",
            "\n",
            "Epoch 17/60\n",
            "Batch 0: Loss 1.1887, Acc 69.14%\n",
            "Batch 100: Loss 1.2461, Acc 66.96%\n",
            "Train Loss: 1.2308, Train Acc: 66.91%\n",
            "Test Loss: 1.2763, Test Acc: 64.63%\n",
            "LR: 0.000415, Epoch time: 42.71s\n",
            "\n",
            "Epoch 18/60\n",
            "Batch 0: Loss 1.2557, Acc 65.62%\n",
            "Batch 100: Loss 1.0776, Acc 67.40%\n",
            "Train Loss: 1.2095, Train Acc: 67.64%\n",
            "Test Loss: 1.2528, Test Acc: 66.06%\n",
            "LR: 0.000404, Epoch time: 43.49s\n",
            "\n",
            "Epoch 19/60\n",
            "Batch 0: Loss 1.2470, Acc 67.58%\n",
            "Batch 100: Loss 1.2307, Acc 68.31%\n",
            "Train Loss: 1.1920, Train Acc: 68.62%\n",
            "Test Loss: 1.2360, Test Acc: 67.19%\n",
            "LR: 0.000394, Epoch time: 43.02s\n",
            "\n",
            "Epoch 20/60\n",
            "Batch 0: Loss 1.2015, Acc 69.14%\n",
            "Batch 100: Loss 1.1881, Acc 69.36%\n",
            "Train Loss: 1.1695, Train Acc: 69.55%\n",
            "Test Loss: 1.1976, Test Acc: 68.46%\n",
            "LR: 0.000383, Epoch time: 42.75s\n",
            "\n",
            "Epoch 21/60\n",
            "Batch 0: Loss 1.1281, Acc 73.44%\n",
            "Batch 100: Loss 1.1312, Acc 70.20%\n",
            "Train Loss: 1.1530, Train Acc: 70.28%\n",
            "Test Loss: 1.2096, Test Acc: 68.56%\n",
            "LR: 0.000371, Epoch time: 43.50s\n",
            "\n",
            "Epoch 22/60\n",
            "Batch 0: Loss 1.0178, Acc 78.91%\n",
            "Batch 100: Loss 1.0624, Acc 71.16%\n",
            "Train Loss: 1.1385, Train Acc: 71.09%\n",
            "Test Loss: 1.1771, Test Acc: 69.55%\n",
            "LR: 0.000359, Epoch time: 42.99s\n",
            "\n",
            "Epoch 23/60\n",
            "Batch 0: Loss 1.0419, Acc 74.61%\n",
            "Batch 100: Loss 1.1558, Acc 72.20%\n",
            "Train Loss: 1.1194, Train Acc: 72.16%\n",
            "Test Loss: 1.1720, Test Acc: 69.93%\n",
            "LR: 0.000347, Epoch time: 43.28s\n",
            "\n",
            "Epoch 24/60\n",
            "Batch 0: Loss 1.1481, Acc 69.14%\n",
            "Batch 100: Loss 1.1330, Acc 72.67%\n",
            "Train Loss: 1.1041, Train Acc: 72.68%\n",
            "Test Loss: 1.1686, Test Acc: 70.07%\n",
            "LR: 0.000335, Epoch time: 43.31s\n",
            "\n",
            "Epoch 25/60\n",
            "Batch 0: Loss 1.1390, Acc 72.66%\n",
            "Batch 100: Loss 1.1434, Acc 73.34%\n",
            "Train Loss: 1.0875, Train Acc: 73.37%\n",
            "Test Loss: 1.1356, Test Acc: 71.24%\n",
            "LR: 0.000322, Epoch time: 42.75s\n",
            "\n",
            "Epoch 26/60\n",
            "Batch 0: Loss 1.0271, Acc 75.00%\n",
            "Batch 100: Loss 1.1147, Acc 74.11%\n",
            "Train Loss: 1.0691, Train Acc: 74.32%\n",
            "Test Loss: 1.1344, Test Acc: 71.38%\n",
            "LR: 0.000309, Epoch time: 42.99s\n",
            "\n",
            "Epoch 27/60\n",
            "Batch 0: Loss 1.0320, Acc 76.17%\n",
            "Batch 100: Loss 1.0591, Acc 75.40%\n",
            "Train Loss: 1.0535, Train Acc: 74.93%\n",
            "Test Loss: 1.1229, Test Acc: 72.42%\n",
            "LR: 0.000296, Epoch time: 42.99s\n",
            "\n",
            "Epoch 28/60\n",
            "Batch 0: Loss 1.0110, Acc 75.78%\n",
            "Batch 100: Loss 1.0706, Acc 75.60%\n",
            "Train Loss: 1.0388, Train Acc: 75.61%\n",
            "Test Loss: 1.1164, Test Acc: 72.98%\n",
            "LR: 0.000283, Epoch time: 42.80s\n",
            "\n",
            "Epoch 29/60\n",
            "Batch 0: Loss 1.0066, Acc 75.39%\n",
            "Batch 100: Loss 1.0821, Acc 76.55%\n",
            "Train Loss: 1.0191, Train Acc: 76.38%\n",
            "Test Loss: 1.1346, Test Acc: 71.99%\n",
            "LR: 0.000270, Epoch time: 43.55s\n",
            "\n",
            "Epoch 30/60\n",
            "Batch 0: Loss 1.0693, Acc 70.70%\n",
            "Batch 100: Loss 0.9982, Acc 77.20%\n",
            "Train Loss: 1.0043, Train Acc: 77.24%\n",
            "Test Loss: 1.1338, Test Acc: 72.37%\n",
            "LR: 0.000257, Epoch time: 43.03s\n",
            "\n",
            "Epoch 31/60\n",
            "Batch 0: Loss 1.0067, Acc 77.73%\n",
            "Batch 100: Loss 0.9633, Acc 77.85%\n",
            "Train Loss: 0.9894, Train Acc: 77.76%\n",
            "Test Loss: 1.0855, Test Acc: 74.26%\n",
            "LR: 0.000243, Epoch time: 43.04s\n",
            "\n",
            "Epoch 32/60\n",
            "Batch 0: Loss 0.9736, Acc 78.52%\n",
            "Batch 100: Loss 1.0785, Acc 78.70%\n",
            "Train Loss: 0.9756, Train Acc: 78.60%\n",
            "Test Loss: 1.0810, Test Acc: 74.61%\n",
            "LR: 0.000230, Epoch time: 43.24s\n",
            "\n",
            "Epoch 33/60\n",
            "Batch 0: Loss 0.9394, Acc 79.69%\n",
            "Batch 100: Loss 0.9992, Acc 79.38%\n",
            "Train Loss: 0.9568, Train Acc: 79.39%\n",
            "Test Loss: 1.0945, Test Acc: 73.78%\n",
            "LR: 0.000217, Epoch time: 43.00s\n",
            "\n",
            "Epoch 34/60\n",
            "Batch 0: Loss 0.9509, Acc 80.08%\n",
            "Batch 100: Loss 0.9659, Acc 80.13%\n",
            "Train Loss: 0.9457, Train Acc: 79.94%\n",
            "Test Loss: 1.0765, Test Acc: 75.03%\n",
            "LR: 0.000204, Epoch time: 42.74s\n",
            "\n",
            "Epoch 35/60\n",
            "Batch 0: Loss 0.9156, Acc 80.08%\n",
            "Batch 100: Loss 0.9817, Acc 80.56%\n",
            "Train Loss: 0.9328, Train Acc: 80.49%\n",
            "Test Loss: 1.0867, Test Acc: 74.62%\n",
            "LR: 0.000191, Epoch time: 43.57s\n",
            "\n",
            "Epoch 36/60\n",
            "Batch 0: Loss 0.8911, Acc 82.03%\n",
            "Batch 100: Loss 0.8779, Acc 81.10%\n",
            "Train Loss: 0.9209, Train Acc: 81.02%\n",
            "Test Loss: 1.0752, Test Acc: 75.04%\n",
            "LR: 0.000178, Epoch time: 43.02s\n",
            "\n",
            "Epoch 37/60\n",
            "Batch 0: Loss 0.8340, Acc 85.55%\n",
            "Batch 100: Loss 0.8703, Acc 82.26%\n",
            "Train Loss: 0.9067, Train Acc: 81.93%\n",
            "Test Loss: 1.0982, Test Acc: 74.79%\n",
            "LR: 0.000165, Epoch time: 42.81s\n",
            "\n",
            "Epoch 38/60\n",
            "Batch 0: Loss 0.9100, Acc 80.47%\n",
            "Batch 100: Loss 0.8381, Acc 82.56%\n",
            "Train Loss: 0.8944, Train Acc: 82.34%\n",
            "Test Loss: 1.0787, Test Acc: 75.33%\n",
            "LR: 0.000153, Epoch time: 43.55s\n",
            "\n",
            "Epoch 39/60\n",
            "Batch 0: Loss 0.9526, Acc 78.91%\n",
            "Batch 100: Loss 0.8633, Acc 82.90%\n",
            "Train Loss: 0.8844, Train Acc: 82.77%\n",
            "Test Loss: 1.0658, Test Acc: 75.68%\n",
            "LR: 0.000141, Epoch time: 42.98s\n",
            "\n",
            "Epoch 40/60\n",
            "Batch 0: Loss 0.8613, Acc 83.59%\n",
            "Batch 100: Loss 0.8499, Acc 83.83%\n",
            "Train Loss: 0.8704, Train Acc: 83.46%\n",
            "Test Loss: 1.0799, Test Acc: 75.28%\n",
            "LR: 0.000129, Epoch time: 42.99s\n",
            "\n",
            "Epoch 41/60\n",
            "Batch 0: Loss 0.8160, Acc 86.72%\n",
            "Batch 100: Loss 0.9047, Acc 84.20%\n",
            "Train Loss: 0.8602, Train Acc: 83.92%\n",
            "Test Loss: 1.0603, Test Acc: 76.56%\n",
            "LR: 0.000117, Epoch time: 43.59s\n",
            "\n",
            "Epoch 42/60\n",
            "Batch 0: Loss 0.7892, Acc 86.33%\n",
            "Batch 100: Loss 0.8666, Acc 84.70%\n",
            "Train Loss: 0.8500, Train Acc: 84.39%\n",
            "Test Loss: 1.0748, Test Acc: 75.92%\n",
            "LR: 0.000106, Epoch time: 43.28s\n",
            "\n",
            "Epoch 43/60\n",
            "Batch 0: Loss 0.7927, Acc 87.50%\n",
            "Batch 100: Loss 0.8596, Acc 85.07%\n",
            "Train Loss: 0.8379, Train Acc: 84.85%\n",
            "Test Loss: 1.0780, Test Acc: 76.11%\n",
            "LR: 0.000096, Epoch time: 42.74s\n",
            "\n",
            "Epoch 44/60\n",
            "Batch 0: Loss 0.8492, Acc 86.72%\n",
            "Batch 100: Loss 0.8048, Acc 85.80%\n",
            "Train Loss: 0.8264, Train Acc: 85.53%\n",
            "Test Loss: 1.0802, Test Acc: 75.84%\n",
            "LR: 0.000085, Epoch time: 43.42s\n",
            "\n",
            "Epoch 45/60\n",
            "Batch 0: Loss 0.8475, Acc 86.33%\n",
            "Batch 100: Loss 0.8008, Acc 86.35%\n",
            "Train Loss: 0.8153, Train Acc: 86.12%\n",
            "Test Loss: 1.0651, Test Acc: 76.55%\n",
            "LR: 0.000076, Epoch time: 42.92s\n",
            "\n",
            "Epoch 46/60\n",
            "Batch 0: Loss 0.8401, Acc 85.55%\n",
            "Batch 100: Loss 0.8226, Acc 86.58%\n",
            "Train Loss: 0.8069, Train Acc: 86.50%\n",
            "Test Loss: 1.0700, Test Acc: 76.62%\n",
            "LR: 0.000066, Epoch time: 42.89s\n",
            "\n",
            "Epoch 47/60\n",
            "Batch 0: Loss 0.7976, Acc 85.94%\n",
            "Batch 100: Loss 0.8065, Acc 86.66%\n",
            "Train Loss: 0.8013, Train Acc: 86.58%\n",
            "Test Loss: 1.0728, Test Acc: 76.69%\n",
            "LR: 0.000058, Epoch time: 43.23s\n",
            "\n",
            "Epoch 48/60\n",
            "Batch 0: Loss 0.7974, Acc 86.33%\n",
            "Batch 100: Loss 0.7588, Acc 87.19%\n",
            "Train Loss: 0.7921, Train Acc: 87.13%\n",
            "Test Loss: 1.0563, Test Acc: 77.16%\n",
            "LR: 0.000049, Epoch time: 43.13s\n",
            "\n",
            "Epoch 49/60\n",
            "Batch 0: Loss 0.7916, Acc 86.72%\n",
            "Batch 100: Loss 0.7699, Acc 87.58%\n",
            "Train Loss: 0.7828, Train Acc: 87.58%\n",
            "Test Loss: 1.0700, Test Acc: 76.69%\n",
            "LR: 0.000042, Epoch time: 42.84s\n",
            "\n",
            "Epoch 50/60\n",
            "Batch 0: Loss 0.7958, Acc 87.11%\n",
            "Batch 100: Loss 0.7647, Acc 87.77%\n",
            "Train Loss: 0.7770, Train Acc: 87.76%\n",
            "Test Loss: 1.0616, Test Acc: 76.94%\n",
            "LR: 0.000035, Epoch time: 43.24s\n",
            "\n",
            "Epoch 51/60\n",
            "Batch 0: Loss 0.7918, Acc 83.98%\n",
            "Batch 100: Loss 0.7818, Acc 87.93%\n",
            "Train Loss: 0.7722, Train Acc: 88.00%\n",
            "Test Loss: 1.0629, Test Acc: 77.16%\n",
            "LR: 0.000028, Epoch time: 42.74s\n",
            "\n",
            "Epoch 52/60\n",
            "Batch 0: Loss 0.7805, Acc 89.06%\n",
            "Batch 100: Loss 0.7054, Acc 88.13%\n",
            "Train Loss: 0.7679, Train Acc: 88.15%\n",
            "Test Loss: 1.0631, Test Acc: 77.39%\n",
            "LR: 0.000022, Epoch time: 43.48s\n",
            "\n",
            "Epoch 53/60\n",
            "Batch 0: Loss 0.7445, Acc 89.45%\n",
            "Batch 100: Loss 0.7767, Acc 88.66%\n",
            "Train Loss: 0.7620, Train Acc: 88.42%\n",
            "Test Loss: 1.0671, Test Acc: 76.99%\n",
            "LR: 0.000017, Epoch time: 42.80s\n",
            "\n",
            "Epoch 54/60\n",
            "Batch 0: Loss 0.7294, Acc 91.41%\n",
            "Batch 100: Loss 0.7797, Acc 88.74%\n",
            "Train Loss: 0.7583, Train Acc: 88.57%\n",
            "Test Loss: 1.0677, Test Acc: 76.97%\n",
            "LR: 0.000013, Epoch time: 42.79s\n",
            "\n",
            "Epoch 55/60\n",
            "Batch 0: Loss 0.7704, Acc 88.67%\n",
            "Batch 100: Loss 0.7511, Acc 88.86%\n",
            "Train Loss: 0.7536, Train Acc: 88.83%\n",
            "Test Loss: 1.0693, Test Acc: 77.04%\n",
            "LR: 0.000009, Epoch time: 43.65s\n",
            "\n",
            "Epoch 56/60\n",
            "Batch 0: Loss 0.7087, Acc 92.19%\n",
            "Batch 100: Loss 0.7464, Acc 89.17%\n",
            "Train Loss: 0.7545, Train Acc: 88.84%\n",
            "Test Loss: 1.0643, Test Acc: 77.36%\n",
            "LR: 0.000006, Epoch time: 42.83s\n",
            "\n",
            "Epoch 57/60\n",
            "Batch 0: Loss 0.7369, Acc 87.11%\n",
            "Batch 100: Loss 0.7269, Acc 88.96%\n",
            "Train Loss: 0.7495, Train Acc: 88.97%\n",
            "Test Loss: 1.0648, Test Acc: 77.25%\n",
            "LR: 0.000003, Epoch time: 42.64s\n",
            "\n",
            "Epoch 58/60\n",
            "Batch 0: Loss 0.6732, Acc 92.97%\n",
            "Batch 100: Loss 0.7097, Acc 89.17%\n",
            "Train Loss: 0.7465, Train Acc: 89.23%\n",
            "Test Loss: 1.0642, Test Acc: 77.31%\n",
            "LR: 0.000001, Epoch time: 43.52s\n",
            "\n",
            "Epoch 59/60\n",
            "Batch 0: Loss 0.7126, Acc 90.23%\n",
            "Batch 100: Loss 0.8211, Acc 89.10%\n",
            "Train Loss: 0.7497, Train Acc: 89.04%\n",
            "Test Loss: 1.0651, Test Acc: 77.34%\n",
            "LR: 0.000000, Epoch time: 42.83s\n",
            "\n",
            "Epoch 60/60\n",
            "Batch 0: Loss 0.7118, Acc 91.41%\n",
            "Batch 100: Loss 0.7292, Acc 88.90%\n",
            "Train Loss: 0.7484, Train Acc: 88.99%\n",
            "Test Loss: 1.0656, Test Acc: 77.25%\n",
            "LR: 0.000000, Epoch time: 42.89s\n",
            "\n",
            "Best Test Accuracy: 77.39%\n"
          ]
        }
      ],
      "source": [
        "class ViTCIFAR10(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if hasattr(self.patch_embed, 'cls_token'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.cls_token, std=0.02)\n",
        "        if hasattr(self.patch_embed, 'positions'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}: Loss {loss.item():.4f}, Acc {100.*correct/total:.2f}%')\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = ViTCIFAR10(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=4,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=0.03,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    num_epochs = 60\n",
        "    warmup_epochs = 1\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "        for _ in range(len(train_loader)):\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "        print(f'LR: {current_lr:.6f}, Epoch time: {epoch_time:.2f}s')\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'vit_cifar10_best.pth')\n",
        "\n",
        "        if test_acc > 90.0:\n",
        "            print(f\"Reached target accuracy!\")\n",
        "            break\n",
        "\n",
        "    print(f'\\nBest Test Accuracy: {best_acc:.2f}%')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = ViTCIFAR10(emb_size=256, depth=6, num_heads=4)\n",
        "    x = torch.randn(4, 3, 32, 32)\n",
        "    out = model(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWvZXNO8Q_-Q"
      },
      "source": [
        "ViT는 일반적으로 대규모 데이터셋에서 사전 학습된(pretrained) 모델을 활용하는 경우가 많기 때문에, 하이퍼파라미터를 조정하거나 학습 epoch을 늘리면 성능이 개선될 수는 있지만, 소규모 데이터셋에서 처음부터 학습한 ViT의 성능이 낮은 것은 구조적 한계에 가깝습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wM3TeYu8Gm8x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}