{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un0kSCKKPz_r"
      },
      "source": [
        "# ViT assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hYQBhItP4Em"
      },
      "source": [
        "colab의 경우, 런타임 유형을 GPU로 바꿔주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCaKr-jb-RuF"
      },
      "source": [
        "# 0. Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGdid66o92_a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from einops import repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0FY_DkY-BOc"
      },
      "source": [
        "# 1. Project input to patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVBEFoV7-a1u",
        "outputId": "88df77c6-73fe-444d-c6e6-0df07735ac51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Patch embeddings shape: torch.Size([8, 196, 768])\n",
            "Number of patches: 196\n"
          ]
        }
      ],
      "source": [
        "class PatchProjection(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2 #이미지 크기와 패치 크기에 따른 총 패치 수 계산\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # TODO: 패치 임베딩을 위한 핵심 레이어 (Conv2d)와 차원 재배치(Rearrange)를 완성하세요\n",
        "            # 힌트: Conv2d 커널 크기와 스트라이드는 patch_size와 같아야 합니다.\n",
        "            #       Rearrange는 (b, emb_size, h, w) -> (b, h*w, emb_size) 형태로 변환합니다.\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: 입력 이미지를 patch embedding으로 변환하는 부분 완성\n",
        "        return self.projection(x)\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)\n",
        "    patch_proj = PatchProjection()\n",
        "    out = patch_proj(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Patch embeddings shape: {out.shape}')\n",
        "    print(f'Number of patches: {patch_proj.num_patches}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXVMCsl-rP9"
      },
      "source": [
        "# 2. Patches embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5-Ulu9Z-n_W",
        "outputId": "78577e7a-061e-4c12-a2db-50b18c2db097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Expected: (8, 197, 768)\n"
          ]
        }
      ],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2 # 이미지 크기와 패치 크기에 따른 총 패치 수 계산\n",
        "        \n",
        "        # Patch projection\n",
        "        self.projection = nn.Sequential(\n",
        "            # TODO: 패치 임베딩을 위한 핵심 레이어 (Conv2d)와 차원 재배치(Rearrange)를 완성하세요\n",
        "            # 힌트: Conv2d 커널 크기와 스트라이드는 patch_size와 같아야 합니다.\n",
        "            #       Rearrange는 (b, emb_size, h, w) -> (b, h*w, emb_size) 형태로 변환합니다.\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e h w -> b (h w) e')\n",
        "        )\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "        # CLS token and positional encoding\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn(self.num_patches + 1, emb_size))\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Project to patches\n",
        "        # TODO: self.projection을 활용해 patch embedding을 수행하세요.\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Add CLS token\n",
        "        # TODO: batch 크기에 맞게 cls_token을 확장하고 입력에 연결하세요.\n",
        "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=B)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        # Add positional encoding\n",
        "        # TODO: positional encoding을 더하세요.\n",
        "        x += self.positions\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)\n",
        "    patch_emb = PatchEmbedding()\n",
        "    out = patch_emb(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Expected: (8, 197, 768)') # 196 patches + 1 CLS token\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhR6ukKj-zBJ"
      },
      "source": [
        "# 3. Multi Head Attention (MHA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJTJmUN-yZW",
        "outputId": "7aea79c1-ad3d-43ae-934e-c74736b7a9b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 2,360,064\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        assert emb_size % num_heads == 0\n",
        "\n",
        "        # Q, K, V projections\n",
        "        # TODO: Q, K, V를 한 번에 만드는 선형레이어와\n",
        "        #       출력 투영 선형레이어를 선언하세요.\n",
        "        #       bias는 qkv에선 False로, proj에선 True(기본)로 둡니다.\n",
        "        self.qkv = nn.Linear(emb_size, 3 * emb_size, bias=False)\n",
        "        self.proj = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        # TODO: qkv 선형레이어로 Q,K,V 생성 후 (B, N, 3, num_heads, head_dim)으로 reshape 하고,\n",
        "        #       (3, B, num_heads, N, head_dim)으로 permute하여 q,k,v로 분리하세요.\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        # Attention computation\n",
        "        # TODO: scaled dot-product attention 계산 후 softmax, dropout 적용\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = nn.Dropout(0,1)(attn)\n",
        "        # Apply attention to values\n",
        "        # TODO: attention 결과에 v를 곱하고,\n",
        "        #       (B, N, C) 형태로 reshape 후 proj와 dropout 적용\n",
        "        x = attn @ v\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = nn.Dropout(0,1)(self.proj(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)  # (batch, patches+cls, emb_size)\n",
        "    mha = MultiHeadAttention()\n",
        "    out = mha(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in mha.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91glGnw--8wB"
      },
      "source": [
        "# 4. Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-OZsyS_-60F",
        "outputId": "84aee5ab-6448-43a5-fa34-c53e740c1f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 7,085,568\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_size=768, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden_size = int(emb_size * mlp_ratio)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            # TODO: nn.Sequential을 활용해\n",
        "            #       emb_size -> hidden_size -> emb_size 순서의 MLP를 만드세요.\n",
        "            #       중간에 GELU와 Dropout 포함\n",
        "            nn.Linear(emb_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, emb_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        # TODO: qkv, proj linear 레이어 선언 및 dropout 선언\n",
        "        # qkv는 bias=False, proj는 기본\n",
        "        self.qkv = nn.Linear(emb_size, 3 * emb_size, bias=False)\n",
        "        self.proj = nn.Linear(emb_size, emb_size, bias=True)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # TODO: qkv 생성, reshape, permute 후 q,k,v 분리\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        # TODO: scaled dot-product attention 계산, softmax, dropout 적용\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = nn.Dropout(0.1)(attn)\n",
        "        # TODO: attention 결과와 v 곱하고, 다시 proj와 dropout 적용\n",
        "        x = attn @ v\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = nn.Dropout(0.1)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: LayerNorm 2개, MultiHeadAttention, MLP 선언\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
        "        self.mlp = MLP(emb_size, mlp_ratio, dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Pre-norm + residual connection for attention\n",
        "        x = x + self.attention(self.norm1(x))\n",
        "\n",
        "        # Pre-norm + residual connection for MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)\n",
        "    block = TransformerEncoderBlock()\n",
        "    out = block(x)\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in block.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GQ4-g8m_Cae"
      },
      "source": [
        "# 5. Complete ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RWUlqVx_AY1",
        "outputId": "2ccb0c7a-a499-438b-f4fe-824eff1f2ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([2, 3, 224, 224])\n",
            "Output shape: torch.Size([2, 1000])\n",
            "Total parameters: 86,540,008\n",
            "\n",
            "=== ViT Configurations ===\n",
            "ViT-Tiny: 5,710,504 parameters\n",
            "ViT-Small: 22,036,840 parameters\n",
            "ViT-Base: 86,540,008 parameters\n",
            "ViT-Large: 304,252,904 parameters\n"
          ]
        }
      ],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        emb_size=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1,\n",
        "        drop_path=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding (from Stage 2)\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        # Transformer encoder blocks (from Stage 4)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "        nn.init.constant_(self.head.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: patch embedding 적용\n",
        "        x = self.patch_embed(x)\n",
        "        # TODO: Transformer encoder blocks 순차 적용\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        # Classification head (use CLS token)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]  # Extract CLS token\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    # ViT-Base configuration\n",
        "    model = VisionTransformer(\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        emb_size=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    x = torch.randn(2, 3, 224, 224)\n",
        "    out = model(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    # Different ViT configurations\n",
        "    print('\\n=== ViT Configurations ===')\n",
        "    configs = {\n",
        "        'ViT-Tiny': {'emb_size': 192, 'depth': 12, 'num_heads': 3},\n",
        "        'ViT-Small': {'emb_size': 384, 'depth': 12, 'num_heads': 6},\n",
        "        'ViT-Base': {'emb_size': 768, 'depth': 12, 'num_heads': 12},\n",
        "        'ViT-Large': {'emb_size': 1024, 'depth': 24, 'num_heads': 16},\n",
        "    }\n",
        "\n",
        "    for name, config in configs.items():\n",
        "        model = VisionTransformer(**config, num_classes=1000)\n",
        "        params = sum(p.numel() for p in model.parameters())\n",
        "        print(f'{name}: {params:,} parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGSRZJkAEuIx"
      },
      "source": [
        "# 6. ViT for CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 코드를 완성했다면, 아래 코드를 실행하여 전체 모델을 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQerYjbvEt1T",
        "outputId": "f208763f-1008-4d27-eec8-3fb703589437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 3, 32, 32])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Parameters: 4,766,474\n",
            "\n",
            "Epoch 1/30\n",
            "Batch 0: Loss 2.3730, Acc 9.38%\n"
          ]
        }
      ],
      "source": [
        "class ViTCIFAR10(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if hasattr(self.patch_embed, 'cls_token'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.cls_token, std=0.02)\n",
        "        if hasattr(self.patch_embed, 'positions'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}: Loss {loss.item():.4f}, Acc {100.*correct/total:.2f}%')\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = ViTCIFAR10(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=4,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=0.03,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    num_epochs = 30\n",
        "    warmup_epochs = 1\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "        for _ in range(len(train_loader)):\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "        print(f'LR: {current_lr:.6f}, Epoch time: {epoch_time:.2f}s')\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'vit_cifar10_best.pth')\n",
        "\n",
        "        if test_acc > 90.0:\n",
        "            print(f\"Reached target accuracy!\")\n",
        "            break\n",
        "\n",
        "    print(f'\\nBest Test Accuracy: {best_acc:.2f}%')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = ViTCIFAR10(emb_size=256, depth=6, num_heads=4)\n",
        "    x = torch.randn(4, 3, 32, 32)\n",
        "    out = model(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM3TeYu8Gm8x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
