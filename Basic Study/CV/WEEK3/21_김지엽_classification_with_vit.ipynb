{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un0kSCKKPz_r"
      },
      "source": [
        "# ViT assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hYQBhItP4Em"
      },
      "source": [
        "colab의 경우, 런타임 유형을 GPU로 바꿔주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCaKr-jb-RuF"
      },
      "source": [
        "# 0. Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VGdid66o92_a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from einops import repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch import Tensor\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0FY_DkY-BOc"
      },
      "source": [
        "# 1. Project input to patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVBEFoV7-a1u",
        "outputId": "a1407ab3-511f-4130-8699-ebb791c84615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Patch embeddings shape: torch.Size([8, 196, 768])\n",
            "Number of patches: 196\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from einops.layers.torch import Rearrange  # 패치 차원 재배치를 위해 필요\n",
        "\n",
        "class PatchProjection(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # (이미지 한 변 // 패치 크기)^2 = 총 패치 수\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # Conv2d: 입력 이미지를 patch 단위로 자르고, 각 patch를 emb_size 차원으로 매핑\n",
        "            nn.Conv2d(\n",
        "                in_channels,              # 입력 채널 수 (RGB면 3)\n",
        "                emb_size,                 # 출력 채널 수 = 임베딩 차원\n",
        "                kernel_size=patch_size,   # 패치 크기만큼 자름\n",
        "                stride=patch_size         # 겹치지 않게 stride도 동일하게\n",
        "            ),\n",
        "            # Rearrange: (B, emb_size, H', W') → (B, num_patches, emb_size)\n",
        "            Rearrange('b e h w -> b (h w) e')  # 패치 차원(h*w)을 flatten\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Xavier 초기화는 일반적으로 Linear/Conv에 사용됨\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # 입력 x: (B, C, H, W) → projection → (B, num_patches, emb_size)\n",
        "        return self.projection(x)\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)  # 배치 크기 8, 3채널, 224x224 이미지\n",
        "    patch_proj = PatchProjection()\n",
        "    out = patch_proj(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')  # (8, 3, 224, 224)\n",
        "    print(f'Patch embeddings shape: {out.shape}')  # (8, 196, 768) expected\n",
        "    print(f'Number of patches: {patch_proj.num_patches}')  # 14 * 14 = 196"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XXVMCsl-rP9"
      },
      "source": [
        "# 2. Patches embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5-Ulu9Z-n_W",
        "outputId": "8a257b0c-df67-4d09-faa7-7ca13d15f9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 3, 224, 224])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Expected: (8, 197, 768)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "\n",
        "        # 이미지 크기를 patch_size로 나눈 후 전체 패치 수 계산\n",
        "        self.num_patches = (img_size // patch_size) ** 2  # 14x14 = 196 patches for 224x224\n",
        "\n",
        "        # Patch projection: Conv2d로 패치 추출 후 Rearrange로 flatten\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, emb_size,\n",
        "                kernel_size=patch_size, stride=patch_size  # 패치 단위로 분할\n",
        "            ),\n",
        "            Rearrange('b e h w -> b (h w) e')  # (B, emb_size, H, W) → (B, num_patches, emb_size)\n",
        "        )\n",
        "\n",
        "        # [CLS] 토큰 (learnable)과 positional encoding (num_patches + 1 for CLS)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))  # (1, 1, D)\n",
        "        self.positions = nn.Parameter(torch.randn(self.num_patches + 1, emb_size))  # (197, D)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # [CLS] 토큰과 포지셔널 임베딩 초기화 (truncated normal)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B = x.shape[0]  # 배치 크기\n",
        "\n",
        "        # 1. 이미지 → 패치 임베딩 (B, 196, 768)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # 2. CLS 토큰을 배치 크기에 맞게 복제 (B, 1, 768)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        # 3. CLS 토큰을 앞에 붙여서 (B, 197, 768)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # 4. 포지셔널 임베딩 더하기 (broadcast됨)\n",
        "        x = x + self.positions  # (B, 197, 768)\n",
        "\n",
        "        return x  # shape: (B, 197, emb_size)\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 3, 224, 224)\n",
        "    patch_emb = PatchEmbedding()\n",
        "    out = patch_emb(x)\n",
        "    print(f'Input shape: {x.shape}')    # (8, 3, 224, 224)\n",
        "    print(f'Output shape: {out.shape}')  # (8, 197, 768)\n",
        "    print(f'Expected: (8, 197, 768)')    # 196 patches + 1 CLS token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhR6ukKj-zBJ"
      },
      "source": [
        "# 3. Multi Head Attention (MHA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJTJmUN-yZW",
        "outputId": "af209293-115c-400c-d6b0-98d670a3a4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 2,360,064\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads  # 각 head당 차원 수\n",
        "        self.scale = self.head_dim ** -0.5     # Scaled dot-product 시 나눌 값\n",
        "\n",
        "        assert emb_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
        "\n",
        "        # Q, K, V를 한 번에 생성하는 Linear 레이어 (bias=False)\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3, bias=False)\n",
        "\n",
        "        # Output projection (attention 결과를 통합 후 projection)\n",
        "        self.proj = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "        # Dropout 레이어\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # QKV의 가중치 초기화 (xavier)\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        # 출력 projection 가중치 초기화\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape  # B: batch, N: tokens, C: embedding dim\n",
        "\n",
        "        # 1. Q, K, V 생성: (B, N, 3 * C)\n",
        "        qkv = self.qkv(x)\n",
        "\n",
        "        # 2. Reshape: (B, N, 3, num_heads, head_dim)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "\n",
        "        # 3. Permute: (3, B, num_heads, N, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # 4. q, k, v 각각 분리 (각 shape: B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # 5. Attention score 계산: (B, num_heads, N, N)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # QK^T / sqrt(d)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)           # Softmax over last dim\n",
        "        attn_probs = self.dropout(attn_probs)                 # Dropout 적용\n",
        "\n",
        "        # 6. Attention 결과 계산: (B, num_heads, N, head_dim)\n",
        "        attn_out = attn_probs @ v\n",
        "\n",
        "        # 7. head 합치기: (B, N, num_heads * head_dim = C)\n",
        "        attn_out = attn_out.transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        # 8. 최종 projection 및 dropout\n",
        "        out = self.proj(attn_out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)  # (batch, tokens, emb_size)\n",
        "    mha = MultiHeadAttention()\n",
        "    out = mha(x)\n",
        "    print(f'Input shape: {x.shape}')      # (8, 197, 768)\n",
        "    print(f'Output shape: {out.shape}')    # (8, 197, 768)\n",
        "    print(f'Parameters: {sum(p.numel() for p in mha.parameters()):,}')  # 파라미터 수\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91glGnw--8wB"
      },
      "source": [
        "# 4. Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-OZsyS_-60F",
        "outputId": "29fb1752-cb60-416b-a847-93b61e748f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 197, 768])\n",
            "Output shape: torch.Size([8, 197, 768])\n",
            "Parameters: 7,085,568\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_size=768, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        hidden_size = int(emb_size * mlp_ratio)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(emb_size, hidden_size),  # 확장 (768 → 3072)\n",
        "            nn.GELU(),                         # 비선형 활성화\n",
        "            nn.Dropout(dropout),              # regularization\n",
        "            nn.Linear(hidden_size, emb_size),  # 축소 (3072 → 768)\n",
        "            nn.Dropout(dropout)               # 다시 dropout\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.scale = self.head_dim ** -0.5  # Scaled Dot-Product에서 나눠줄 값\n",
        "\n",
        "        # Q, K, V를 한 번에 생성 (B, N, 3 * C)\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3, bias=False)\n",
        "\n",
        "        # Attention 결과 projection\n",
        "        self.proj = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.qkv.weight)\n",
        "        nn.init.xavier_uniform_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, N, C = x.shape  # Batch, Token 수, Embedding dim\n",
        "\n",
        "        # qkv: (B, N, 3 * C) → reshape → (B, N, 3, num_heads, head_dim)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "\n",
        "        # permute: (3, B, num_heads, N, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # 각각 shape: (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Attention score 계산: (B, num_heads, N, N)\n",
        "        attn_score = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn_score, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # attention-weighted value 합산: (B, num_heads, N, head_dim)\n",
        "        out = attn @ v\n",
        "\n",
        "        # head 병합: (B, N, C)\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        # 최종 projection + dropout\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pre-LayerNorm → Attention → Residual\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
        "\n",
        "        # Pre-LayerNorm → MLP → Residual\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.mlp = MLP(emb_size, mlp_ratio, dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Residual connection with attention block\n",
        "        x = x + self.attention(self.norm1(x))  # LayerNorm → Attention → Add\n",
        "\n",
        "        # Residual connection with MLP block\n",
        "        x = x + self.mlp(self.norm2(x))        # LayerNorm → MLP → Add\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == \"__main__\":\n",
        "    x = torch.randn(8, 197, 768)  # 8개의 이미지, 197개 토큰 (196 patch + 1 CLS)\n",
        "    block = TransformerEncoderBlock()\n",
        "    out = block(x)\n",
        "    print(f'Input shape: {x.shape}')         # (8, 197, 768)\n",
        "    print(f'Output shape: {out.shape}')      # (8, 197, 768)\n",
        "    print(f'Parameters: {sum(p.numel() for p in block.parameters()):,}')  # 총 파라미터 수\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GQ4-g8m_Cae"
      },
      "source": [
        "# 5. Complete ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1RWUlqVx_AY1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "# (기존 PatchEmbedding, TransformerEncoderBlock이 이미 정의돼 있다고 가정)\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_channels=3,\n",
        "        num_classes=1000,\n",
        "        emb_size=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1,\n",
        "        drop_path=0.0  # 사용 안함 (placeholder)\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Stage 1-2: Patch embedding + CLS token + positional encoding 포함\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        # Stage 3-4: Transformer encoder blocks (N개 반복)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Stage 5: Classification head (LayerNorm + Linear)\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "        nn.init.constant_(self.head.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # 1. 패치 임베딩 + CLS 토큰 + 포지셔널 인코딩 (B, 197, D)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # 2. Transformer 블록 N개 순차 적용\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # 3. LayerNorm 적용 후, CLS 토큰 (첫 번째 토큰) 추출\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]  # (B, D)\n",
        "\n",
        "        # 4. Linear head로 분류 (B, num_classes)\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGSRZJkAEuIx"
      },
      "source": [
        "# 6. ViT for CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ4yt_Y6fADG"
      },
      "source": [
        "위의 코드를 완성했다면, 아래 코드를 실행하여 전체 모델을 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQerYjbvEt1T",
        "outputId": "b6b130ac-7bcb-4fe2-a23e-e23342dfc859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 32, 32])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Parameters: 4,766,474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 13.1MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "Batch 0: Loss 2.3584, Acc 7.81%\n",
            "Batch 100: Loss 2.3397, Acc 9.58%\n",
            "Train Loss: 2.3465, Train Acc: 9.56%\n",
            "Test Loss: 2.3452, Test Acc: 10.14%\n",
            "LR: 0.000500, Epoch time: 40.17s\n",
            "\n",
            "Epoch 2/30\n",
            "Batch 0: Loss 2.3524, Acc 7.81%\n",
            "Batch 100: Loss 1.8780, Acc 27.10%\n",
            "Train Loss: 1.9420, Train Acc: 31.24%\n",
            "Test Loss: 1.7955, Test Acc: 39.13%\n",
            "LR: 0.000499, Epoch time: 40.79s\n",
            "\n",
            "Epoch 3/30\n",
            "Batch 0: Loss 1.7942, Acc 37.89%\n",
            "Batch 100: Loss 1.6713, Acc 41.00%\n",
            "Train Loss: 1.7243, Train Acc: 42.64%\n",
            "Test Loss: 1.6558, Test Acc: 45.61%\n",
            "LR: 0.000494, Epoch time: 41.27s\n",
            "\n",
            "Epoch 4/30\n",
            "Batch 0: Loss 1.7299, Acc 40.62%\n",
            "Batch 100: Loss 1.5954, Acc 47.18%\n",
            "Train Loss: 1.6087, Train Acc: 48.18%\n",
            "Test Loss: 1.5648, Test Acc: 49.45%\n",
            "LR: 0.000487, Epoch time: 43.83s\n",
            "\n",
            "Epoch 5/30\n",
            "Batch 0: Loss 1.5420, Acc 51.56%\n",
            "Batch 100: Loss 1.5252, Acc 49.74%\n",
            "Train Loss: 1.5568, Train Acc: 50.68%\n",
            "Test Loss: 1.5206, Test Acc: 52.54%\n",
            "LR: 0.000477, Epoch time: 42.73s\n",
            "\n",
            "Epoch 6/30\n",
            "Batch 0: Loss 1.5522, Acc 49.61%\n",
            "Batch 100: Loss 1.5252, Acc 53.01%\n",
            "Train Loss: 1.5030, Train Acc: 53.77%\n",
            "Test Loss: 1.4917, Test Acc: 54.27%\n",
            "LR: 0.000464, Epoch time: 43.38s\n",
            "\n",
            "Epoch 7/30\n",
            "Batch 0: Loss 1.4800, Acc 50.00%\n",
            "Batch 100: Loss 1.4094, Acc 54.87%\n",
            "Train Loss: 1.4691, Train Acc: 55.10%\n",
            "Test Loss: 1.4909, Test Acc: 54.59%\n",
            "LR: 0.000449, Epoch time: 42.82s\n",
            "\n",
            "Epoch 8/30\n",
            "Batch 0: Loss 1.3961, Acc 58.59%\n",
            "Batch 100: Loss 1.3424, Acc 56.30%\n",
            "Train Loss: 1.4426, Train Acc: 56.48%\n",
            "Test Loss: 1.4270, Test Acc: 57.55%\n",
            "LR: 0.000431, Epoch time: 43.69s\n",
            "\n",
            "Epoch 9/30\n",
            "Batch 0: Loss 1.4727, Acc 53.52%\n",
            "Batch 100: Loss 1.4928, Acc 57.71%\n",
            "Train Loss: 1.4129, Train Acc: 57.80%\n",
            "Test Loss: 1.3946, Test Acc: 58.72%\n",
            "LR: 0.000412, Epoch time: 43.40s\n",
            "\n",
            "Epoch 10/30\n",
            "Batch 0: Loss 1.3997, Acc 59.77%\n",
            "Batch 100: Loss 1.3587, Acc 59.19%\n",
            "Train Loss: 1.3854, Train Acc: 59.38%\n",
            "Test Loss: 1.3665, Test Acc: 60.25%\n",
            "LR: 0.000390, Epoch time: 43.26s\n",
            "\n",
            "Epoch 11/30\n",
            "Batch 0: Loss 1.3053, Acc 63.67%\n",
            "Batch 100: Loss 1.4192, Acc 60.81%\n",
            "Train Loss: 1.3524, Train Acc: 60.86%\n",
            "Test Loss: 1.3688, Test Acc: 60.04%\n",
            "LR: 0.000367, Epoch time: 43.83s\n",
            "\n",
            "Epoch 12/30\n",
            "Batch 0: Loss 1.2926, Acc 67.58%\n",
            "Batch 100: Loss 1.2318, Acc 61.97%\n",
            "Train Loss: 1.3302, Train Acc: 62.08%\n",
            "Test Loss: 1.3582, Test Acc: 60.77%\n",
            "LR: 0.000343, Epoch time: 43.11s\n",
            "\n",
            "Epoch 13/30\n",
            "Batch 0: Loss 1.2716, Acc 63.28%\n",
            "Batch 100: Loss 1.2481, Acc 63.45%\n",
            "Train Loss: 1.3021, Train Acc: 63.20%\n",
            "Test Loss: 1.3095, Test Acc: 63.46%\n",
            "LR: 0.000317, Epoch time: 43.49s\n",
            "\n",
            "Epoch 14/30\n",
            "Batch 0: Loss 1.3013, Acc 63.67%\n",
            "Batch 100: Loss 1.3068, Acc 63.77%\n",
            "Train Loss: 1.2898, Train Acc: 63.83%\n",
            "Test Loss: 1.2992, Test Acc: 63.27%\n",
            "LR: 0.000290, Epoch time: 43.47s\n",
            "\n",
            "Epoch 15/30\n",
            "Batch 0: Loss 1.3005, Acc 66.02%\n",
            "Batch 100: Loss 1.2351, Acc 65.03%\n",
            "Train Loss: 1.2640, Train Acc: 65.09%\n",
            "Test Loss: 1.3268, Test Acc: 62.41%\n",
            "LR: 0.000264, Epoch time: 42.87s\n",
            "\n",
            "Epoch 16/30\n",
            "Batch 0: Loss 1.1909, Acc 65.62%\n",
            "Batch 100: Loss 1.2935, Acc 66.19%\n",
            "Train Loss: 1.2430, Train Acc: 66.15%\n",
            "Test Loss: 1.2872, Test Acc: 64.03%\n",
            "LR: 0.000236, Epoch time: 44.07s\n",
            "\n",
            "Epoch 17/30\n",
            "Batch 0: Loss 1.2339, Acc 65.62%\n",
            "Batch 100: Loss 1.2240, Acc 67.02%\n",
            "Train Loss: 1.2191, Train Acc: 67.26%\n",
            "Test Loss: 1.2699, Test Acc: 64.99%\n",
            "LR: 0.000210, Epoch time: 43.27s\n",
            "\n",
            "Epoch 18/30\n",
            "Batch 0: Loss 1.1938, Acc 67.19%\n",
            "Batch 100: Loss 1.2142, Acc 68.12%\n",
            "Train Loss: 1.1998, Train Acc: 68.06%\n",
            "Test Loss: 1.2251, Test Acc: 66.53%\n",
            "LR: 0.000183, Epoch time: 43.13s\n",
            "\n",
            "Epoch 19/30\n",
            "Batch 0: Loss 1.0972, Acc 68.75%\n",
            "Batch 100: Loss 1.0664, Acc 68.89%\n",
            "Train Loss: 1.1790, Train Acc: 68.95%\n",
            "Test Loss: 1.2562, Test Acc: 65.32%\n",
            "LR: 0.000157, Epoch time: 43.59s\n",
            "\n",
            "Epoch 20/30\n",
            "Batch 0: Loss 1.1886, Acc 68.36%\n",
            "Batch 100: Loss 1.0904, Acc 69.91%\n",
            "Train Loss: 1.1593, Train Acc: 69.72%\n",
            "Test Loss: 1.2396, Test Acc: 66.27%\n",
            "LR: 0.000133, Epoch time: 43.21s\n",
            "\n",
            "Epoch 21/30\n",
            "Batch 0: Loss 1.1717, Acc 69.53%\n",
            "Batch 100: Loss 1.1724, Acc 70.85%\n",
            "Train Loss: 1.1357, Train Acc: 70.88%\n",
            "Test Loss: 1.2118, Test Acc: 67.72%\n",
            "LR: 0.000110, Epoch time: 43.25s\n",
            "\n",
            "Epoch 22/30\n",
            "Batch 0: Loss 1.0306, Acc 77.34%\n",
            "Batch 100: Loss 1.0834, Acc 71.86%\n",
            "Train Loss: 1.1186, Train Acc: 71.84%\n",
            "Test Loss: 1.2124, Test Acc: 68.02%\n",
            "LR: 0.000088, Epoch time: 43.57s\n",
            "\n",
            "Epoch 23/30\n",
            "Batch 0: Loss 1.2284, Acc 65.62%\n",
            "Batch 100: Loss 1.1648, Acc 72.55%\n",
            "Train Loss: 1.1009, Train Acc: 72.54%\n",
            "Test Loss: 1.1863, Test Acc: 69.23%\n",
            "LR: 0.000069, Epoch time: 43.25s\n",
            "\n",
            "Epoch 24/30\n",
            "Batch 0: Loss 1.1364, Acc 70.31%\n",
            "Batch 100: Loss 1.2214, Acc 73.46%\n",
            "Train Loss: 1.0801, Train Acc: 73.57%\n",
            "Test Loss: 1.1959, Test Acc: 68.98%\n",
            "LR: 0.000051, Epoch time: 43.83s\n",
            "\n",
            "Epoch 25/30\n",
            "Batch 0: Loss 0.9799, Acc 77.73%\n",
            "Batch 100: Loss 1.1412, Acc 74.24%\n",
            "Train Loss: 1.0656, Train Acc: 74.32%\n",
            "Test Loss: 1.1585, Test Acc: 70.35%\n",
            "LR: 0.000036, Epoch time: 43.04s\n",
            "\n",
            "Epoch 26/30\n",
            "Batch 0: Loss 1.1003, Acc 73.83%\n",
            "Batch 100: Loss 1.0972, Acc 74.62%\n",
            "Train Loss: 1.0565, Train Acc: 74.75%\n",
            "Test Loss: 1.1637, Test Acc: 70.18%\n",
            "LR: 0.000023, Epoch time: 43.21s\n",
            "\n",
            "Epoch 27/30\n",
            "Batch 0: Loss 1.0142, Acc 76.17%\n",
            "Batch 100: Loss 1.0482, Acc 75.40%\n",
            "Train Loss: 1.0455, Train Acc: 75.13%\n",
            "Test Loss: 1.1775, Test Acc: 69.75%\n",
            "LR: 0.000013, Epoch time: 43.57s\n",
            "\n",
            "Epoch 28/30\n",
            "Batch 0: Loss 1.0958, Acc 72.66%\n",
            "Batch 100: Loss 1.0126, Acc 75.84%\n",
            "Train Loss: 1.0375, Train Acc: 75.77%\n",
            "Test Loss: 1.1567, Test Acc: 70.78%\n",
            "LR: 0.000006, Epoch time: 43.01s\n",
            "\n",
            "Epoch 29/30\n",
            "Batch 0: Loss 1.0613, Acc 74.22%\n",
            "Batch 100: Loss 1.0358, Acc 76.05%\n",
            "Train Loss: 1.0324, Train Acc: 76.02%\n",
            "Test Loss: 1.1581, Test Acc: 70.52%\n",
            "LR: 0.000001, Epoch time: 43.05s\n",
            "\n",
            "Epoch 30/30\n",
            "Batch 0: Loss 0.9467, Acc 79.69%\n",
            "Batch 100: Loss 1.0181, Acc 76.13%\n",
            "Train Loss: 1.0270, Train Acc: 76.22%\n",
            "Test Loss: 1.1588, Test Acc: 70.60%\n",
            "LR: 0.000000, Epoch time: 43.34s\n",
            "\n",
            "Best Test Accuracy: 70.78%\n"
          ]
        }
      ],
      "source": [
        "class ViTCIFAR10(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=3,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(emb_size, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if hasattr(self.patch_embed, 'cls_token'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.cls_token, std=0.02)\n",
        "        if hasattr(self.patch_embed, 'positions'):\n",
        "            nn.init.trunc_normal_(self.patch_embed.positions, std=0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        x = self.head(cls_token)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}: Loss {loss.item():.4f}, Acc {100.*correct/total:.2f}%')\n",
        "\n",
        "    return running_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = ViTCIFAR10(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        num_classes=10,\n",
        "        emb_size=256,\n",
        "        depth=6,\n",
        "        num_heads=4,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=0.03,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    num_epochs = 30\n",
        "    warmup_epochs = 1\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    warmup_steps = len(train_loader) * warmup_epochs\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "        for _ in range(len(train_loader)):\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "        print(f'LR: {current_lr:.6f}, Epoch time: {epoch_time:.2f}s')\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'vit_cifar10_best.pth')\n",
        "\n",
        "        if test_acc > 90.0:\n",
        "            print(f\"Reached target accuracy!\")\n",
        "            break\n",
        "\n",
        "    print(f'\\nBest Test Accuracy: {best_acc:.2f}%')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = ViTCIFAR10(emb_size=256, depth=6, num_heads=4)\n",
        "    x = torch.randn(4, 3, 32, 32)\n",
        "    out = model(x)\n",
        "\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    print(f'Output shape: {out.shape}')\n",
        "    print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWvZXNO8Q_-Q"
      },
      "source": [
        "ViT는 일반적으로 대규모 데이터셋에서 사전 학습된(pretrained) 모델을 활용하는 경우가 많기 때문에, 하이퍼파라미터를 조정하거나 학습 epoch을 늘리면 성능이 개선될 수는 있지만, 소규모 데이터셋에서 처음부터 학습한 ViT의 성능이 낮은 것은 구조적 한계에 가깝습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}